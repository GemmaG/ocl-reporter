Computer Laboratory – OCaml Labs: Blogs      a.icon-github {
    background: url(../github.png) no-repeat 0 0;
          background: url(../github.png) no-repeat 0 0;
    padding: 0 0 2px 2em;
      }
      a.icon-cloud {
    background: url(../cloud.png) no-repeat 0 0;
          background-size: 17px;
    padding: 0 0 2px 2em;
      }
      a.icon-bullhorn {
    background: url(../bullhorn.png) no-repeat 0 0;
          background-size: 17px;
    padding: 0 0 2px 2em;
      }
      a.icon-wrench {
    background: url(../wrench.png) no-repeat 0 0;
          background-size: 17px;
    padding: 0 0 2px 2em;
      }
      h2.posttitle {
          font-size: 120%;
      }
  div.toc {
      background-color: rgb(239, 239, 239);
      margin: 0.5em 0em 1.5em 1px;
      border: 1px solid black;
      font-size: 0.7em;
      padding: 0px 0px 1ex;
      font-size: 100%;
  }
    a.planet-toggle {
      font-size: 90%;
      padding: 5px 10px;
      margin-bottom: 2ex;
      color: #4b4b4b;
      background: #e6e6e6;
      border: 1px solid #dedede;
    }

    a.planet-toggle:hover, a.planet-toggle:focus {
      color: #ffffff;
      background: #c77a27;
    }

    .btn {
      display: inline-block;
      color: #ffffff;
      *display: inline;
      /* IE7 inline-block hack */

      *zoom: 1;
      padding: 10px 20px;
      margin-bottom: 0;
      font-family: Lato, sans-serif;
      font-weight: bold;
      font-size: 18px;
      line-height: 28px;
      text-align: center;
      vertical-align: middle;
      cursor: pointer;
      background: #8eaf20;
      border: 1px solid #8eaf20;
      *border: 0;
      -webkit-border-radius: 4px;
      -moz-border-radius: 4px;
      border-radius: 4px;
      *margin-left: .3em;
      text-shadow: rgba(0, 0, 0, 0.34) 1px 1px 2px;
      -webkit-box-shadow: rgba(0, 0, 0, 0.46) 0 2px 2px;
      -moz-box-shadow: rgba(0, 0, 0, 0.46) 0 2px 2px;
      box-shadow: rgba(0, 0, 0, 0.46) 0 2px 2px;
    }
    .btn:first-child {
      *margin-left: 0;
    }
    .btn:hover,
    .btn:focus {
      color: #ffffff;
      text-decoration: none;
      background-position: 0 -15px;
      -webkit-transition: background-position 0.1s linear;
      -moz-transition: background-position 0.1s linear;
      -o-transition: background-position 0.1s linear;
      transition: background-position 0.1s linear;
    }
    .btn:focus {
      outline: none;
    }
    .btn.active,
    .btn:active {
      background-image: none;
      outline: 0;
      -webkit-box-shadow: inset 0 2px 4px rgba(0,0,0,.15), 0 1px 2px rgba(0,0,0,.05);
      -moz-box-shadow: inset 0 2px 4px rgba(0,0,0,.15), 0 1px 2px rgba(0,0,0,.05);
      box-shadow: inset 0 2px 4px rgba(0,0,0,.15), 0 1px 2px rgba(0,0,0,.05);
    }
    .btn.disabled,
    .btn[disabled] {
      cursor: default;
      background-image: none;
      opacity: 0.65;
      filter: alpha(opacity=65);
      -webkit-box-shadow: none;
      -moz-box-shadow: none;
      box-shadow: none;
    }

  div#content-primary p img, div#content-primary img.right { float: none; }

      function switchContent(id1,id2) {
     // Get the DOM reference
     var contentId1 = document.getElementById(id1);
     var contentId2 = document.getElementById(id2);
     // Toggle
     contentId1.style.display = "none";
     contentId2.style.display = "block";
     }
  

 Skip to content | Access key help   
  

    
                Search
            
      

      
    

  Advanced searchA–ZContact us
  


Computer Laboratory


 
University of CambridgeComputer LaboratoryOCaml LabsBlogsOCaml LabsNewsBlogsProjectsPapersPeopleCollaborationContact UsOCaml Labs
  
  Recent Posts
       Jun 29, 2015 
    Reviewing the Bitcoin Pinata
    Hannes Mehnert
 

     Jun 26, 2015 
    MirageOS v2.5 with full TLS support
    Amir Chaudhry
 

     Jun 26, 2015 
    Why OCaml-TLS?
    Amir Chaudhry
 

     Jun 22, 2015 
    CueKeeper internals: Experiences with Irmin, React, TyXML and IndexedDB
    Thomas Leonard
 

     May 27, 2015 
    Pearls of Algebraic Effects and Handlers
    KC Sivaramakrishnan
 

     May 20, 2015 
    Effective Concurrency with Algebraic Effects
    KC Sivaramakrishnan
 

     Apr 28, 2015 
    CueKeeper: Gitting Things Done in the browser
    Thomas Leonard
 

     Apr 06, 2015 
    Let's Play Network Address Translation: The Home Game
    Mindy Preston
 

     Apr 03, 2015 
    Towards Heroku for Unikernels: Part 2 - Self Scaling Systems
    Amir Chaudhry
 

     Apr 02, 2015 
    Apr 2015 news update
    Anil Madhavapeddy
 

     Mar 31, 2015 
    Towards Heroku for Unikernels: Part 1 - Automated deployment
    Amir Chaudhry
 

     Mar 25, 2015 
    Opam Switch to Multicore OCaml
    KC Sivaramakrishnan
 

     Mar 25, 2015 
    A unikernel experiment: A VM for every URL
    Magnus Skjegstad
 

     Mar 19, 2015 
    Part 3: Running your own DNS Resolver with MirageOS
    Heidi Howard
 

     Mar 02, 2015 
    Part 2: Running your own DNS Resolver with MirageOS
    Heidi Howard
 

     Feb 18, 2015 
    Part 1: Running your own DNS Resolver with MirageOS
    Heidi Howard
 

     Feb 10, 2015 
    Smash the Bitcoin Pinata for fun and profit!
    Amir Chaudhry
 

     Feb 05, 2015 
    South of England Regional Programming Language Seminar (S-REPLS)
    Compiler Hacking
 

     Feb 05, 2015 
    Ninth OCaml compiler hacking evening (back in the lab, with a talk from Oleg)
    Compiler Hacking
 

     Jan 21, 2015 
    Securing the Unikernel
    Thomas Leonard
 

     Jan 19, 2015 
    Local MirageOS development with Xen and Virtualbox
    Magnus Skjegstad
 

     Jan 08, 2015 
    Towards a governance framework for OCaml.org
    Amir Chaudhry
 

     Dec 31, 2014 
    MirageOS 2014 review: IPv6, TLS, Irmin, Jitsu and community growth
    Anil Madhavapeddy
 

     Oct 27, 2014 
    Visualising an asynchronous monad
    Thomas Leonard
 

     Sep 23, 2014 
    Eighth OCaml compiler hacking evening (at Mill Lane, by the river)
    Compiler Hacking
 

     Sep 17, 2014 
    Simplifying the solver with functors
    Thomas Leonard
 

     Aug 15, 2014 
    Optimising the unikernel
    Thomas Leonard
 

     Jul 28, 2014 
    My first unikernel
    Thomas Leonard
 

     Jul 25, 2014 
    Release of  “ARC: Analysis of Raft Consensus”
    Heidi Howard
 

     Jul 24, 2014 
    Seventh OCaml compiler hacking session (at Citrix)
    Compiler Hacking
 

     Jul 22, 2014 
    MirageOS v2.0: a recap of the new features
    Anil Madhavapeddy
 

     Jul 22, 2014 
    Building an ARMy of Xen unikernels
    Thomas Leonard
 

     Jul 21, 2014 
    Using Irmin to add fault-tolerance to the Xenstore database
    Dave Scott
 

     Jul 18, 2014 
    Introducing Irmin: Git-like distributed, branchable storage
    Thomas Gazagnaire
 

     Jul 17, 2014 
    Fitting the modular MirageOS TCP/IP stack together
    Mindy Preston
 

     Jul 16, 2014 
    Vchan: Low-latency inter-VM communication channels
    Jon Ludlam
 

     Jul 15, 2014 
    Modular foreign function bindings
    Jeremy Yallop
 

     Jul 14, 2014 
    OCaml-TLS: the protocol implementation and mitigations to known attacks
    David Kaloper
 

     Jul 11, 2014 
    OCaml-TLS: ASN.1 and notation embedding
    David Kaloper
 

     Jul 10, 2014 
    OCaml-TLS: Adventures in X.509 certificate parsing and validation
    Hannes Mehnert
 

     Jul 09, 2014 
    OCaml-TLS: building the nocrypto library core
    David Kaloper
 

     Jul 08, 2014 
    Introducing transport layer security (TLS) in pure OCaml
    Hannes Mehnert
 

     Jul 08, 2014 
    MirageOS 1.2 released and the 2.0 runup begins
    Anil Madhavapeddy
 

     Jun 24, 2014 
    Highlights from recent sessions
    Compiler Hacking
 

     Jun 20, 2014 
    Sixth OCaml compiler hacking session
    Compiler Hacking
 

     Jun 06, 2014 
    Python to OCaml: retrospective
    Thomas Leonard
 

     May 08, 2014 
    Welcome to the summer MirageOS hackers
    Anil Madhavapeddy
 

     Apr 29, 2014 
    Writing Planet in pure OCaml
    Amir Chaudhry
 

     Apr 24, 2014 
    Fifth OCaml compiler hacking session
    Compiler Hacking
 

     Mar 10, 2014 
    From Jekyll site to Unikernel in fifty lines of code.
    Amir Chaudhry
 

     Feb 25, 2014 
    MirageOS is in Google Summer of Code 2014
    Anil Madhavapeddy
 

     Feb 13, 2014 
    OCaml: what you gain
    Thomas Leonard
 

     Feb 11, 2014 
    MirageOS 1.1.0: the eat-your-own-dogfood release
    Anil Madhavapeddy
 

     Feb 11, 2014 
    Fourth OCaml compiler hacking session
    Compiler Hacking
 

     Feb 04, 2014 
    How to handle success
    Compiler Hacking
 

     Jan 07, 2014 
    OCaml: the bugs so far
    Thomas Leonard
 

     Jan 03, 2014 
    Presenting Decks
    Richard Mortier
 

     Dec 29, 2013 
    Dec 2013 news update
    Anil Madhavapeddy
 

     Dec 20, 2013 
    Polymorphism for beginners
    Thomas Leonard
 

     Dec 19, 2013 
    MirageOS 1.0.3 released; tutorial on building this website available
    Anil Madhavapeddy
 

     Dec 09, 2013 
    MirageOS 1.0: not just a hallucination!
    Anil Madhavapeddy
 

     Nov 28, 2013 
    Asynchronous Python vs OCaml
    Thomas Leonard
 

     Nov 26, 2013 
    Switching from Bootstrap to Zurb Foundation
    Amir Chaudhry
 

     Nov 20, 2013 
    Announcing the new OCaml.org
    Amir Chaudhry
 

     Nov 06, 2013 
    Migration plan for the OCaml.org redesign
    Amir Chaudhry
 

     Oct 30, 2013 
    Third OCaml compiler hacking session
    Compiler Hacking
 

     Oct 28, 2013 
    Review of the OCaml FPDays tutorial
    Amir Chaudhry
 

     Oct 22, 2013 
    FP Days OCaml Session
    Amir Chaudhry
 

     Oct 13, 2013 
    OCaml tips
    Thomas Leonard
 

     Oct 08, 2013 
    FPDays 2013 Real World OCaml tutorial in Cambridge
    Anil Madhavapeddy
 

     Oct 06, 2013 
    Using Travis for secure deployments with SSH
    Anil Madhavapeddy
 

     Oct 03, 2013 
    Intellisense for OCaml with Vim and Merlin
    Anil Madhavapeddy
 

     Oct 02, 2013 
    Liveblogging the first Human Data Interaction workshop
    Anil Madhavapeddy
 

     Sep 30, 2013 
    Test your OCaml packages in minutes using Travis CI
    Anil Madhavapeddy
 

     Sep 28, 2013 
    Experiences with OCaml objects
    Thomas Leonard
 

     Sep 24, 2013 
    Feedback requested on the OCaml.org redesign
    Amir Chaudhry
 

     Sep 24, 2013 
    Liveblogging OCaml Workshop 2013
    Anil Madhavapeddy
 

     Sep 22, 2013 
    Liveblogging CUFP 2013
    Anil Madhavapeddy
 

     Sep 20, 2013 
    OPAM 1.1 beta available, with pretty colours
    Anil Madhavapeddy
 

     Sep 17, 2013 
    Inaugural compiler hackers meeting
    Compiler Hacking
 

     Sep 17, 2013 
    OCaml Monthly Meeting – Live Blog
    Heidi Howard
 

     Sep 16, 2013 
    Camlpdf, the first good command-line PDF tool I've found
    Anil Madhavapeddy
 

     Sep 09, 2013 
    OCaml Development in Vim
    Heidi Howard
 

     Sep 09, 2013 
    OCamlot--exploring the edges of OPAM packages
    Anil Madhavapeddy
 

     Aug 31, 2013 
    Option handling with OCaml polymorphic variants
    Thomas Leonard
 

     Aug 28, 2013 
    ICFP, CUFP & OCaml2013
    Heidi Howard
 

     Aug 23, 2013 
    Introducing vchan
    Vincent Bernardoff
 

     Aug 19, 2013 
    Real World OCaml beta3 release
    Heidi Howard
 

     Aug 08, 2013 
    MirageOS travels to OSCON'13: a trip report
    Richard Mortier
 

     Aug 06, 2013 
    Final Real World OCaml beta; the good, the bad and the ugly
    Anil Madhavapeddy
 

     Aug 01, 2013 
    OCaml Lecture Notes
    Heidi Howard
 

     Jul 18, 2013 
    Creating Xen block devices with MirageOS
    Dave Scott
 

     Jul 10, 2013 
    Profiling OCaml – Getting Started Guide
    Heidi Howard
 

     Jul 07, 2013 
    OCaml binary compatibility
    Thomas Leonard
 

     Jun 20, 2013 
    Replacing Python: second round
    Thomas Leonard
 

     Jun 17, 2013 
    Phew, Real World OCaml beta now available.
    Anil Madhavapeddy
 

     Jun 09, 2013 
    Replacing Python: candidates
    Thomas Leonard
 

     Jun 01, 2013 
    Jun 2013 news update
    Anil Madhavapeddy
 

     May 20, 2013 
    The road to a developer preview at OSCON 2013
    Anil Madhavapeddy
 

     May 01, 2013 
    May 2013 news update
    Anil Madhavapeddy
 

       
          Reviewing the Bitcoin Pinata
      (Mirage OS)
    
                                      TL;DR: Nobody took our BTC.  Random people from the Internet even donated
into our BTC wallet.
We showed the feasibility of a
transparent self-service bounty.  In the style of Dijkstra: security
bounties can be a very effective way to show the presence of
vulnerabilities, but they are hopelessly inadequate for showing their
absence.
What are you talking about?

Earlier this year, we released a Bitcoin Piñata.
The Piñata was a security bounty
containing 10 BTC and it's been online since 10th February 2015.
Upon successful
mutual authentication, where the Piñata has only a single trust anchor, it sends the
private key to the Bitcoin address.
It is open source,
and exposes both the client and server side of
ocaml-tls, running as an 8.2MB
MirageOS unikernel.  You can see the code manifest to find out which libraries are involved.  We put this online and invited people to attack it.
Any approach was permitted in attacking the Piñata:
the host system, the MirageOS TCP/IP
stack, ou…Read more...      TL;DR: Nobody took our BTC.  Random people from the Internet even donated
into our BTC wallet.
We showed the feasibility of a
transparent self-service bounty.  In the style of Dijkstra: security
bounties can be a very effective way to show the presence of
vulnerabilities, but they are hopelessly inadequate for showing their
absence.
What are you talking about?

Earlier this year, we released a Bitcoin Piñata.
The Piñata was a security bounty
containing 10 BTC and it's been online since 10th February 2015.
Upon successful
mutual authentication, where the Piñata has only a single trust anchor, it sends the
private key to the Bitcoin address.
It is open source,
and exposes both the client and server side of
ocaml-tls, running as an 8.2MB
MirageOS unikernel.  You can see the code manifest to find out which libraries are involved.  We put this online and invited people to attack it.
Any approach was permitted in attacking the Piñata:
the host system, the MirageOS TCP/IP
stack, our TLS,
X.509 and ASN.1 implementations, as well as the Piñata code.
A successful attacker could do whatever they want with the BTC, no
questions asked (though we would notice the transaction).
The exposed server could even be short-circuited to the exposed
client: you could proxy a TLS connection in which the (encrypted!)
secret was transmitted via your machine.
This post summarises what we've seen so far and what we've learned about attempts people have made to take the BTC.
Accesses

There were 50,000 unique IP addresses who accessed the website.
1000 unique IP addresses initiated more than 20,000 TLS
connections to the Piñata, trying to break it.  Cumulative numbers of
the HTTP and TLS accesses are shown in the diagram:

There were more than 9000 failing and 12000 successful TLS sessions,
comprised of short-circuits described earlier, and our own tests.
No X.509 certificate was presented in 1200 of the failed TLS
connections.  Another 1000 failed due to invalid input as the first
bytes.  This includes attempts using telnet — I'm looking at you,
xx.xxx.74.126 please give key (on 10th February at 16:00) and
xx.xxx.166.143 hi give me teh btcs (on 11th February at 05:57)!
We are not talking to everybody

Our implementation first parses the record version of a client hello,
and if it fails, an unknown record version is reported.  This happened
in 10% of all TLS connections (including the 1000 with invalid input in the
last section).
Another big class, 6%, were attempted Heartbeat packets (popular due
to Heartbleed), which we
do not implement.
Recently, issues in the state machines of TLS implementations were
published in smacktls (and CCS
injection).  3% of the Piñata connections
received an unexpected handshake record at some point, which the Piñata handled
correctly by shutting down the connection.
In 2009, the renegotiation
attack
on the TLS protocol was published, which allowed a person in the
middle to inject prefix bytes, because a renegotiated handshake was
not authenticated with data from the previous handshake.  OCaml-TLS
closes a connection if the renegotiation
extension is not present, which
happened in 2% of the connections.
Another 2% did not propose a ciphersuite supported by OCaml-TLS; yet
another 2% tried to talk SSL version 3 with us, which we do not
implement (for good reasons, such as
POODLE).
In various other (old versions of) TLS implementations, these
connections would have been successful and insecure!
Attempts worth noting

Interesting failures were: 31 connections which sent too many or too
few bytes, leading to parse errors.
TLS requires each communication partner who authenticates themselves to
present a certificate.  To prove ownership of the private key of the
certificate, a hash of the concatenated handshake records needs to be
signed and transmitted over the wire.  22 of our TLS traces had
invalid signatures.  Not verifying such signatures was the problem of Apple's famous goto
fail.
Another 100 failure traces tested our X.509 validation:
The majority of these failures (58) sent us certificates which were not signed by our trust
anchor, such as CN=hacker/emailAddress=hacker@hacker and CN=Google
Internal SNAX Authority and various Apple and Google IDs -- we're still trying to figure out what SNAX is, Systems Network Architecture maybe?
Several certificates contained invalid X.509 extensions: we require
that a server certificate does not contain the BasicConstraints =
true extension, which marks this certificate as certificate
authority, allowing to sign other certificates.  While not explicitly
forbidden, best practices (e.g. from
Mozilla)
reject them.  Any sensible systems administrator would not accept a CA
as a server certificate.
Several other certificates were self-signed or contained an invalid
signature: one certificate was our client certificate, but with a
different RSA public key, thus the signature on the certificate was
invalid; another one had a different RSA public key, and the signature
was zeroed out.
Some certificates were not of X.509 version 3, or were expired.
Several certificate chains were not pairwise signed, a common attack
vector.
Two traces contained certificate structures which our ASN.1 parser
rejected.
Another two connections (both initiated by ourselves) threw an
exception which lead to shutdown of the connection: there
was
an out-of-bounds access while parsing handshake records.  This did not
lead to arbitrary code execution.
Conclusion

The BTC Piñata was the first transparent self-service bounty, and it
was a success: people showed interest in the topic; some even donated
BTC; we enjoyed setting it up and running it; we fixed a non-critical
out of bounds access in our implementation; a large fraction of our
stack has been covered by the recorded traces.
There are several points to improve a future Piñata: attestation that the code
running is the open sourced code, attestation that the service owns
the private key (maybe by doing transactions or signatures with input
from any user).
There are several applications using OCaml-TLS, using MirageOS as well
as Unix:
mirage-seal compiles to
a unikernel container which serves a given directory over https;tlstunnel is a
(stud like) TLS proxy, forwarding
to a backend server;jackline is a
(alpha version) terminal-based XMPP client;conduit is an abstraction
over network connections -- to make it use OCaml-TLS, set
CONDUIT_TLS=native.

Again, a big thank you to IPredator for
hosting our BTC Piñata and lending us the BTC!

   Hide
        
      
                    by Hannes Mehnert at Jun 29, 2015 
      
      
    
  


       
                  MirageOS v2.5 with full TLS support
      (Mirage OS)
    
    
                                      Today we're announcing the new release of MirageOS v2.5, which includes
first-class support for SSL/TLS in the MirageOS configuration language. We
introduced the pure OCaml implementation of
transport layer security (TLS) last summer and have been working since
then to improve the integration and create a robust framework.  The recent
releases allow developers to easily build and deploy secure unikernel services
and we've also incorporated numerous bug-fixes and major stability
improvements (especially in the network stack).  The full list of changes is
available on the releases page and the breaking API changes
now have their own page.
Over the coming week, we'll share more about the TLS stack by diving into the
results of the Bitcoin Piñata, describing a new workflow for
building secure static sites, and discussing insights on entropy in
virtualised environments.
In the rest of this post, we'll cover why OCaml-TLS matters (and link to some
tools), mention our new domain name,…Read more...      Today we're announcing the new release of MirageOS v2.5, which includes
first-class support for SSL/TLS in the MirageOS configuration language. We
introduced the pure OCaml implementation of
transport layer security (TLS) last summer and have been working since
then to improve the integration and create a robust framework.  The recent
releases allow developers to easily build and deploy secure unikernel services
and we've also incorporated numerous bug-fixes and major stability
improvements (especially in the network stack).  The full list of changes is
available on the releases page and the breaking API changes
now have their own page.
Over the coming week, we'll share more about the TLS stack by diving into the
results of the Bitcoin Piñata, describing a new workflow for
building secure static sites, and discussing insights on entropy in
virtualised environments.
In the rest of this post, we'll cover why OCaml-TLS matters (and link to some
tools), mention our new domain name, and mention our security advisory
process.
Why OCaml-TLS matters

The last year has seen a slew of security flaws, which are even reaching the
mainstream news.  This history of flaws are often the result of implementation
errors and stem from the underlying challenges of interpreting ambiguous
specifications, the complexities of large APIs and code bases, and the use of
unsafe programming practices.  Re-engineering security-critical software
allows the opportunity to use modern approaches to prevent these recurring
issues. In a separate post, we cover some of the benefits of
re-engineering TLS in OCaml.
TLS Unix Tools

To make it even easier to start benefiting from OCaml-TLS, we've also made a
collection of TLS unix tools.  These are designed to make it
really easy to use a good portion of the stack without having to use Xen. For
example, Unix tlstunnel is being used on https://realworldocaml.org. If
you have stunnel or stud in use somewhere, then replacing it with the 
tlstunnel binary is an easy way to try things out.  Please do give this a go
and send us feedback!
openmirage.org -> mirage.io

We've also switched our domain over to https://mirage.io, which is a
unikernel running the full stack. We've been discussing this transition for a
while on our fortnightly calls and have actually been running this
unikernel in parallel for a while. Setting things up this way has allowed us
to stress test things in the wild and we've made big improvements to the
networking stack as a result.
We now have end-to-end deployments for our secure-site unikernels, which is
largely automated -- going from git push all the way to live site. You can
get an idea of the workflows we have set up by looking over the following
links:
Automated unikernel deployment -- Description of the end-to-end flow for one of our sites.mirage-www-deployment repo -- The repo from which we pull the site you're currently reading! You might find the scripts useful.

Security disclosure process

Since we're incorporating more security features, it's important to consider
the process of disclosing issues to us.  Many bugs can be reported as usual on
our issue tracker but if you think you've discovered a
security vulnerability, the best way to inform us is described on a new
page at https://mirage.io/security.
Get started!

As usual, MirageOS v2.5 and the its ever-growing collection of
libraries is packaged with the OPAM package
manager, so look over the installation instructions
and run opam install mirage to get the command-line
tool. To update from a previously installed version of MirageOS,
simply use the normal workflow to upgrade your packages by using opam
update -u (you should do this regularly to benefit from ongoing fixes).
If you're looking for inspiration, you can check out the examples on
mirage-skeleton or ask on the mailing list. Please do be aware
that existing config.ml files using
the conduit and http constructors might need to be updated -- we've made a
page of backward incompatible changes to explain what you need to
do.
We would love to hear your feedback on this release, either on our
issue tracker or our mailing lists!

   Hide
        
      
                    by Amir Chaudhry at Jun 26, 2015 
      
      
    
  


       
                  Why OCaml-TLS?
      (Mirage OS)
    
    
                                      TLS implementations have a history of security flaws, which are often the
result of implementation errors.  These security flaws stem from the
underlying challenges of interpreting ambiguous specifications, the
complexities of large APIs and code bases, and the use of unsafe programming
practices.
Re-engineering security-critical software allows the opportunity to use modern
approaches to prevent these recurring issues. Creating the TLS stack in OCaml
offers a range of benefits, including:
Robust memory safety: Lack of memory safety was the largest single source
of vulnerabilities in various TLS stacks throughout 2014, including
Heartbleed (CVE-2014-0160). OCaml-TLS avoids this
class of issues entirely due to OCaml's automatic memory management, safety
guarantees and the use of a pure-functional programming style.
Improved certificate validation: Implementation errors in other stacks
allowed validation to be skipped under certain conditions, leaving users
exposed (e.g.
CVE-2014-…Read more...      TLS implementations have a history of security flaws, which are often the
result of implementation errors.  These security flaws stem from the
underlying challenges of interpreting ambiguous specifications, the
complexities of large APIs and code bases, and the use of unsafe programming
practices.
Re-engineering security-critical software allows the opportunity to use modern
approaches to prevent these recurring issues. Creating the TLS stack in OCaml
offers a range of benefits, including:
Robust memory safety: Lack of memory safety was the largest single source
of vulnerabilities in various TLS stacks throughout 2014, including
Heartbleed (CVE-2014-0160). OCaml-TLS avoids this
class of issues entirely due to OCaml's automatic memory management, safety
guarantees and the use of a pure-functional programming style.
Improved certificate validation: Implementation errors in other stacks
allowed validation to be skipped under certain conditions, leaving users
exposed (e.g.
CVE-2014-0092).
In our TLS stack, we return errors explicitly as values and handle all
possible variants. The OCaml toolchain and compile-time checks ensure that
this has taken place.
Mitigation of state machine errors: Errors such as
Apple's GoTo Fail (CVE-2014-1266) involved code being
skipped and a default 'success' value being returned, even though signatures
were never verified. Our approach encodes the state machine explicitly, while
state transitions default to failure. The code structure also makes clear the
need to consider preconditions.
Elimination of downgrade attacks: Legacy requirements forced other TLS
stacks to incorporate weaker 'EXPORT' encryption ciphers. Despite the
environment changing, this code still exists and leads to attacks such as
FREAK (CVE-2015-0204) and
Logjam (CVE-2015-4000). Our TLS server does not support
weaker EXPORT cipher suites so was never vulnerable to such attacks.
In addition our stack never supported SSLv3, which was known to be the cause of many vulnerabilities and is only now in the process of being deprecated (RFC: 7568).
Greatly reduced TCB: The size of the trusted computing base (TCB) of a
system, measured in lines of code, is a widely accepted approximation of the
size of its attack surface.  Our secure Bitcoin Piñata, a unikernel built
using our TLS stack, is less than 4% the size of an equivalent, traditional
stack (102 kloc as opposed to 2560 kloc).
These are just some of the benefits of re-engineering critical software using
modern techniques.

   Hide
        
      
                    by Amir Chaudhry at Jun 26, 2015 
      
      
    
  


       
                  CueKeeper internals: Experiences with Irmin, React, TyXML and IndexedDB
      (Thomas Leonard)
    
    
                                In CueKeeper: Gitting Things Done in the browser, I wrote about CueKeeper, a Getting Things Done application that runs client-side in your browser.
It stores your actions in a Git-like data-store provided by Irmin, allowing you to browse the history, revert changes, and sync (between tabs and, once the server backend is available, between devices).
Several people asked about the technologies used to build it, so that’s what this blog will cover.



CueKeeper screenshot. Click for interactive version.

Table of Contents

  Overview
  Compiling to Javascript
  Using Javascript APIs
  Data structures
  Backwards compatibility
  Irmin
  IndexedDB backend
  Revisions and merging
  React
  TyXML
  Reactive TyXML
  Problems with React
  Debugging
  Conclusions
  Next steps
  Acknowledgements


( this post also appeared on Reddit )

Overview

CueKeeper is written in OCaml and compiled to Javascript using js_of_ocaml.
The HTML is produced using TyXML, and kept up-to-date with React (note: th…Read more...In CueKeeper: Gitting Things Done in the browser, I wrote about CueKeeper, a Getting Things Done application that runs client-side in your browser.
It stores your actions in a Git-like data-store provided by Irmin, allowing you to browse the history, revert changes, and sync (between tabs and, once the server backend is available, between devices).
Several people asked about the technologies used to build it, so that’s what this blog will cover.



CueKeeper screenshot. Click for interactive version.

Table of Contents

  Overview
  Compiling to Javascript
  Using Javascript APIs
  Data structures
  Backwards compatibility
  Irmin
  IndexedDB backend
  Revisions and merging
  React
  TyXML
  Reactive TyXML
  Problems with React
  Debugging
  Conclusions
  Next steps
  Acknowledgements


( this post also appeared on Reddit )

Overview

CueKeeper is written in OCaml and compiled to Javascript using js_of_ocaml.
The HTML is produced using TyXML, and kept up-to-date with React (note: that’s OCaml React, not Facebook React).
Records are serialised using Sexplib and stored by Irmin in a local IndexedDB database in your browser.
Action descriptions are written in Markdown, which is parsed using Omd.

Here’s a diagram of the main modules that make up CueKeeper:



  disk_node defines the on-disk data types representing stored items such as actions and projects.
  rev loads all the items in a single Git commit (revision), which together represent the state of the system at some point.
  update keeps track of the current branch head, loading the new version when it updates. It is also responsible for writing changes to storage.
  merge can merge any two branches using a 3-way merge.
  model queries the state to extract the information to be displayed (e.g. list of current actions).
  template renders the results to HTML. It also uses e.g. the Pikaday date-picker widget.
  client is the main entry point for the Javascript (client-side) part of CueKeeper (the server is currently under development).
  git_storage provides a Git-like interface to Irmin, which uses the irmin_IDB backend to store the data in the browser using IndexedDB (plus a little HTML storage for cross-tab notifications).


The full code is available at https://github.com/talex5/cuekeeper.

Compiling to Javascript

To generate Javascript code from OCaml, first compile to OCaml bytecode and then run js_of_ocaml on the result, like this:

test.ml 
1
2
let () =
  print_endline "Hello from OCaml!"


$ ocamlc test.ml -o test.byte
$ js_of_ocaml test.byte -o test.js


To test it, create an HTML file to load the new test.js code and open the HTML file in a web browser:

test.html 
1
2
3
4
5
6
7
<!DOCTYPE html>
<html>
  <body>
    <p>Open the browser's Javascript console to see the output.</p>
    <script type="text/javascript" src="test.js"></script>
  </body>
</html>


OCaml bytecode statically includes any OCaml libraries it uses, so this method also works for complex real-world programs.
Many OCaml libraries can be used directly.
For example, I used ocaml-tar to create .tar archives in the browser for the export feature, and
the omd Markdown parser for the descriptions.

If the OCaml code uses external C functions (that aren’t already provided) then you need to implement them in Javascript.
In the case of CueKeeper, I had to implement a few trivial functions for blitting blocks of memory between OCaml strings, bigarrays and bin_prot buffers.
I put these in a helpers.js file and added it to the js_of_ocaml arguments.

Using Javascript APIs

My first attempt at writing code for the browser was my Lwt trace visualiser.
I initially wrote that for the desktop but it turned out that running it in the browser was just a matter of replacing calls to GTK’s Cairo canvas with calls to the very similar HTML canvas.
Writing CueKeeper required learning a bit more about the mysterious world of the Javascript DOM.

I also needed to integrate with the Pikaday date picker widget.
To do this, you first declare an OCaml class type for each Javascript class (you only have to define the methods you want to use), like this:

pikaday.ml 
1
2
3
4
5
6
7
8
9
10
11
12
class type pikaday =
  object
    method getDate : Js.date Js.t Js.Opt.t Js.meth
  end
class type config =
  object
    method container : Dom_html.element Js.t Js.prop
    method onSelect : (pikaday, Js.date Js.t -> unit) Js.meth_callback Js.prop
    method defaultDate : Js.date Js.t Js.Optdef.t Js.prop
    method setDefaultDate : bool Js.t Js.prop
  end


This says that a pikaday object has a getDate method which returns an optional Javascript date object, and that
a config object provides properties such as onSelect, which is a callback of a pikaday object which takes a date and returns nothing.

The constructors are built using Js.Unsafe:

1
2
3
let make_config () : config Js.t = Js.Unsafe.obj [| |]
let pikaday_constr : (config Js.t -> pikaday Js.t) Js.constr =
  Js.Unsafe.global##_Pikaday


They’re “unsafe” because this isn’t type checked; there’s no way to know whether Pikaday really implements the interface we defined above.
However, from this point on everything we do with Pikaday is statically checked against our definitions.

js_of_ocaml provides a syntax extension for OCaml to make using native Javascript objects easier.
object##property reads a property, object##property <- value sets a property, and object##method(args) calls a method.
Note that parentheses around the arguments are required, unlike with regular OCaml method calls.
Note also that js_of_ocaml ignores underscores in various places to avoid differences between Javascript and OCaml naming conventions (properties can’t start with an uppercase character, for example).

It’s interesting the way OCaml’s type inference is used here: Js.Unsafe.global can take any type, and OCaml infers that its type is “object with a Pikaday property, which is a pikaday constructor taking a config argument” because that’s how we use it.

Finally, here’s the code that creates a new Pikaday object:

pikaday.ml 
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
let make ?(initial:Ck_time.user_date option) ~on_select () =
  let div = Html5.div [] in
  let elem = Tyxml_js.To_dom.of_div div in
  let config = make_config () in
  config##container <- elem;
  config##onSelect <- Js.wrap_callback (fun d ->
    on_select (to_user_date d)
  );
  begin match (initial :> (int * int * int) option) with
  | Some (y, m, d) ->
      let js_date = jsnew Js.date_day (y, m, d) in
      config##defaultDate <- Js.Optdef.return js_date;
      config##setDefaultDate <- Js._true;
  | None -> () end;
  let pd = jsnew pikaday_constr (config) in
  (div, pd)


Here, we create a <div> element and use it as the container field of the Pikaday config object.
on_select is an OCaml function to handle the result, which we wrap with Js.wrap_callback and set as the Javascript callback.
If an initial date is given, we construct a Javascript Date object and set that as the default.
Finally, we create the Pikaday object and return it, along with the containing div.

All this means that binding to Javascript APIs is very easy and, thanks to the extra type-checking, feels more pleasant even than using Javascript libraries directly from Javascript.

Data structures

In CueKeeper, areas, projects and actions all share a common set of fields, which I defined using an OCaml record:

ck_disk_node.ml 
1
2
3
4
5
6
7
8
type node_details = {
  parent : Ck_id.t sexp_option;
  name : string;
  description : string;
  ctime : float;
  contact : Ck_id.t sexp_option;
  conflicts : string sexp_list;
} with sexp




A Ck_id.t is a UUID (unique string).
I refer to other nodes using UUIDs so that renaming a node doesn’t require updating everything that points to it.
This simplifies merging.
Each record is stored as a single file, and the name of the file is the item’s UUID.
The conflicts field is used to store messages about any conflicts that had to be resolved during merging.

The with sexp annotation makes use of Sexplib to auto-generate code for serialising and deserialising these structures.
I use sexp_option and sexp_list rather than option and list to provide slightly nicer output: these fields will be omitted if empty.

I also (rather lazily) reuse this structure for contacts and contexts, but always keep parent and contact as None for them.

For actions and projects, we also need to record some extra data:

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
type project_details = {
  pstarred : bool with default(false);
  pstate : [ `Active | `SomedayMaybe | `Done ]
} with sexp
type astate =
  [ `Next
  | `Waiting
  | `Waiting_for_contact
  | `Waiting_until of Ck_time.user_date
  | `Future
  | `Done ] with sexp
type action_details = {
  astarred : bool with default(false);
  astate : astate;
  context : Ck_id.t sexp_option;
  repeat: Ck_time.repeat sexp_option;
} with sexp


Here’s what a project looks like when printed with Sexplib.Sexp.to_string_hum
(the hum suffix turns on pretty-printing; the real code uses plain to_string):



(Project
 (((pstarred false) (pstate Active))
  ((parent 3eba7466-dafd-4b96-9fad-c9859ef825f2)
  (name "Make a Mirage unikernel") (description "") (ctime 1429555212.546))))


Note that the child nodes don’t appear at all here.
Instead, we find them through their parent field.

Finally, I wrapped everything up in some polymorphic variants:

1
2
3
4
5
6
7
8
9
10
11
12
13
type action_node = action_details * node_details
type project_node = project_details * node_details
type area_node = node_details
type contact_node = node_details
type context_node = node_details
type action = [`Action of action_node]
type project = [`Project of project_node]
type area = [`Area of area_node]
type contact = [`Contact of contact_node]
type context = [`Context of context_node]
type generic = [ area | project | action | contact | context ]


This is very useful, because other parts of the code often want to deal with subsets of the types.
The interface lists which types can be used in each operation:

ck_disk_node.mli 
1
2
3
val parent : [< area | project | action ] -> Ck_id.t option
val starred : [< project | action] -> bool
val action_repeat : action -> Ck_time.repeat option


This says that only areas, projects and actions have parents; only projects and actions can be starred; and only actions can repeat.

Using variants makes it easy for other modules to match on the different types.
For example, here’s the code for generating the Process tab’s tree:



ck_model.ml 
1
2
3
4
5
6
7
8
9
10
11
  let make_process_tree r =
    let rec aux items =
      M.fold (fun _key item acc ->
	match item with
	| `Action _ -> acc
	| `Project _ as p when Node.project_state p <> `Active -> acc
	| `Area _ | `Project _ ->
	    let children = aux (R.child_nodes item) in
	    acc |> TreeNode.add (TreeNode.unique_of_node ~children item)
      ) items TreeNode.Child_map.empty in
    aux (R.roots r)


Actions and inactive projects don’t appear (they return the accumulator unmodified),
while for areas and active projects we add a node, including a recursive call to get the children.

One problem I had with this scheme was the return types for modifications:

ck_disk_node.mli 
1
val with_conflict : string -> ([< generic] as 'a) -> 'a


with_conflict msg node returns a copy of node with its conflict messages field extended with the given message.
The type says that it works with any subset of the node types, and that the result will be the same subset.
For example, when adding a conflict about repeats to an action, the result will be an action.
When adding a message to something that could be an area, project or action, the result will be another area, project or action.

However, I couldn’t work out how to implement this signature without using Obj.magic (unsafe cast).
I asked on StackOverflow (Map a subset of a polymorphic variant) and it seems there’s no easy answer.

I also experimented with a couple of other approaches:

  Using GADTs didn’t work because they don’t support arbitrary subsets. A function either handles a specific node type, or all node types, but not e.g. just projects and actions.
  Using objects avoided the need for the unsafe cast, but required more code elsewhere. Objects work well when you have a fixed set of operations and you want to make it easy to add new kinds of thing, but in GTD the set of types is fixed, while the operations (report generation, rendering on different devices, merging) are more open-ended.


Backwards compatibility

Although this is the 0.1-alpha release, I made various changes to the format during development and it’s never too early to check that smooth upgrades are possible.
Besides, I’ve been recklessly using it as my action tracker during development and I don’t like typing things in twice.

You’ll notice some fields above have a with_default annotation.
This provides a default value when loading from earlier versions.
For more complex cases, it’s possible to write custom code.
For example, I changed the date representation at one point from Unix timestamps to calendar dates (this provides more intuitive behaviour when moving between time-zones I think).
There is code in Ck_time to handle this:

ck_time.ml 
1
2
3
4
5
6
type user_date = (int * int * int) with sexp_of
let user_date_of_sexp =
  let open Sexplib.Type in function
  | Atom _ as x -> <:of_sexp<float>> x |> of_unix_time (* Old format *)
  | List _ as x -> <:of_sexp<int * int * int>> x


In the implementation (ck_time.ml) I use with sexp_of so that only the serialisation code is created automatically, while it uses my custom code for deserialising.
In the interface (ck_time.mli), I just declare it as with sexp and code outside doesn’t see anything special.

Irmin

The next step was to write the data to the Irmin repository.
Irmin itself provides a fairly traditional key/value store API with some extra features for version control.
That might be useful for existing applications, but I wanted a more Git-like API.
For example, Irmin allows you to read files directly from the branch head, but in the browser another tab might update the branch between the two reads, leading to inconsistent results.
I wanted something that would force me to use atomic operations.
Also, the Irmin API is still being finalised, so I wanted to provide an example of my “ideal” API.

Here’s the API wrapper I used (it doesn’t provide access to all Irmin’s features, just the ones I needed):

A Staging.t corresponds to the Git staging area / working directory. It is mutable, and not shared with other tabs:

git_storage_s.mli 
1
2
3
4
5
6
7
8
9
module Staging : sig
  type t
  val list : t -> path -> path list Lwt.t
  val read_exn : t -> path -> string Lwt.t
  val update : t -> path -> string -> unit Lwt.t
  val remove : t -> path -> unit Lwt.t
  val mem : t -> path -> bool Lwt.t
end


A Commit.t represents a single (immutable) Git commit.
You can check out a commit to get a staging area, modify that, and then commit it to create a new Commit.t:

1
2
3
4
5
6
7
8
9
module Commit : sig
  type t
  val checkout : t -> Staging.t Lwt.t
  val commit : ?parents:t list -> Staging.t -> msg:string -> t Lwt.t
  val equal : t -> t -> bool
  val history : ?depth:int -> t -> Log_entry.t Log_entry_map.t Lwt.t
  val export_tar : t -> string Lwt.t
end


A branch is a mutable pointer to a commit.
The head is represented as a reactive signal (more on React later), making it easy to follow updates.
The only thing you can do with a branch is fast-forward it to a new commit.

1
2
3
4
5
6
module Branch : sig
  type t
  val head : t -> Commit.t option React.S.t
  val fast_forward_to : t -> Commit.t -> [ `Ok | `Not_fast_forward ] Lwt.t
end


Finally, a Repository.t represents a repository as a whole.
You can look up a branch by name or a commit by hash:

1
2
3
4
5
6
7
8
9
10
11
12
13
module Repository : sig
  type t
  val branch : t -> if_new:(Commit.t Lwt.t Lazy.t) -> branch_name -> Branch.t Lwt.t
  (** Get the named branch.
   * If the branch does not exist yet, [if_new] is called to get the initial commit. *)
  val commit : t -> Irmin.Hash.SHA1.t -> Commit.t option Lwt.t
  (** Look up a commit by its hash. *)
  val empty : t -> Staging.t Lwt.t
  (** Create an empty checkout with no parent. *)
end


Thomas Gazagnaire provided many useful updates to Irmin to let me implement this API: atomic operations (needed for a reliable fast_forward_to), support for creating commits with arbitrary parents (needed for custom merging, described below), and performance improvements (very important for running in a browser!).

IndexedDB backend

Irmin provides a Git backend that supports normal Git repositories, as well as a simpler filesystem backend, a remote HTTP backend, and an in-memory-only backend.
To run Irmin in the browser, I initially added a backend for HTML 5 storage.

However, HTML 5 storage is limited to 5 MB of data and since my backend lacked compression, it eventually ran out, so I then replaced it with support for IndexedDB.

js_of_ocaml supports most (standardised) HTML features, but IndexedDB had only just come out, so I had to write my own bindings (as for Pikaday, above).

IndexedDB is rather complicated compared to local storage, so I split it across several modules.
I first defined the Javascript API (indexedDB.mli), then wrapped it in a nicer OCaml API, providing asynchronous operations with Lwt threading rather than callbacks (indexedDB_lwt.mli).
I then made an Irmin backend that uses it (irmin_IDB.ml).

The Irmin API for backends can be a little confusing at first.
An Irmin “branch consistent” (Git-like) repository internally consists of two simpler stores: an append-only store that stores immutable blobs (files, directories and commits), indexed by their SHA1 hash, and a read-write store that is used to record which commit each branch currently points to.
If you can provide implementations of these two APIs, Irmin can automatically provide the full branch-consistent database API itself.

One problem with moving to IndexedDB is that it doesn’t support notifications.
To get around this, when CueKeeper updates the master branch to point at a new commit, it also writes the SHA1 hash to local storage.
Other open windows or tabs get notified of this and then read the new data from IndexedDB.

I also found a couple of browser bugs while testing this.
Firefox seems to not clean up IndexedDB transactions, though this doesn’t cause any obvious problems in practice.

Safari, however, has a more serious problem: if two threads (tabs) try to read from the database at the same time, one of the transactions will fail!
I was able to reproduce the error with a few lines of JavaScript (see idb_reads.html).

The page will:

  Open a test database, creating a single key=value pair if it doesn’t exist.
  Attempt to read the value of the key ten times, once per second.


If you open this in two windows in Safari at once, one of them will likely fail with AbortError.
I reported it to Apple, but their feedback form says they don’t respond to feedback, and they were as good as their word.
In the end, I added some code to sleep for a random period and retry on aborted reads.

Revisions and merging

Each object (project, action, contact, etc) in CueKeeper is a file in Irmin and each change creates a new commit.
This model tends to avoid race conditions.
For example, when you edit the title of an action and press Return, CueKeeper will:

  Create a new commit with the updates, whose parent is the commit you started editing.
  Merge this commit with the master branch.


Usually nothing else has changed since you started editing and the merge is a trivial “fast-forward” merge.
However, if you had edited something else about that action at the same time then instead of overwriting the changes, CueKeeper will merge them.

If you change the same field in two tabs at once, CueKeeper will pick one value and add a merge conflict note telling you the change it discarded.
You can try it here (click the image for an interactive page running two copies of CueKeeper split-screen):



The merge code takes three commits (the tips of the two branches being merged and an optional-but-usually-present “least common ancestor”), and produces a resulting commit (which may include merge conflict notes for the user to check):

ck_merge.mli 
1
2
3
4
5
6
7
8
9
10
module Make (Git : Git_storage_s.S) (R : Ck_rev.S with type commit = Git.Commit.t) : sig
  val merge : ?base:Git.Commit.t -> theirs:Git.Commit.t -> Git.Commit.t ->
    [`Ok of Git.Commit.t | `Nothing_to_do] Lwt.t
  (* [merge ?base ~theirs ours] merges changes from [base] to [ours] into [theirs] and
   * returns the resulting merge commit. *)
  val revert : repo:Git.Repository.t -> master:Git.Commit.t -> Git_storage_s.Log_entry.t ->
    [`Ok of Git.Commit.t | `Nothing_to_do | `Error of string] Lwt.t
  (** [revert ~master log_entry] returns a new commit on [master] which reverts the changes in [log_entry]. *)
end


A revert operation is essentially the same as a merge, except that the base commit is the commit being undone, its (single) parent is one branch and the current state is the other.
It’s a separate operation in the API because the commit it generates has a different format (it has only one parent and gives the commit being reverted in the log message).

I wanted to make sure that the merge code would always produce a valid result (e.g. the parent field of a node should point to a node that exists in the merged version).
I wrote a unit-test that performs many merges at random and checks the result loads without error.

My first thought was to perform edits at random to get a base commit, then make two branches from that and perform more random edits on each one.
After a while, I realised that you can edit any valid state into pretty-much any other valid state, so a simpler approach is to generate three commits at random.

You have to be a little bit careful here, however.
If the three commits are completely random then they won’t have any UUIDs in common and the merges will be trivial.
Therefore, the UUIDs (and all field values) are chosen from a small set of candidates to ensure they’re often the same.

I wrote the tests before the merge code, and as I wrote the merge code I deliberately failed to implement the required features first to check the tests caught each possible failure.
The tests found these problems automatically:

  Commit refers to a contact or context that doesn’t exist.
  Project has area as child.
  Repeating action marked as done.
  Action, project or area has a missing parent.


It was easy enough to check which cases I’d missed because each possible failure corresponds to a call to bug in ck_rev.ml.
These problems weren’t detected initially by the tests:

  Cycles in the parent relation
  Initially, my random state generator created nodes with random IDs, but at the time there was a bug in Irmin (since fixed) where it didn’t sort the entries before writing them, which confused the Git tools I was using to examine the test results.
To work around this, I changed the test code to create the nodes with monotonically increasing IDs.
However, it would only set the parent to a previously-created node, so this meant that e.g. node 1 could never have a parent of node 2, which meant it could never generate two commits with the parent relation the other way around.
Easily fixed.
  A Waiting_for_contact action has no contact
  I was running 1000 iterations of the tests with a fixed seed while writing them.
This particular case only triggered after 1500 iterations, but it would have been found eventually when I removed the fixed seed.
To help things along, I added a slow_test make target that compiles the tests to native code and runs 10,000 iterations, and set this to run on the Travis build (it still only takes 14 seconds, but that’s too long to do on every build, and long enough that the extra couple of seconds compiling to native code is worth it).
  An action is a parent of another node
  This one was a bit surprising.
To trigger it, you start with a base containing an action and a project.
On one branch, make the action a child of the project (only the action changes).
On the other, convert the project to an action (only the project changes).
If the bug is present, you end up with one action being a child of the other.
This wasn’t picked up because it only happens if the project doesn’t change at all in the first branch.
If it does change, the code for merging nodes gets called, and that copes with trying to merge a project with an action by converting the action to a project, which avoids the bug.
Because the nodes were being generated at random, the chance that every field in the base and the first branch would be identical was very low.
To fix it, I now generate a random number at the start of each test iteration and use it to bias the creation of the three states so that many of the fields will be shared.
This is enough to trigger detection of the bug.


React

The OCaml React library provides support for Functional reactive programming.
The idea here is to represent a (mutable) variable as a “signal”.
Instead of operating on the current value of the variable, you operate on the signal as a whole.

Say you want to show a live display of the number of actions.
A traditional approach might be:

1
2
3
4
let actions = ref 0 in
let update () =
  show (Printf.sprintf "There are %d actions" !actions) in
update ()


Then you have to remember to call update whenever you change actions.
Instead, in FRP you work on the signal as a whole:

1
2
let actions, set_actions = S.create 0 in
actions |> S.map (Printf.sprintf "There are %d actions") |> show


Here, actions is a signal, the S.map creates a new (string valued) signal from the old (int valued) one, and show ensures that the current value of the signal is displayed on the screen.

It’s a bit like using a spreadsheet: you just enter the formulae, and the system ensures everything stays up-to-date.
CueKeeper uses signals all over the place: the Git commit at the tip of a branch, the description of an action, the currently selected tab, etc.

TyXML

I initially tried to generate the HTML using Caml on the Web (COW).
This provides a syntax extension for embedding HTML in your code.
For example, I wrote some code to render a tree to HTML, something like this:

1
2
3
4
5
6
7
8
9
10
11
let rec render_list items =
  <:html<
    <ul>
      $list:List.map render_item items$
    </ul>
  >>
and render_item (name, Node children) =
  <:html<
    <li>$str:name$</li>
    $render_list children$
  >>


It wasn’t really suitable for what I wanted, though, because there was no obvious way to make it update (except by regenerating the whole thing and setting the innerHTML DOM attribute).
Also, while embedding another language with its own syntax is usually a nice feature, in the case of HTML I’m happy to make an exception.

I’d come across TyXML before, but had given up after being baffled by the documentation.
However, spurred on by the promise of React integration, I started reading the source code and it turned out to be fairly simple.

For every HTML element, TyXML provides a function with the same name.
The function takes a list of child nodes as its argument and, optionally, a list of attributes.
Written this way, the above code looks something like this:

1
2
3
4
5
6
7
8
9
open Tyxml_js.Html5
let rec render_list items =
  ul (List.map render_item items |> List.concat)
and render_item (name, Node children) =
  [
    li [pcdata name];
    render_list children
  ]


It didn’t compile, though, with a typically complicated error:

Error: This expression has type
     ([> Html5_types.ul ] as 'a) Tyxml_js.Html5.elt
   but an expression was expected of type
     Html5_types.li Tyxml_js.Html5.elt
   Type 'a = [> `Ul ] is not compatible with type
     Html5_types.li = [ `Li of Html5_types.li_attrib ] 
   The second variant type does not allow tag(s) `Ul


Eventually, I realised what it was saying.
My COW code above was wrong: it output each item as <li>name</li><ul>...</ul>.
The browser accepted this, but it’s not valid HTML - the <ul> needs to go inside the <li>.
In fact, all we need is:

1
2
3
4
let rec render_list items =
  ul (List.map render_item items)
and render_item (name, Node children) =
  li [pcdata name; render_list children]


Shorter and more correct - a win for TyXML!
It also type-checks attributes.
For example, if you provide an onclick attribute then you can’t provide a handler function with the wrong type (or get the name of the attribute wrong, or use a non-standard attribute, at least without explicit use of “unsafe” features).

Reactive TyXML

The Tyxml_js.Html5 module provides static elements, while Tyxml_js.R.Html5 provides reactive ones.
These take signals for attribute values and child lists and update the display automatically as the signal changes.
You can mix them freely (e.g. a static element with a reactive attribute).

For example, here’s a (slightly simplified) version of the code that displays the tabs along to the top:

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
open Tyxml_js
open Tyxml_js.Html5
open React
let current_mode, set_mode = S.create `Work
let (>|~=) x f = S.map f x
let tab name mode =
  let clicked _ev = set_mode mode; false in
  let classes = current_mode >|~= fun current ->
    if current = mode then ["active"] else [] in
  li ~a:[R.Html5.a_class classes] [
    a ~a:[a_onclick clicked] [pcdata name]
  ]
let mode_switcher =
  ul ~a:[a_class ["ck-mode-selector"]] [
    tab "Process" `Process;
    tab "Work" `Work;
    tab "Contact" `Contact;
    tab "Schedule" `Schedule;
    tab "Review" `Review;
  ]


The tabs are an HTML <ul> element with one <li> for each tab.
current_mode is a reactive signal for the currently selected mode, which is initially Work.
Each <li> has a reactive class attribute which is "active" when the tab’s mode is equal to the current mode.
Clicking the tab sets the mode.




Problems with React

My experience with using react is that it’s very easy to write code that is short, clear, and subtly wrong.
Consider this (slightly contrived) example, which shows up-to-date information about how many of our actions have been completed:

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
let total_actions, set_total = React.S.create 0
let complete_actions, set_complete = React.S.create 0
let (>>~=) = React.S.bind
let (>|~=) x f = React.S.map f x
let () =
  let _ =
    complete_actions >>~= (function
    | 0 -> React.S.const "(nothing complete)"
    | complete ->
        total_actions >|~= fun total ->
          Thread.delay 0.1;
          Printf.sprintf "%d/%d (%d%%)"
            complete total
            (100 * complete / total)
    ) >|~= Printf.printf "Update: %s\n%!" in
  while true do
    let t = Random.int 1000 in
    let step = React.Step.create () in
    set_total ~step t;
    set_complete ~step (Random.int (t + 1));
    React.Step.execute step
  done


We have two signals, representing the total number of actions and the number of them that are complete.
We connect the complete_actions signal to a function that outputs one of two signals: either a constant “(nothing complete)” signal if there are no complete actions, or a signal that shows the number and percentage complete.
This string signal is then connected up to an output function which, in this case, just prints it to the console with Update:  prepended.

The loop at the end sets the total to a random number and the number complete to a random number less than or equal to that.
We use a “step” to ensure that the two signals are updated atomically.
Looks reasonable, right?
Running it, it works for a bit, but gets slower and slower as it runs, before eventually failing in one of two ways:

First, if the total_actions signal ever becomes zero again after being non-zero, it will crash with Division_by_zero:

Update: (nothing complete)
Update: 25/44 (56%)
Update: 27/82 (32%)
Update: 20/39 (51%)
Update: (nothing complete)
Update: 8/21 (38%)
Update: 11/17 (64%)
Update: 28/49 (57%)
Update: 12/15 (80%)
Update: (nothing complete)
Update: 24/28 (85%)
Update: 43/89 (48%)
Update: 3/14 (21%)
Update: 8/23 (34%)
Update: 87/96 (90%)
Fatal error: exception Division_by_zero


The reason is that callbacks are not removed immediately.
When complete_actions is non-zero, we attach a callback to track complete and show the percentage.
When complete_actions becomes zero again, this callback continues to run, even though its output is no longer used.

If it doesn’t crash, the garbage collector will eventually  be run and the old callbacks will be removed.
Unfortunately, this will also garbage collect the callback that prints the status updates, and the program will simply stop producing any new output at this point.

At least, that’s what happens with native code.
Javascript doesn’t have weak references, so old callbacks are never removed there.

Early versions of CueKeeper uses React extensively, but I had to scale back my use of it due to these kinds of problems with callback lifetimes.
My general work-around is to break the reactive signal chains into disconnected sub-graphs, which can be garbage-collected individually.
For example, each panel in the display (e.g. showing the details of an action) contains a number of signals (name, parent, children, etc) which are used to keep the display up-to-date, but these signals are updated using imperative code, not by connecting them to the signal of the Irmin branch head.
When you close the panel, the functions for updating these signals become unreachable, allowing them to be GC’d, and they immediately stop being called.
Thus, we make leek a few callbacks while the panel is open, but closing it returns us to a clean state.

In a similar way, the tree view in the left column is a collection of signals that are updated manually.
Switching to a different tab will allow them to be freed.
It’s not ideal, but it works.

I think that to complete its goal of having well-defined semantics, React needs to stop relying on weak references.
I imagine it would be possible to define a “global sink” object of some sort, such that a signal is live if and only if it is connected to that sink, or is a dependency of something else that is.
Then the let _ = above could be replaced with a connection to the global sink and the rest of the program would behave as expected.
I haven’t thought too much about exactly how this would work, though.

Debugging

There was another problem, which I hit twice.
OCaml always optimises tail calls, but Javascript doesn’t (I’m not sure about ES6).
In most cases where it matters, js_of_ocaml turns the code into a loop, but it doesn’t handle continuation-passing style.
Both Sexplib and Omd failed to parse larger documents, and did so unpredictably.
I suspect that Firefox’s JIT may be affecting things, because my test case didn’t always trigger at the same point.
In both cases, I was able to modify the code to avoid the problem.

Conclusions

For CueKeeper, I used js_of_ocaml to let me write reliable type-checked OCaml code and compile it to Javascript.
js_of_ocaml is surprisingly easy to use, provides most of the standard DOM APIs and is easy to extend to other APIs such as Pikaday or IndexedDB.
TyXML provides a pleasant way to generate HTML output, checking at compile time that it will produce valid (not just well-formed) HTML.

Functional reactive programming makes it easy to define user interfaces that always show up-to-date information.
However, I had problems with signals leaking or running after they were no longer needed due to the React library’s reliance on weak references and the garbage collector to clean up old signals.
If this problem could be fixed, this would be an ideal way to write interactive applications.
As it is, it is still useful but must be used with care.

Data structures are defined in OCaml and (de)serialised automatically using the Sexplib library.
Sexplib is easy to extend with custom behaviour, for example to support changes in the format, but required a minor patch to work reliably in the browser.

Most applications store data using filesystems or relational databases.
CueKeeper uses Irmin to store data in a Git-like repository.
Writing the merge code can be somewhat tricky, but you have to do this anyway if you want your application to support off-line use or multiple users, and once done you get race-free operation, multi-tab support, history and revert for free.

Irmin can be extended with new backends and I created one that uses IndexedDB to store the data client-side in the browser.
The standard is rather new and there are still browser bugs to watch out for, but it seems to be working reliably now.

The full code is available at https://github.com/talex5/cuekeeper.

Next steps

I hope to get back to working on sync between devices.
I made a start on the server branch, which runs a sync service as a Mirage unikernel, but there’s no access control yet, so don’t use it unless you want to share your TODO list with the whole world!

However, I got distracted by an interesting TCP bug, where a connection would sometimes hang, and wondering what caused that made me think there should be a way to ask the system why a thread didn’t resolve, which resulted in some interesting improvements to the tracing and visualisation system…

Acknowledgements

Some of the research leading to these results has received funding from the European Union’s Seventh Framework Programme FP7/2007-2013 under the UCN project, grant agreement no 611001.

Hide
        
      
                    by Thomas Leonard at Jun 22, 2015 
      
      
    
  


       
          Pearls of Algebraic Effects and Handlers
      (KC Sivaramakrishnan)
    
                                In the previous
post, I
presented a simple cooperative multithreaded scheduler written using algebraic
effects and their handlers. Algebraic effects are of course useful for
expressing other forms of effectful computations. In this post, I will present
a series of simple examples to illustrate the utility of algebraic effects and
handlers in OCaml. Some of the examples presented here were borrowed from the
excellent paper on Eff programming language1. All of the examples
presented below are available
here.

State

We can use algebraic effects to model stateful
computation,
with the ability to retrieve (get) and update (put) the current state:

module type STATE = sig
  type t
  val put : t -> unit
  val get : unit -> t
  val run : (unit -> unit) -> init:t -> unit
end

The function run runs a stateful computation with the given initial state.
Here is the implementation of the module State which provides the desired
behaviour:

 1 module State (S : sig type t end) : STATE with type t = SRead more...In the previous
post, I
presented a simple cooperative multithreaded scheduler written using algebraic
effects and their handlers. Algebraic effects are of course useful for
expressing other forms of effectful computations. In this post, I will present
a series of simple examples to illustrate the utility of algebraic effects and
handlers in OCaml. Some of the examples presented here were borrowed from the
excellent paper on Eff programming language1. All of the examples
presented below are available
here.

State

We can use algebraic effects to model stateful
computation,
with the ability to retrieve (get) and update (put) the current state:

module type STATE = sig
  type t
  val put : t -> unit
  val get : unit -> t
  val run : (unit -> unit) -> init:t -> unit
end

The function run runs a stateful computation with the given initial state.
Here is the implementation of the module State which provides the desired
behaviour:

 1 module State (S : sig type t end) : STATE with type t = S.t = struct
 2   type t = S.t
 3 
 4   effect Put : t -> unit
 5   let put v = perform (Put v)
 6 
 7   effect Get : t
 8   let get () = perform Get
 9 
10   let run f ~init =
11     let comp =
12       match f () with
13       | () -> (fun s -> ())
14       | effect (Put s') k -> (fun s -> continue k () s')
15       | effect Get k -> (fun s -> continue k s s)
16     in comp init
17 end

The key idea here is that the handler converts the stateful computation to
functions that accept the state. For example, observe that if the function f
returns a unit value (line 13), we return a function which accepts a state
s and returns unit. The handler for effect Get (line 15) passes the current state s
to the continuation k. The expression continue k s returns a function that
accepts the current state and returns unit. Since fetching the current state
does not modify it, we apply this function to s, the original state. Since
Put modifies the state (line 14), the function returned by continue k () is applied
to the new state s'. We evaluate the computation by applying it to the initial
state init (line 16).

Observe that the implementation of the handler for the stateful computation is
similar to the implementation of State
monad in Haskell. Except
that in Haskell, you would have the stateful computation f have the type
State t (), which says that f is a stateful computation where t is the
type of state and  () the type of return value. Since multicore OCaml does
not have a effect system, f simply has type unit -> unit as opposed to
being explicitly tagged with the effects being performed. While the OCaml type
of f under specifies the behaviour of f, it does allow you to combine various
kinds of effects directly, without the need for monad transformer
gymnastics2. For example, the following code snippet combines an int
and string typed state computations, each with its own handler:

module IS = State (struct type t = int end)
module SS = State (struct type t = string end)

let foo () : unit =
  printf "%d\n" (IS.get ());
  IS.put 42;
  printf "%d\n" (IS.get ());
  IS.put 21;
  printf "%d\n" (IS.get ());
  SS.put "hello";
  printf "%s\n" (SS.get ());
  SS.put "world";
  printf "%s\n" (SS.get ())

let _ = IS.run (fun () -> SS.run foo "") 0

which prints:

0
42
21
hello
world

References

We can expand upon our state example, to model ML style
references:

module State : sig
    type 'a t

    val ref  : 'a -> 'a t
    val (!)  : 'a t -> 'a
    val (:=) : 'a t -> 'a -> unit

    val run  : (unit -> 'a) -> 'a
  end = struct

  type 'a t = {inj : 'a -> Univ.t; prj : Univ.t -> 'a option}

  effect Ref : 'a -> 'a t
  let ref v = perform (Ref v)

  effect Read : 'a t -> 'a
  let (!) = fun r -> perform (Read r)

  effect Write : 'a t * 'a -> unit
  let (:=) = fun r v -> perform (Write (r,v))

  let run f =
    let comp =
      match f () with
      | v -> (fun s -> v)
      | effect (Ref v) k -> (fun s ->
          let (inj, prj) = Univ.embed () in
          let cont = continue k {inj;prj} in
          cont (inj v::s))
      | effect (Read {inj; prj}) k -> (fun s ->
          match find prj s with
          | Some v -> continue k v s
          | None -> failwith "Ref.run: Impossible -> ref not found")
      | effect (Write ({inj; prj}, v)) k -> (fun s ->
          continue k () (inj v::s))
    in comp []
end

The idea is to represent the state as a list of universal typed values,
references as a record with inject and project functions to and from universal
type values, assign as appending a new value to the head of the state list, and
dereference as linear search through the list for a matching assignment. The
universal type
implementation is
due to Alan Frisch.

Transactions

We may handle lookup and update to implement
transactions
that discards the updates to references in case an exception occurs:

let atomically f =
    let comp =
      match f () with
      | x -> (fun _ -> x)
      | exception e -> (fun rb -> rb (); raise e)
      | effect (Update (r,v)) k -> (fun rb ->
          let old_v = !r in
          r := v;
          continue k () (fun () -> r := old_v; rb ()))
    in comp (fun () -> ())

Updating a reference builds up a rollback function that negates the effect of
the update. In case of an exception, the rollback function is evaluated before
re-raising the exception. For example, in the following code snippet:

exception Res of int

let () = atomically (fun () -> (* T0 *)
  let r = ref 10 in
  printf "T0: %d\n" (!r);
  try atomically (fun () -> (* T1 *)
    r := 20;
    r := 21;
    printf "T1: Before abort %d\n" (!r);
    raise (Res !r);
    printf "T1: After abort %d\n" (!r);
    r := 30)
  with
  | Res v -> printf "T0: T1 aborted with %d\n" v;
  printf "T0: %d\n" !r)

the updates to reference r by transaction T1 are discarded on exception and
the program prints the following:

T0: 10
T1: Before abort 21
T0: T1 aborted with 21
T0: 10

From Iterators to Generators

An iterator is a fold-function of type ('a -> unit) -> unit, that iterates a
client function over all the elements of a data structure. A generator is a
function of type unit -> 'a option that returns Some v each time the
function is invoked, where v is the next-element in the data structure. The
function returns None if the traversal is complete. Unlike an iterator, the
generator hands over control of the traversal to the client of the library.

Gabriel Scherer's insightful article on generators, iterators, control and
continuations
nicely distinguish, motivate and provide implementation of different kinds of
iterators and generators for binary trees. While the iterator implementation is
obvious and straight-forward, the generator implementation requires translating
the code to CPS style and manually performing simplifications for efficient
traversal. Since algebraic effects handlers give us a handle to the
continuation, we can essentially derive the generator implementation from
the
iterator.

Let us consider a binary tree with the following type:

type 'a t = Leaf | Node of 'a t * 'a * 'a t

We can define an iterator that traverses the tree from left to right as follows:

let rec iter f = function
  | Leaf -> ()
  | Node (l, x, r) -> iter f l; f x; iter f r

From this iterator, we derive the generator as follows:

 1 let to_gen (type a) (t : a t) =
 2   let module M = struct effect Next : a -> unit end in
 3   let open M in
 4   let step = ref (fun () -> assert false) in
 5   let first_step () =
 6     try
 7       iter (fun x -> perform (Next x)) t;
 8       None
 9     with effect (Next v) k ->
10       step := continue k;
11       Some v
12   in
13     step := first_step;
14     fun () -> !step ()

At each step of the iteration, we perform the effect Next : a -> unit (line
7), which is handled by saving the continuation to a local reference and
returning the value (line 9 - 11). Since the effect handlers are provided with
the continuation, we are able to invert the control from the library to the
client of the library. This avoids the need to perform manual CPS translation.

Direct-style asynchronous IO

Since the effect handler has access to the continuation, we can implement
minimal asynchronous IO in
direct-style
as opposed to the monadic style of asynchronous IO libraries such as Lwt and
Async. Our asynchronous IO library has the following interface:

module type AIO = sig

  val fork  : (unit -> unit) -> unit
  val yield : unit -> unit

  type file_descr = Unix.file_descr
  type sockaddr = Unix.sockaddr
  type msg_flag = Unix.msg_flag

  val accept : file_descr -> file_descr * sockaddr
  val recv   : file_descr -> bytes -> int -> int -> msg_flag list -> int
  val send   : file_descr -> bytes -> int -> int -> msg_flag list -> int
  val sleep  : float -> unit

  val run : (unit -> unit) -> unit
end

Observe that the return type of the non-blocking function calls accept,
recv, send and sleep are the same as their blocking counterparts from
Unix module.

The asynchronous IO implementation works as follows. For each blocking action,
if the action can be performed immediately, then it is. Otherwise, the thread
performing the blocking task is suspended and add to a pool of threads waiting
to perform IO:

(* Block until data is available to read on the socket. *)
effect Blk_read  : file_descr -> unit
(* Block until socket is writable. *)
effect Blk_write : file_descr -> unit
(* Sleep for given number of seconds. *)
effect Sleep : float -> unit

let rec core f =
  match f () with
  ...
  | effect (Blk_read fd) k ->
      if poll_rd fd then continue k ()
      else (Hashtbl.add read_ht fd k;
            dequeue ())
  | effect (Blk_write fd) k ->
      if poll_wr fd then continue k ()
      else (Hashtbl.add write_ht fd k;
            dequeue ())
  | effect (Sleep t) k ->
        if t <= 0. then continue k ()
        else (Hashtbl.add sleep_ht (Unix.gettimeofday () +. t) k;
              dequeue ())

let accept fd =
  perform (Blk_read fd);
  Unix.accept fd

let recv fd buf pos len mode =
  perform (Blk_read fd);
  Unix.recv fd buf pos len mode

let send fd bus pos len mode =
  perform (Blk_write fd);
  Unix.send fd bus pos len mode

The scheduler works by running all of the available threads until there are no
more threads to run. At this point, if there are threads that are waiting to
complete an IO operation, the scheduler invokes select() call and blocks
until one of the IO actions becomes available. The scheduler then resumes those
threads whose IO actions are now available:

(* When there are no threads to run, perform blocking io. *)
let perform_io timeout =
  let rd_fds = Hashtbl.fold (fun fd _ acc -> fd::acc) read_ht [] in
  let wr_fds = Hashtbl.fold (fun fd _ acc -> fd::acc) write_ht [] in
  let rdy_rd_fds, rdy_wr_fds, _ = Unix.select rd_fds wr_fds [] timeout in
  let rec resume ht = function
  | [] -> ()
  | x::xs ->
      enqueue (Hashtbl.find ht x);
      Hashtbl.remove ht x;
      resume ht xs
  in
  resume read_ht rdy_rd_fds;
  resume write_ht rdy_wr_fds;
  if timeout >= 0. then ignore (wakeup (Unix.gettimeofday ())) else ();
  dequeue ()

The
program
implements a simple echo server. The server listens on localhost port 9301. It
accepts multiple clients and echoes back to the client any data sent to the
server. This server is a direct-style reimplementation of the echo server found
here,
which implements the echo server in CPS style:

(* Repeat what the client says until the client goes away. *)
let rec echo_server sock addr =
  try
    let data = recv sock 1024 in
    if String.length data > 0 then
      (ignore (send sock data);
       echo_server sock addr)
    else
      let cn = string_of_sockaddr addr in
      (printf "echo_server : client (%s) disconnected.\n%!" cn;
       close sock)
  with
  | _ -> close sock

The echo server can be tested with a telnet client by starting the server and
on the same machine running telnet localhost 9301.

Conclusion

The aim of the post is to illustrate the variety of alternative programming
paradigms that arise due to algebraic effects and handlers, and hopefully
kindle interest in reasoning and programming with effects and handlers in
OCaml. Algebraic effects and handlers support in OCaml is in active development
within the context of multicore
OCaml. When you find those
inevitable bugs, please report them to the issue
tracker.


Programming with Algebraic Effects and Handlers (pdf) ↩


Programming and Reasoning with Algebraic Effects and Dependent Types (pdf) ↩




Hide
        
      
                    by KC Sivaramakrishnan at May 27, 2015 
      
      
    
  


       
          Effective Concurrency with Algebraic Effects
      (KC Sivaramakrishnan)
    
                                Algebraic effects and handlers provide a modular abstraction for expressing
effectful computation, allowing the programmer to separate the expression of an
effectful computation from its implementation. In this post, I will present an
extension to OCaml for programming with linear algebraic effects, and
demonstrate its use in expressing concurrency primitives for multicore
OCaml. The design and
implementation of algebraic effects for multicore OCaml is due to Leo
White, Stephen Dolan and
the multicore team at OCaml
Labs.

Motivation

Multicore-capable functional programming language implementations such as
Glasgow Haskell Compiler,
F#, Manticore and
MultiMLton expose one or more
libraries for expressing concurrent programs. The concurrent threads of
execution instantiated through the library are in turn multiplexed over the
available cores for speed up. A common theme among such runtimes is that the
primitives for concurrency along with the concurrent thread scheduler is baked
into the…Read more...Algebraic effects and handlers provide a modular abstraction for expressing
effectful computation, allowing the programmer to separate the expression of an
effectful computation from its implementation. In this post, I will present an
extension to OCaml for programming with linear algebraic effects, and
demonstrate its use in expressing concurrency primitives for multicore
OCaml. The design and
implementation of algebraic effects for multicore OCaml is due to Leo
White, Stephen Dolan and
the multicore team at OCaml
Labs.

Motivation

Multicore-capable functional programming language implementations such as
Glasgow Haskell Compiler,
F#, Manticore and
MultiMLton expose one or more
libraries for expressing concurrent programs. The concurrent threads of
execution instantiated through the library are in turn multiplexed over the
available cores for speed up. A common theme among such runtimes is that the
primitives for concurrency along with the concurrent thread scheduler is baked
into the runtime system. Over time, the runtime system itself tends to become a
complex, monolithic piece of software, with extensive use of locks, condition
variables, timers, thread pools, and other arcana. As a result, it becomes
difficult to maintain existing concurrency libraries, let alone add new ones.
Such lack of malleability is particularly unfortunate as it prevents developers
from experimenting with custom concurrency libraries and scheduling strategies,
preventing innovation in the ecosystem. Our goal with this work is to provide a
minimal set of tools with which programmers can implement new concurrency
primitives and schedulers as OCaml libraries.

A Taste of Effects

A Simple Scheduler

Let us illustrate the algebraic effect extension in multicore OCaml by
constructing a concurrent round-robin scheduler with the following interface:

(* Control operations on threads *)
val fork  : (unit -> unit) -> unit
val yield : unit -> unit
(* Runs the scheduler. *)
val run   : (unit -> unit) -> unit

The basic tenet of programming with algebraic effects is that performing an
effectful computation is separate from its interpretation1.
In particular, the interpretation is dynamically chosen based on the context in
which an effect is performed. In our example, spawning a new thread and
yielding control to another are effectful actions, for which we declare the
following effects:

type _ eff +=
| Fork  : (unit -> unit) -> unit eff
| Yield : unit eff

The type 'a eff is the predefined extensible variant type for effects,
where 'a represents the return type of performing the effect. For
convenience, we introduce new syntax using which the same declarations are
expressed as follows:

effect Fork  : (unit -> unit) -> unit
effect Yield : unit

Effects are performed using the primitive perform of type 'a eff -> 'a. We
define the functions fork and yield as follows:

let fork f = perform (Fork f)
let yield () = perform Yield

What is left is to provide an interpretation of what it means to perform
fork and yield. This interpretation is provided with the help of
handlers.

 1 let run main =
 2   let run_q = Queue.create () in
 3   let enqueue k = Queue.push k run_q in
 4   let rec dequeue () =
 5     if Queue.is_empty run_q then ()
 6     else continue (Queue.pop run_q) ()
 7   in
 8   let rec spawn f =
 9     match f () with
10     | () -> dequeue ()
11     | exception e ->
12         print_string (to_string e);
13         dequeue ()
14     | effect Yield k ->
15         enqueue k; dequeue ()
16     | effect (Fork f) k ->
17         enqueue k; spawn f
18   in
19   spawn main

The function spawn f (line 8) evaluates f in a new thread of control. f
may return normally with value () or exceptionally with an exception e or
effectfully with the effect performed along with the delimited
continuation2 k. In the pattern effect e k, if the
effect e has type 'a eff, then the delimited continuation k has type
('a,'b) continuation, i.e., the return type of the effect 'a matches the
argument type of the continuation, and the return type of the delimited
continuation is 'b.

Observe that we represent the scheduler queue with a queue of delimited
continuations, with functions to manipulate the queue (lines 2--6). In the case
of normal or exceptional return, we pop the scheduler queue and resume the
resultant continuation using the continue primitive (line 6). continue k v
resumes the continuation k : ('a,'b) continuation with value v : 'a and
returns a value of type 'b. In the case of effectful return with Fork f
effect (lines 16--17), we enqueue the current continuation to the scheduler
queue and spawn a new thread of control for evaluating f. In the case of
Yield effect (lines 14--15), we enqueue the current continuation, and resume
some other saved continuation from the scheduler queue.

Testing the scheduler

Lets write a simple concurrent program that utilises this scheduler, to create
a binary tree of tasks. The sources for this test are available
here. The program
concurrent.ml:

let log = Printf.printf

let rec f id depth =
  log "Starting number %i\n%!" id;
  if depth > 0 then begin
    log "Forking number %i\n%!" (id * 2 + 1);
    Sched.fork (fun () -> f (id * 2 + 1) (depth - 1));
    log "Forking number %i\n%!" (id * 2 + 2);
    Sched.fork (fun () -> f (id * 2 + 2) (depth - 1))
  end else begin
    log "Yielding in number %i\n%!" id;
    Sched.yield ();
    log "Resumed number %i\n%!" id;
  end;
  log "Finishing number %i\n%!" id

let () = Sched.run (fun () -> f 0 2)

generates a binary tree of depth 2, where the tasks are numbered as shown
below:



The program forks new tasks in a depth-first fashion and yields when it reaches
maximum depth, logging the actions along the way. To run the program, first
install multicore OCaml compiler, available from the OCaml Labs dev
repo. Once the compiler is
installed, the above test program can be compiled and run as follows:

$ git clone https://github.com/kayceesrk/ocaml-eff-example
$ cd ocaml-eff-example
$ make
$ ./concurrent
Starting number 0
Forking number 1
Starting number 1
Forking number 3
Starting number 3
Yielding in number 3
Forking number 2
Starting number 2
Forking number 5
Starting number 5
Yielding in number 5
Forking number 4
Starting number 4
Yielding in number 4
Resumed number 3
Finishing number 3
Finishing number 0
Forking number 6
Starting number 6
Yielding in number 6
Resumed number 5
Finishing number 5
Finishing number 1
Resumed number 4
Finishing number 4
Finishing number 2
Resumed number 6
Finishing number 6

The output illustrates how the tasks are forked and scheduled.

Implementation

Fibers for Concurrency

The main challenge in the implementation of algebraic effects is the efficient
management of delimited continuations. In multicore OCaml3, the delimited
continuations are implemented using fibers, which are small heap-allocated,
dynamically resized stacks. Fibers represent the unit of concurrency in the
runtime system.

Our continuations are linear (one-shot)4, in that once captured,
they may be resumed at most once. Capturing a one-shot continuation is fast,
since it involves only obtaining a pointer to the underlying fiber, and
requires no allocation. OCaml uses a calling convention without callee-save
registers, so capturing a one-shot continuation requires saving no more context
than that necessary for a normal function call.

Since OCaml does not have linear types, we enforce the one-shot property at
runtime by raising an exception the second time a continuation is invoked. For
applications requiring true multi-shot continuations (such as probabilistic
programming5), we envision providing an explicit operation to copy
a continuation.

While continuation based concurrent functional programming runtimes such as
Manticore and MultiMLton use undelimited continuations, our continuations are
delimited. We believe delimited continuations enable complex nested and
hierarchical schedulers to be expressed more naturally due to the fact that
they introduce parent-child relationship between fibers similar to a function
invocation.

Running on Multiple Cores

Multicore OCaml provides support for shared-memory parallel execution. The unit
of parallelism is a domain, each running a separate system thread, with a
relatively small local heap and a single shared heap shared among all of the
domains. In order to distributed the fibers amongst the available domains, work
sharing/stealing schedulers are initiated on each of the domains. The multicore
runtime exposes to the programmer a small set of locking and signalling
primitives for achieving mutual exclusion and inter-domain communication.

The multicore runtime has the invariant that there are no pointers between the
domain local heaps. However, the programmer utilising the effect library to
write schedulers need not be aware of this restriction as fibers are
transparently promoted from local to shared heap on demand. We will have to
save multicore-capable schedulers for another post.


Eff ↩


Representing Monads ↩


Multicore OCaml (pdf) ↩


Representing Control in the presence of One-shot Continuations ↩


Embedded domain-specific language HANSEI for probabilistic models and (nested) inference ↩




Hide
        
      
                    by KC Sivaramakrishnan at May 20, 2015 
      
      
    
  


       
                  CueKeeper: Gitting Things Done in the browser
      (Thomas Leonard)
    
    
                                Git repositories store data with history, supporting replication, merging and revocation.
The Irmin library lets applications use Git-style storage for their data.
To try it out, I’ve written a GTD-based action tracker that runs entirely client-side in the browser.

CueKeeper uses Irmin to handle history and merges, with state saved in the browser using the new IndexedDB standard (requires a recent browser; Firefox 37, Chromium 41 and IE 11.0.9600 all work, but Safari apparently has problems if you open the page in multiple tabs).

Open interactive version full screen





In the future, I plan to have the browser sync to a master Git repository and use the browser storage only for off-line use, but for now note that:

  All data is stored only in your browser.
  There is no server communication.
  Any changes you make will persist for you, but will not affect other users.
  Mozilla’s IndexedDB docs say that “the general philosophy of the browser vendors is to make the best effo…Read more...Git repositories store data with history, supporting replication, merging and revocation.
The Irmin library lets applications use Git-style storage for their data.
To try it out, I’ve written a GTD-based action tracker that runs entirely client-side in the browser.

CueKeeper uses Irmin to handle history and merges, with state saved in the browser using the new IndexedDB standard (requires a recent browser; Firefox 37, Chromium 41 and IE 11.0.9600 all work, but Safari apparently has problems if you open the page in multiple tabs).

Open interactive version full screen





In the future, I plan to have the browser sync to a master Git repository and use the browser storage only for off-line use, but for now note that:

  All data is stored only in your browser.
  There is no server communication.
  Any changes you make will persist for you, but will not affect other users.
  Mozilla’s IndexedDB docs say that “the general philosophy of the browser vendors is to make the best effort to keep the data when possible”, but vaguely notes that your data may be deleted if you run out of space! If someone can clarify things, that would be great. I’ve been using it for 5 weeks on Firefox, and haven’t lost anything, but it would be nice to know the exact conditions for safety.
  Take backups! On my Linux/Firefox system, the data is stored here: $HOME/.mozilla/firefox/SALT.default/storage/default/http+++roscidus.com/idb
  This is version 0.1 alpha ;-)


This post contains a brief introduction to using GTD and CueKeeper, followed by a look at some nice features that result from using Irmin.
The code is available at https://github.com/talex5/cuekeeper.
Alpha testers welcome!

Table of Contents

  Background          Getting Things Done (GTD)
      Irmin
      mGSD
      Nymote, MirageOS and UCN
    
  
  Using CueKeeper          Core concepts
      Editing items
      Processing
      Work
      Contact
      Schedule
      The weekly review
      The top-right controls
    
  
  Interesting Irmin features          Sync
      History
      Revert
      Check-before-merge
      Out-of-date UI actions
    
  
  Next steps
  Acknowledgements


( this post also appeared on Hacker News and
  Reddit )

Background

Getting Things Done (GTD)

The core idea behind David Allen’s GTD is: the human brain is terrible at remembering things at the right time:

  You go into work in the morning thinking about a phone call you need to make this evening.
  As you read through your emails, you keep reminding yourself to remember the call.
  You’re in a meeting and someone is speaking. You’re thinking you shouldn’t forget the call.
  etc


Maybe you end up remembering and maybe you don’t, but either way you’ve distracted yourself all day from the other things you wanted to work on.

The goal of using GTD is to have a system where:

  the system reminds you of things when you need to be reminded about them, and 
  you trust it enough that your brain can stop thinking about them until then.


There is no reason ever to have the same thought twice, unless you like having that thought.David Allen Getting Things Done

Irmin

Irmin is “a library for persistent stores with built-in snapshot, branching and reverting mechanisms”. It has multiple backends (including one that uses a regular Git repository, allowing you to view and modify your application’s data using the real git commands).

Git’s storage model is useful for many applications because it gives you race-free updates (each worker writes to its own branch and then merges), disconnected operation, history, remote sync and incremental backups.

Using js_of_ocaml I was able to compile Irmin to JavaScript and run it in the browser, adding a new IndexedDB backend.

mGSD

Simon Baird’s mGSD is an excellent GTD system, which I’ve been using for the last few years.
It’s a set of extensions built on the TiddlyWiki “personal wiki” system.
Like CueKeeper, mGSD runs entirely in your browser and doesn’t require a server.
It’s implemented as a piece of self-modifying HTML that writes itself back to your local disk when you save.
That’s pretty scary, but I’ve found it surprisingly robust.



However, it’s largely unmaintained and there were various areas I wanted to improve:

  Browser security
  Over the years, browsers have become more locked down, and no longer allow web-pages to write to the disk,
requiring a browser plugin to override the check.
CueKeeper uses the IndexedDB support that modern browsers provide to store data (mGSD pre-dates IndexedDB).
  History
  Sometimes you click on a button by mistake and have no idea what changed.
Thanks to Irmin, CueKeeper logs all changes and provides the ability to view earlier states.
Also, CueKeeper uses brief animations to make it easier to see what changed.
  Navigation
  Navigation in mGSD can be awkward because overview panels and details are all mixed in together as wiki pages.
With CueKeeper, I’m experimenting with a two-column layout to separate overview pages from the details.
  Safe multi-tab use
  If you accidentally open mGSD in two tabs, changes in one tab will overwrite changes made in the other. CueKeeper uses Irmin to keep multiple tabs in sync, merging changes between them automatically.
  Sync between devices
  There’s no easy way to Sync multiple mGSD instances. CueKeeper doesn’t implement sync yet either, but it should be easy to add (it can sync between tabs already, so the core logic is there).
  Escaping bugs
  mGSD has various bugs related to escaping (e.g. things will go wrong if you use square brackets in a title). CueKeeper uses type-safe TyXML to avoid such problems.
  Stale-display bugs
  mGSD mostly does a good job of keeping all elements of the display up-to-date, but there are some flaws.
For example, if you add a new contact in one panel, then open the contacts menu from another, the new contact doesn’t show up.
CueKeeper uses Functional reactive programming with the React library to make sure everything is current.
  Clean separation of code and data
  As a self-modifying .html file, updating mGSD is terrifying! CueKeeper can be recompiled and reloaded like any other program.


Nymote, MirageOS and UCN

The Nymote project describes itself as “Lifelong control of your networked personal data”:

By adopting large centralised services we’ve answered the call of the siren servers and made an implicit trade. That we will share our habits and data with them in exchange for something useful. In doing so we’ve empowered internet behemoths while simultaneously reducing our ability to influence them. We risk becoming slaves to the current system unless we can create alternatives that compete. It’s time to work on those alternatives.

The idea here is to provide services that people can run in their own homes (e.g. on a PC, a low-powered ARM board, or the house router).
The three key pieces of infrastructure it needs are Mirage, Irmin and Signpost.

I’ve talked about MirageOS before (see My first unikernel): it allows you to run extremely small, highly secure services as Xen guests (a few MB in size, written in type-safe OCaml, rather than 100s of MB you would have with a Linux guest).
I haven’t looked at Signpost yet.
Irmin is the subject of this blog post.

UCN (User Centric Networking) is an EC-funded project that is building a “Personal Information Hub” (PIH), responsible for storing users’ personal data in their home, and then using that data for content recommendation.
If you use Google to manage your ToDo-list then when you add “Book holiday” to it, Google can show you relevant ads.
But what if you want good recommendations without sharing personal data with third parties?
Tools such as CueKeeper could be configured to sync with a local PIH to provide input for its recommendations without the data leaving your home.

Using CueKeeper

You can either use the example on roscidus.com, or download the standalone release cuekeeper-bin-0.1.zip.
To use the release, unzip the directory and open index.html in a browser (no need for a web-server).
If you do this, note that the database is tied to the path of the file, so if you move or rename the directory, it will show a different database (which might make it look like your items have disappeared).

Core concepts

There are five kinds of “thing” in CueKeeper:

  Action
  Something you will do (e.g. “Follow Mirage tutorial”).

    Beside each action you will see some toggles showing its state:
The tick means done,
“n” is a next action (something you could start now),
“w” means waiting-for (something you can’t start now),
“f” means future (something you don’t want to think about yet).
The star is for whatever you want.
Repeating actions can’t be completed, so for those the tick box will be blank.
  
  Project
  Something you want to achieve (e.g. “Make a Mirage unikernel”).

    A project may require several actions to be taken.
The possible states are done (the tick),
“a” for active projects,
and “sm” for “Someday/Maybe” (a project you don’t plan to work on yet).
  
  Area
  An “Area of responsibility” is a way of grouping things (e.g. “Personal/Hobbies” or “Job/Accounts”).

    Unlike projects, areas generally cannot be completed.
One thing that confused me when I started with GTD was that what my organisation called “projects” were actually areas.
If your boss says “You’re working on project X until further notice” then “X” is probably an “area” in GTD terms.
  
  Contact
  Someone you work with.

    You can associate any area, project or action with a contact, which provides a quick way to find all the things you need to discuss with someone when you meet them.
If an action is being performed by someone else, you can also mark it as waiting for them.
It will then appear on the Review/Waiting list.
  
  Context
  Another way of grouping actions, by what kind of activity it is, or where it will occur.

    Assigning a context to an action is an important check that the action isn’t too vague.
Your eye will tend to glide over vague actions like “Sort out car”; choosing a context “Phone” (garage) or “Shopping” (buy tools) forces you to clarify things.
  


Notes:

  GTD also has the concept of a “tickler”. In CueKeeper this is just an action waiting until some time.
  GTD also has “reference material”, but I never used this in mGSD, so I didn’t implement it.
Regular files on your computer seem to work fine for this.
  mGSD has the concept of “realms” to group areas. CueKeeper uses sub-areas for this instead (e.g. CueKeeper’s “Personal/Health” sub-area corresponds to an mGSD “Health” area within a “Personal” realm).


Editing items

Clicking on an item or creating a new one opens a panel showing its details in the right column.
There are various things you can edit here:

  The toggles here work just as elsewhere (see above).
  Click the title to rename.
  Click (edit) to edit the notes.
These can be whatever you like.
They’re in Markdown format, so you can add structure, links, etc.
  Click (add log entry) to start editing with today’s date added at the end.
This is convenient to add date-stamped notes quickly.
  The (delete) button at the bottom will remove it (without confirmation; use Show history to revert accidental deletions, as explained later).


For areas, projects and actions:

  You can convert between these types by clicking on the type (e.g. “An action in …”).
This is useful if e.g. you realise that an action is really a project with multiple steps.
  Click on the parent to move it to a different parent.
  You can set the contact field for any of these types too.


For actions, you can also set the context, which is useful for grouping actions on the Work page, and helps to make sure the action is well-defined.

You can also make an action repeat.
Setting the repeat for an action will move it to the waiting state until the given date.
There are only two differences between repeating actions and regular (one-shot) scheduled actions:

  You can’t mark a repeating action as done (clear the repeat first if you want to).
  When you click on the “w” on a repeating action, the next repeat date after it was last scheduled is highlighted by default. If that date has already arrived, it keeps moving it forward by the specified interval until it’s in the future.


Processing

There are several stages to applying GTD, corresponding to the tabs along the top.
The first is processing, which is about going through your various inboxes (email, paper, voicemail, etc) and determining what actions each item requires.
After processing, your inbox should be empty and everything you need to do either done (for quick items) or recorded in CueKeeper.
Also, see if you can think of any projects or actions that are only in your head and add those too.

  Click the + next to an area and enter a name for the new project.(the Work tab will go red at this point, indicating an alert: “Active project with no next action”)
  Click (edit) in the new project panel to add some details, if desired.
  Click +action to add the next action to perform towards this project.


Note that it is not necessary to add all the actions needed to complete the project.
Just add the next thing that you can do now.
When you later mark the action as done, CueKeeper will then prompt you to think about a new next action.

If a project will only require a single action (e.g. “Buy milk”), then instead of adding a project and an action, you can just convert the new project to an action and not bother about having a project at all.

If you don’t plan to work on the project soon, click “sm” to convert it to a “Someday/Maybe” project.

Work

This is the default view, showing all the things you could be working on now.

The filters just below the tab allow you to hide top-level areas (e.g. if you don’t want to see any personal actions while you’re at work).

When an item is done, click on the tick mark.

If it’s not possible to start it now, click on the “w” to mark it as waiting:

  If an action is waiting for someone else, first add them as the contact, then click the w and select “Waiting for name” from the menu.
  If an action can’t be started until some date, click the “w” and choose the date from the popup calendar.
  Otherwise, you can mark it as “Waiting (reason unspecified)”.


If you’re not going to do it this week, click on the “f” (future) to defer it until the next review.

Contact

This view lists your contacts and any actions you’re waiting for them to do.
It’s useful if someone phones and you want to see everything you need to discuss with them, for example.
The list only shows actions you’re actually waiting for, but if you open up a particular contact then you’ll also see things they’re merely associated with.

Schedule

Lists actions than can’t be done until some date.

When due, scheduled actions will appear highlighted on the Work tab (even if their area is filtered out).
If you pin the browser tab showing the CueKeeper page, the tab icon will also go red to indicate attention is needed.
If you want to test the effect, schedule an action for a date in the past.
Click n to acknowledge a due action and convert it to a next action.

The weekly review

GTD only works if you trust yourself to look at the system regularly.
There are various reports available under the Review tab to help with this.



The available reports are:

  Done shows completed actions and projects and provides a button to delete them all.
If you’re the sort of person who likes to write weekly summaries, this might be useful input to that.
  Waiting shows actions that are waiting for someone or something (but not scheduled actions).
You might want to check up on the status of these, or do something to unblock them.
  Future shows all actions you marked as “Future” and all projects you marked as “Someday/Maybe”.
  Areas lists all your areas of responsibility.
  Everything shows every item in the system in one place (you don’t need to review this; it’s just handy sometimes to see everything).


mGSD has more reports, but these are the ones I use.
The default configuration has a repeating action scheduled for next Sunday to review things.
This is what I do:

  Process tab Empty inboxes, adding any actions to CueKeeper:
          email inbox
      paper inbox
    
  
  Review/Done
          Admire done list, then delete all.
    
  
  Review/Waiting
          Any reminders needed?
    
  
  Review/Future
          Make any of these current?
      Delete any that will never get done.
    
  
  Review/Areas
          Any areas that need new projects?
    
  
  Work
          Make sure each action is obvious (not vague).
      Could it be started now? Set to Waiting if not.
      List too long? Mark some actions as Future, or their projects as Someday/Maybe.
    
  


It’s important to look at all these items during the review.
Knowing you’re going to look at each waiting or future item soon is what allows you to forget about them during the rest of the week!

The top-right controls



To search, enter some text (or a regular expression) into the box and select from the drop-down menu that appears.
Pressing Return opens the first result.

To create a new items, enter a label for it and select one of the “Add” items from the menu.
Pressing Return when there are no search results will create a new action.

Export allows you to save the current state (without history) as a tar file.
There’s no import feature currently, though.

Show history shows some recent entries from the Irmin log (see below).

Interesting Irmin features

So, what benefits do we get from using Irmin?

Sync

The first benefit, of course, is that we can synchronise between multiple instances.
You may have already tried opening CueKeeper in two windows (of the same browser) and observed that changes made in one propagate to the other.
Here’s an easier way to experiment with sync (click the screenshot for the interactive version):



This page has two instances of CueKeeper running, representing two separate devices such as a laptop and mobile phone.
You can edit them separately and then click the buttons in the middle to see how the changes are merged.

Clicking Upper to lower pushes all changes from the upper pane to the lower (the lower instance will merge them with its current state). Clicking Lower to upper does the reverse. A full sync would do these two in sequence, but of course it could be interrupted part way through.

The “Criss-cross” button can be used to test the unusual-but-interesting case of merging in both directions simultaneously (i.e. each instance merges with the previous state of the other instance, generating two new merges). CueKeeper tries to merge deterministically, so that both instances should end up in the same state, avoiding unnecessary conflicts on future merges.

Where you make conflicting edits, CueKeeper will pick a suitable resolution and add a conflict note to say what it did.
For example, if you edit the title of the “Try OCaml tutorials” action to different strings in each instance and then sync, you’ll see something like:



CueKeeper uses a three-way merge - the merge algorithm takes the states of the two branches to be merged and their most recent common ancestor, and generates a new commit from these.
The common ancestor is used to determine which branch changed which things (anything that is the same as in the common ancestor wasn’t changed on that branch).
If there are multiple possible ancestors (which can happen after a criss-cross merge) we just pick one of them.

CueKeeper has a unit test for merging that repeatedly generates three commits at random and ensures the merge code produces a valid (loadable) result.
This should ensure that we can merge any pair of states, but it can’t check that the result will necessarily seem sensible to a human, so let me know if you spot anything odd!

History

We have the full history, which you can view with the Show history button:



The history view is useful if you clicked on something by accident and you’re not sure what you did.
Click on an entry to see the state of the system just after that change.
A box appears at the top of the page to indicate that you’re in “time travel” mode - close the box to return to the present.

If you edit anything while viewing a historical version, CueKeeper will commit against that version and then merge the changes to master and return to the present.

You might like to open each instance’s history panel while trying the sync demo above.

Revert

When in time-travel mode, you can click on the Revert this change button there to undo the change.

Reverting was easy to add, as it reuses the existing three-way merge code.
The only difference is that the “common ancestor” is the commit being reverted and the parent of that commit is used as the “branch” to be merged.

Because CueKeeper can merge any three commits, it can also revert any commit (with a single parent), although you’ll get the most sensible results if you revert the most recent changes first.

For example, if you create an action and then modify it, and then revert the creation then CueKeeper will see that as:

  One branch that modified the action (the main branch).
  One branch that deleted the action (the revert of the creation).


When something is modified and deleted, CueKeeper will opt to keep it, so the effect of the “revert” will simply be to add a note that it decided to keep it.
Of course, the sensible way to delete something is to use the regular (delete) button.

Check-before-merge

It’s important to make sure that the system doesn’t get into an inconsistent state, and Irmin can help here.
Whenever CueKeeper updates the database, it first generates the new commit, then it loads the new commit to check it works, then it updates the master branch to point at the new commit.

This means that CueKeeper will never put the master branch into a state that it can’t itself load.

Out-of-date UI actions

Perhaps the most interesting effect of using Irmin is that it eliminates various edge cases related to out-of-date UI elements.
Consider this example:

  You open a menu in one tab to set the contact for an action.
  You delete one of the contacts in another tab.
  You choose the deleted contact from the menu in the first tab.


With a regular database, this would probably result in some kind of error that you’d need to handle.
These edge cases don’t occur often and are hard to test.

With CueKeeper though, we record which revision each UI element came from and commit against that revision.
We then merge the new commit with the master branch, using the existing merge logic to deal with any problems (normally, there is nothing to merge and we do a trivial “fast-forward” merge here).
This means we never have to worry about concurrent updates.

A similar system is used with editable fields.
When you click on a panel’s title to edit it, make some changes and press Return, we commit against the version you started editing, not the current one.
This means that CueKeeper won’t silently overwrite changes, even if you edit something in two tabs at the same time (you’ll get a merge conflict note containing the version it discarded instead).

Next steps

If you’d like to help out, there’s still plenty more to do, both coding and testing.
For example:

  Editing doesn’t work well on mobile phones. Menus and input boxes should fill the screen in this case.
  I’ve had reports that merging between tabs is unreliable on Safari for some reason (AbortError from IndexedDB).
  It would be good to use pack files for compression. Needs a JavaScript compression library.
  It should be possible for an action to be marked as waiting for some other action or project to be completed.
  Remote sync needs to be implemented.
  The UI needs some work. In particular, could someone find a tasteful way to style the fields in the panels so they look like drop-downs? I keep clicking on the item’s name instead of the (show) button by mistake (although this might be because I used to have it the other way around, with a (change) button, but that was worse).
  CueKeeper’s IndexedDB Irmin backend should be split off so other people can use it easily.


If you’d like to help out, the code is available at https://github.com/talex5/cuekeeper and discussion happens on the MirageOS-devel mailing list.
If there’s interest, I may write a follow-up post documenting my experiences implementing CueKeeper (using Irmin, React, js_of_ocaml and IndexedDB).

Acknowledgements

Some of the research leading to these results has received funding from the European Union’s Seventh Framework Programme FP7/2007-2013 under the UCN project, grant agreement no 611001.

Hide
        
      
                    by Thomas Leonard at Apr 28, 2015 
      
      
    
  


       
          Let's Play Network Address Translation: The Home Game
      (Mindy Preston)
    
                                When last we spoke, I left you with a teaser about writing your own NAT implementation. iptables (and friends nftables and pf, to be a little less partisan and outdated) provide the interfaces to the kernel modules that implement NAT in many widely-used routers. If we wanted to implement our own in a traditional OS, we’d have to either take a big dive into kernel programming or find a way to manipulate packets at the Ethernet layer in userspace.But if all we need to do is NAT traffic, why not just build something that only knows how to NAT traffic? I’ve looked at building networked applications on top of (and with) the full network stack provided by the MirageOS library OS a lot, but we can also build lower-level applications with fundamentally the same programming tactics and tools we use to write, for example, DNS resolvers.Building A Typical Stack From ScratchLet’s have a look at the ethif-v4 example in the mirage-skeleton example repository. This example unikernel shows how t…Read more...When last we spoke, I left you with a teaser about writing your own NAT implementation. iptables (and friends nftables and pf, to be a little less partisan and outdated) provide the interfaces to the kernel modules that implement NAT in many widely-used routers. If we wanted to implement our own in a traditional OS, we’d have to either take a big dive into kernel programming or find a way to manipulate packets at the Ethernet layer in userspace.But if all we need to do is NAT traffic, why not just build something that only knows how to NAT traffic? I’ve looked at building networked applications on top of (and with) the full network stack provided by the MirageOS library OS a lot, but we can also build lower-level applications with fundamentally the same programming tactics and tools we use to write, for example, DNS resolvers.Building A Typical Stack From ScratchLet’s have a look at the ethif-v4 example in the mirage-skeleton example repository. This example unikernel shows how to build a network stack “by hand” from a bunch of different functors, starting from a physical device (provided by config.ml at build time, representing either a Xen backend if you configure with mirage configure --xen or a Unix tuntap backend if you build with mirage configure --unix). I’ve reproduced the network setup bits from the most recent version as of now and annotated them a bit:```ocaml module Main (C: CONSOLE) (N: NETWORK) (Clock : V1.CLOCK) = struct (* N, a module of type NETWORK (defined in module V1_LWT from mirage-types), is the building point for the
 rest of our stack.  Modules E, I, U, and T provide
 functions like [write], which take a record of the type
 matching the module (e.g., E.write needs an E.t argument)
 along with some information to write and generate a
 reasonable set of headers of the appropriate layer before
 calling a lower-level [write] function.
 *)
 module E = Ethif.Make(N) module I = Ipv4.Make(E) module U = Udp.Make(I) (* Ethernet, Ipv4, and UDP don’t need outside timers or randomness, just an underlying implementation to listen from and write to,
 but TCP does *)
 module T = Tcp.Flow.Make(I)(OS.Time)(Clock)(Random)  ( DHCP also needs timers and randomness ) module D = Dhcp_clientv4.Make©(OS.Time)(Random)(U) let or_error c name fn t =fn t
>>= function
| `Error e -> fail (Failure ("Error starting " ^ name))
| `Ok t -> return t
 let start c net _ = ( net is of type N.t )or_error c "Ethif" E.connect net
>>= fun e ->
(* e is of type Ethif.t, on which we can call
ethernet-level listen and write *)

or_error c "Ipv4" I.connect e
>>= fun i ->
(* we can manually set IP options here for interface i,
   in addition to overwriting them (potentially) with
   DHCP below *)
I.set_ip i (Ipaddr.V4.of_string_exn "10.0.0.2")
>>= fun () ->
I.set_ip_netmask i (Ipaddr.V4.of_string_exn "255.255.255.0")
>>= fun () ->
I.set_ip_gateways i [Ipaddr.V4.of_string_exn "10.0.0.1"]
>>= fun () ->
or_error c "UDPv4" U.connect i
>>= fun udp ->
let dhcp, offers = D.create c (N.mac net) udp in
or_error c "TCPv4" T.connect i
>>= fun tcp ->
(* main body of code continues... *)
```The code doesn’t do much once it’s built the stack — just prints lines to the console when various types of traffic are received — so I’ve elided that portion from the reproduction here. If we wanted to work with an Ethif.t (a type representing the Ethernet layer communications on that interface), an I.t (the IP layer), or even the raw physical device passed to the start function with the name of net, we can do that just as we can work with tcp or udp.Working with Multiple Network InterfacesWorking with two interfaces rather than one is fairly similar. A nice minimal example, working right down on the netif layer, is the netif-forward example unikernel, also in mirage-skeleton. The config.ml for this unikernel defines two interfaces, and unikernel.ml provides a module Main functorized over two modules of type NETWORK – there’s no expectation that these are necessarily the same type of physical interface, just that they both know how to satisfy the basic operations required of a network device.Instead of building something on top of the provided netifs, netif-forward (as of the latest revision) works with them directly — it takes packets from the first interface (n1, of type N1.t), queues them, and then sends them out the second interface (n2, of type N2.t) as quickly as it can.```ocaml module Main (C: CONSOLE)(N1: NETWORK)(N2: NETWORK) = struct let (in_queue, in_push) = Lwt_stream.create ()  let (out_queue, out_push) = Lwt_stream.create () let listen nf =let hw_addr =  Macaddr.to_string (N1.mac nf) in
let _ = printf "listening on the interface with mac address '%s' \n%!" hw_addr in
N1.listen nf (fun frame -> return (in_push (Some frame)))
 let update_packet_count () =let _ = packets_in := Int32.succ !packets_in in
let _ = packets_waiting := Int32.succ !packets_waiting in
if (Int32.logand !packets_in 0xfl) = 0l then
    let _ = printf "packets (in = %ld) (not forwarded = %ld)" !packets_in !packets_waiting in
    print_endline ""
 let start console n1 n2 =let forward_thread nf =
  while_lwt true do
    lwt _ = Lwt_stream.next in_queue >>= fun frame -> return (out_push (Some frame)) in
    return (update_packet_count ())
  done
  <?> (
  while_lwt true do
    lwt frame = Lwt_stream.next out_queue in
      let _ = packets_waiting := Int32.pred !packets_waiting in
      N2.write nf frame
  done
  )
 in  (listen n1) <?> (forward_thread n2)return (print_endline “terminated.”)end  ```Building a NAT Library and UnikernelFor our NAT implementation, we need to be able to:make reference to the publicly-routable IP address on the Internet-facing interfacegenerate new and unique port numbers to use to disambiguate traffic from different hosts on the private network sidekeep a table mapping private-network connections to their public-network analogsadd new entries to the table based on new connection attemptsalter Ethernet, IP, TCP, and UDP headers of incoming and outgoing packets:replace ip addresses and ports according to table entriesrecalculate checksums on IP and transport layers after making other mutationsSince there’s nothing privileged about any of the data structures we’re using, or the memory we’re accessing, it’s relatively straightforward to pull the packet-transformation and inspection code out into a simple library that does the following:decomposes incoming packets into either a tuple of the relevant layers or Nonepulls relevant information for NAT decision-making (Ethernet layer ethertype, IP-layer source and destination address and protocol, transport-layer port numbers) out of packet layersgiven an existing NAT table and an incoming packet, either rewrites the packet according to the rules in the table or returns Nonegiven an existing NAT table and an incoming packet, along with an IP address and port number, creates a new NAT table rule for the packet using the IP address and port number providedAlong with a library that provides basic CRUD operations on the table itself, this is enough to get Internet browsing working through a NATting unikernel with not much code at all. If you’d like to try it out, here are some instructions on setting up a Xen machine to NAT via mirage-nat. The instructions given are for a CubieBoard2 or CubieTruck, but any machine running Xen with multiple network interfaces (or even virtual bridges, if you wish to NAT nonphysical devices) can run the NATting unikernel.Some Comments on Limitations of the ImplementationThis is not enough to have a stable or even reasonably secure Internet browsing through a NATting unikernel, largely because there’s no nice facility for table entries to be removed. This has two important consequences:the NAT table will grow until it consumes all available memory and the NAT device crashes. This mimics the behavior of many commercial implementations (memory exhaustion due to NAT table size is a common reason you need to restart your home router), but in this case that isn’t a feature.the NAT table will allow servers which previously replied to requests, to send new traffic to the host which made the original request. In other words, if a client made an unencrypted HTTP request to the-toast.net, downloaded a webpage, and then closed the connection three days ago, the NAT device has no way of knowing that the-toast.net shouldn’t be sending responses now. This is particularly bad in the case of UDP, which has fewer protocol-level safeguards against state-violating traffic.There’s nothing about the MirageOS architecture that imposes these limitations — code which times out and maintains state is already implemented in MirageOS. Ideally, we’d want to make use of (the state machine logic for TCP connections) already included in the mirage-tcpip library, so we could continue to use the power of our library OS architecture to avoid duplicating this code. We’d be stuck writing our own UDP “connection” expiry logic no matter what, since UDP is a connectionless protocol, although we could provide those as a library as well — perhaps a firewalling unikernel might be able to use this code in the future?AcknowledgementsSome of the research leading to these results has received funding from the European Union’s Seventh Framework Programme FP7/2007-2013 under the UCN project, grant agreement no 611001. Hide
        
      
                    by Mindy Preston at Apr 06, 2015 
      
      
    
  


       
                  Towards Heroku for Unikernels: Part 2 - Self Scaling Systems
      (Amir Chaudhry)
    
    
                                In the previous post I described the continuous end-to-end system
that we’ve set up for some of the MirageOS projects — automatically going from
a git push all the way to live deployment, with everything under
version-control.

Everything I described previously already exists and you can set up the
workflow for yourself, the same way many others have done with the Travis CI
scripts for testing/build.  However, there are a range of exciting
possibilities to consider if we’re willing to extrapolate just a little from
the tools we have right now.  The rest of this post explores these ideas and
considers how we might extend our system.  

Previously, we had finished the backbone of the workflow and I discussed a few
ideas about how we should flesh it out — namely more testing and some form of
logging/reporting.  There’s substantially more we could do when we consider
how lean and nimble unikernels are, especially if we speculate about the
systems we could create as our toolstackRead more...In the previous post I described the continuous end-to-end system
that we’ve set up for some of the MirageOS projects — automatically going from
a git push all the way to live deployment, with everything under
version-control.

Everything I described previously already exists and you can set up the
workflow for yourself, the same way many others have done with the Travis CI
scripts for testing/build.  However, there are a range of exciting
possibilities to consider if we’re willing to extrapolate just a little from
the tools we have right now.  The rest of this post explores these ideas and
considers how we might extend our system.  

Previously, we had finished the backbone of the workflow and I discussed a few
ideas about how we should flesh it out — namely more testing and some form of
logging/reporting.  There’s substantially more we could do when we consider
how lean and nimble unikernels are, especially if we speculate about the
systems we could create as our toolstack matures.  A couple of
things immediately come to mind.  

The first is the ability to boot a unikernel only when it is required, which
opens up the possibility of highly-elastic infrastructure.  The second is the
ease with which we can push, pull or otherwise distribute unikernels
throughout a system, allowing new forms of deployment to both cloud and
embedded systems. We’ll consider these in turn and see where they take us,
comparing with the current ‘mirage-decks’ deployment I described in
Part 1.

Demand-driven clouds

The way cloud services are currently provisioned means that you may have
services operating and consuming resources (CPU, memory, etc), even when there
is no demand for them. It would be significantly more efficient if we could
just activate a service when required and then shut it down again when the
demand has passed.  In our case, this would mean that when a unikernel is
‘deployed to production’, it doesn’t actually have to be live — it merely
needs to be ready to boot when demand arises.  With tools like
Jitsu (Just-In-Time Summoning of Unikernels), we can work
towards this kind of architecture. 

Summon when required

Jitsu allows us to have unikernels sitting in storage then ‘summon’ them into
existence.  This can occur in response to an incoming request and with no
discernible latency for the requester. While unikernels are inactive, they
consume only the actual physical storage required and thus do not take up any
CPU cycles, nor RAM, etc. This means that more can be achieved with fewer
resources and it would significantly improve things like utilization rates of
hardware and power efficiency.

In the case of the decks.openmirage.org unikernel that I
discussed last time, it would mean that the site would only come online if
someone had requested it and would shut down again afterwards.  

In fact, we’ve already been working on this kind of system and
Jitsu will be presented at NSDI in Oakland, California this May.
In the spirit of looking ahead, there’s more we could do.


Hyper-elastic scaling

At the moment, Jitsu lets you set up a system where unikernels will boot in
response to incoming requests.  This is already pretty cool but we could take
this a step further.  If we can boot unikernels on demand, then we could use
that to build a system which can automate the scale-out of those services to
match demand.  We could even have that system work across multiple machines,
not just one host.  So how would all this look in practice for ‘mirage-decks’?

Auto-scaling and dispersing our slide decks

Our previous toolchain automatically boots the new unikernel as soon as it is
pulled from the git repo.  Using Jitsu, our deployment machine would pull the
unikernel but leave it in the repo — it would only be activated when someone
requests access to it.  Most of the time, it may receive no traffic and
therefore would remain ‘turned off’ (let’s ignore webcrawlers for now). When
someone requests to see a slide deck, the unikernel would be booted and
respond to the request.  In time it can be turned off again, thus freeing
resources.  So far, so good.

Now let’s say that a certain slide deck becomes really popular (e.g. posted
to HackerNews or Reddit).  Suddenly, there are many incoming requests and we
want to be able to serve them all.  We can use the one unikernel, on one
machine, until it is unable to handle the load efficiently.  At this point,
the system can create new copies of that unikernel and automatically balance
across them. These unikernels don’t need to be on the same host and we should
be able to spin them up on different machines.

To stretch this further, we can imagine coordinating the creation of those new
unikernels nearer the source of that demand, for example starting off on a
European cloud, then spinning up on the East coast US and finally over to the
West coast of the US.  All this could happen seamlessly and the process can
continue until the demand passes or we reach a predefined limit — after all,
given that we pay for the machines, we don’t really want to turn a Denial of
Service into a Denial of Credit. 

After the peak, the system can automatically scale back down to being largely
dormant — ready to react when the next wave of interest occurs.

Can we actually do this?

If you think this is somewhat fanciful, that’s perfectly understandable — as I
mentioned previously, this post is very much about extrapolating from where
the tools are right now.  However, unikernels actually make it very easy to
run quick experiments which indicate that we could iterate towards what I’ve
described.  

A recent and somewhat extreme experiment ran a
unikernel VM for each URL.  Every URL on a small static
site was served from its own, self-contained unikernel, complete with it’s own
web server (even the ‘rss.png’ icon was served separately).  You can read the
post to see how this was done and it also led to an interesting
discussion on the mailing list (e.g. if you’re only serving a
single item, why use a web server at all?).  Of course, this was just an
experiment but it demonstrates what is possible now and how we can iterate,
uncover new problems, and move forward.  One such question is how to
automatically handle networking during a scale-out, and this is an area were
tools like Signpost can be of use.

Overall, the model I’ve described is quite different to the way we currently
use the cloud, where the overhead of a classic OS is constantly consuming
resources.  Although it’s tempting to stick with the same frame of reference
we have today we should recognise that the current model is inextricably
intertwined with the traditional software stacks themselves.  Unikernels allow
completely new ways of creating, distributing and managing software and it
takes some thought in order to fully exploit their benefits. 

For example, having a demand-driven system means we can deliver more services
from just the one set of physical hardware — because not all those services
would be consuming resources at the same time.  There would also be a dramatic
impact on the economics, as billing cycles are currently measured in hours,
whereas unikernels may only be active for seconds at a time.  In addition to
these benefits, there are interesting possibilities in how such scale-outs can
be coordinated across different devices.

Hybrid deployments

As we move to a world with more connected devices, the software and services
we create will have to operate across both the cloud and embedded systems.
There have been many names for this kind of distributed system, ranging from
ubiquitous computing to dust clouds and the ‘Internet of Things’ but they all
share the same idea of running software at the edges of the network (rather
than just cloud deployments).

When we consider the toolchain we already have, it’s not much of a stretch to
imagine that we could also build and store a unikernel for ARM-based
deployments.  Those unikernels can be deployed onto embedded devices and
currently we target the Cubieboard2.  





We could make such a system smarter. Instead of having the edge devices
constantly polling for updates, our deployment process could directly push
the new unikernels out to them. Since these devices are likely to be behind
NATs and firewalls, tools like Signpost could deal with the issue
of secure connectivity. In this way, the centralized deployment process
remains as a coordination point, whereas most of the workload is dealt with by
the devices the unikernels are running on.  If a central machine happens to be
unavailable for any reason, the edge-devices would continue to function as
normal.  This kind of arrangement would be ideal for Internet-of-Things style
deployments, where it could reduce the burden on centralised infrastructure
while still enabling continuous deployment.

In this scenario, we could serve the traffic for ‘mirage-decks’ from a
unikernel on a Cubieboard2, which could further minimise the cost of running
such infrastructure.  It could be configured such that if demand begins to
peak, then an automated scale-out can occur from the Cubieboard2 directly out
onto the public cloud and/or other Cubieboards. Thus, we can still make use
of third-party resources but only when needed and of the kind we desire.  Of
course, running a highly distributed system leads to other needs.

Remember all the things

When running services at scale it becomes important to track the activity and
understand what is taking place in the system. In practice, this means logging
the activity of the unikernels, such as when and where they were created and
how they perform.  This becomes even more complex for a distributed system.

If we also consider the logging needs of a highly-elastic system, then another
problem emerges.  Although scaling up a system is straightforward to
conceptualise, scaling it back down again presents new challenges.  Consider
all the additional logs and data that have been created during a scale-out — 
all of that history needs to be merged back together as the system contracts.
To do that properly, we need tools designed to manage distributed data
structures, with a consistent notion of merges.

Irmin addresses these kinds of needs and it enables a style of
programming very similar to the Git workflow, where distributed nodes fork,
fetch, merge and push data between each other.  Building an end-to-end logging
system with Irmin would enable data to be managed and merged across different
nodes and keep track of activity, especially in the case of a scale down. The
ability to capture such information also means the opportunity to provide
analytics to the creators of those unikernels around performance and usage
characteristics. 

The use of Irmin wouldn’t be limited to logging as the unikernels themselves
could use it for managing data in lieu of other file systems.  I’ll refrain
from extrapolating too far about this particular tool as it’s still under
rapid development and we’ll write more as it matures.



On immutable infrastructure

You may have noticed that one of the benefits of the unikernel approach arises
because the artefacts themselves are not altered once they’re created. 
This is in line with the recent resurgence of ideas around ‘immutable
infrastructure’.  Although there isn’t a precise definition of this, the
approach is that machines are treated as replaceable and can be regularly re
provisioned with a known state.  Various tools help the existing systems to
achieve this but in the case of unikernels, everything is already under
version control, which makes managing a deployment much easier.

As our approach is already compatible with such ideas, we can take it a step
further.  Immutable infrastructure essentially means the artefact produced
doesn’t matter. It’s disposable because we have the means to easily recreate
it.  In our current example, we still ship the unikernel around.  In order to
make this ‘fully immutable’, we’d have to know the state of all the packages
and code used when building the unikernel. That would give us a complete
manifest of which package versions were pulled in and from which sources. 
Complete information like this would allow us to recreate any given unikernel
in a highly systematic way.  If we can achieve this, then it’s the manifest
which generates everything else that follows.

In this world-view, the unikernel itself becomes something akin to caching.
You use it because you don’t want to rebuild it from source — even though
unikernels are quicker to build than a whole OS/App stack.  For more security
critical applications, you may want to be assured of the code that is pulled
in, so you examine the manifest file before rebuilding for yourself. This also
allows you to pin to specific versions of libraries so that you can explicitly
adjust the dependencies as you wish.  So how do we encode the manifest?  This
is another area where Irmin can help as it can keep track of the state of
package history and can recreate the environment that existed for any given
build run.  That build run can then be recreated elsewhere without having to
manually specify package versions.  

There’s a lot more to consider here as this kind of approach opens up new
avenues to explore. For the time being, we can recognise that the unikernel
approach lends itself to the achieving immutable infrastructure.

What happens next?

As I mentioned at the beginning of this post, most of what I’ve described is
speculative. I’ve deliberately extrapolated from where the tools are now so as
to provoke more thoughts and discussion about how this new model can be used
in the wild.  Some of the things we’re already working towards but there are
many other uses that may surprise us — we won’t know until we get there and
experimenting is half the fun.

We’ll keep marching on with more libraries, better tooling and improving
quality.  What happens with unikernels in the rest of 2015 is largely up to
the wider ecosystem.  

That means you.


Thanks to Thomas Gazagnaire and Richard Mortier for comments on an earlier draft.




Hide
        
      
                    by Amir Chaudhry at Apr 03, 2015 
      
      
    
  


       
                  Apr 2015 news update
      (OCL Monthly News)
    
    
                                      The OCaml Labs initiative within the Cambridge Computer Laboratory
is now just over two years old, and it is time for an update about our  activities
since the last update at the end of 2013 and 2012.
The theme of our group was not to be pure research, but rather a hybrid group that takes on
some of the load of day-to-day OCaml maintenance from INRIA,
as well as help grow the wider community and meet our own research agendas around
topics such as unikernels.
To this end, all of our projects have been highly collaborative, often involving colleagues
from OCamlPro, INRIA,
Jane Street, Lexifi and Citrix.
This post covers our progress in tooling, the compiler and language, community efforts, research projects and concludes with our priorities for 2015.
Tooling

At the start of 2014, we had just helped to release
OPAM 1.1.1 with our colleagues
at OCamlPro, and serious OCaml users had just started moving
over to using it.
Our overall goal at OCaml Labs is to deliver a modular set of o…Read more...      The OCaml Labs initiative within the Cambridge Computer Laboratory
is now just over two years old, and it is time for an update about our  activities
since the last update at the end of 2013 and 2012.
The theme of our group was not to be pure research, but rather a hybrid group that takes on
some of the load of day-to-day OCaml maintenance from INRIA,
as well as help grow the wider community and meet our own research agendas around
topics such as unikernels.
To this end, all of our projects have been highly collaborative, often involving colleagues
from OCamlPro, INRIA,
Jane Street, Lexifi and Citrix.
This post covers our progress in tooling, the compiler and language, community efforts, research projects and concludes with our priorities for 2015.
Tooling

At the start of 2014, we had just helped to release
OPAM 1.1.1 with our colleagues
at OCamlPro, and serious OCaml users had just started moving
over to using it.
Our overall goal at OCaml Labs is to deliver a modular set of of development tools around
OCaml that we dub the OCaml Platform.  The remainder of 2014 was thus spent polishing
this nascent OPAM release into a solid base (both as a command-line tool and as a library)
that we could use as the basis for documentation, testing and build infrastructure, all
the while making sure that bigger OCaml projects continued to migrate over to it.
Things have been busy; here are the highlights of this effort.
OPAM

The central OPAM repository that 
contains the package descriptions has grown tremendously in 2014, with over
280 contributors committing almost 10000 changesets across 3800 
pull requests on GitHub.
The front line of incoming testing has been continuous integration by the wonderful
Travis CI, who also
granted us access to their experimental MacOS X 
build pool.  The OPAM package team also to expanded to give David Sheets, Jeremy Yallop, 
Peter Zotov and Damien Doligez commit rights, and they have all been busily triaging new packages
as they come in.
Several large projects such as Xapi, Ocsigen and
our own MirageOS switched over to using OPAM
for day-to-day development, as well as prolific individual developers such as 
Daniel Buenzli and Markus Mottl.
Jane Street continued to send
regular monthly updates 
of their Core/Async suite, and releases appeared from the 
Facebook open-source team
as well (who develop Hack,
Flow and Pfff in OCaml).
We used feedback from the users to smooth away many of the rough edges, with:
a redesigned development workflow that
 lets developers quickly grab a development version of a library  recompile all 
 dependent packages automatically, and quickly publish results to GitHub.binary distributions for common OS distributions via their 
 native packaging, as well 
 as 0install and 
 Vagrant boxes.a unified way of cloning the source of any package via opam source.  This
 handles any supported OPAM archive, including Git, Mercurial or Darcs remotes. a richer package metadata, including source code, development archives and
 bug report URLs.

These changes were all incorporated into the OPAM 1.2,
along with backwards compatibility shims to keep the old 1.1 metadata format working until
the migration is complete.  The 1.2.x series has been a solid and usable
development manager, and last week's release of OPAM 1.2.1 has further polished the core scripting engine.
Platform Blog

One of the more notable developments during 2014 was the 
adoption of OPAM further up the 
ecosystem by the Coq theorem prover.  This broadening of the
community prompted us to create an official OPAM blog to give
us a central place for new and tips, and we've had posts about
XenServer developments, 
the Merlin IDE tool
and the modern UTop interactive REPL.
If you are using OPAM in an interesting or production capacity, please do
get in touch so that we can work
with you to write about it for the wider community.
The goal of the blog is also to start bringing together the various
components that form the OCaml Platform.  These are designed to be
modular tools (so that you can pick and choose which ones are necessary
for your particular use of OCaml).  There are more details available
from the OCaml Workshop presentation at ICFP 2014 (abstract,
slides,
video).
Onboarding New Users

OPAM has also been adopted now by several big universities
(including us at Cambridge!)
for undergraduate and graduate Computer Science courses.  The demands increased for
an out-of-the-box solution that makes it as easy possible for new users to
get started with minimum hassle.
We created a dedicated teaching list
to aid collaboration, and a list of teaching resources on ocaml.org
and supported several initiatives in collaboration with
Louis Gesbert at OCamlPro, as usual with OPAM development).
The easiest way to make things "just work" are via regular binary builds of the
latest releases of OCaml and OPAM on Debian, Ubuntu, CentOS and Fedora, via
Ubuntu PPAs 
and the OpenSUSE Build Service repositories.
Our industrial collaborators from Citrix, Jon Ludlam 
and Dave Scott began an 
upstreaming initiative to Fedora
and sponsored the creation of a CentOS SIG
to ensure that binary packages remain up-to-date.  We also contribute to the hardworking packagers
on MacOS X, Debian, FreeBSD, NetBSD and OpenBSD where possible as well to ensure that
binary builds are well rounded out.  Richard Mortier also assembled Vagrant boxes
that contain OCaml, for use with VirtualBox.
Within OPAM itself, we applied polish to the handling of external dependencies
to automate checking that the system libraries required by OPAM are present.  Two emerging tools that should
help further in 2015 are the opam-user-setup and
OPAM-in-a-box plugins that automate first-time configuration.
These last two are primarily developed at OCamlPro, with design input and support from OCaml Labs.
We do have a lot of work left to do with making the new user experience really
seamless, and help is very welcome from anyone who is interested.  It often helps
to get the perspective of a newcomer to find out where the stumbling blocks are, and
we value any such advice.  Just mail opam-devel@lists.ocaml.org
with your thoughts, or create an issue on how we
can improve.  A particularly good example of such an initiative was started by 
Jordan Walke, who prototyped CommonML with
a NodeJS-style development workflow, and 
wrote up his 
design document for the mailing list. (Your questions or ideas do not need to be as
well developed as Jordan's prototype!)
Testing Packages

The public Travis CI testing does come with some limitations, since it only
checks that the latest package sets install, but not if any transitive
dependencies fail due to interface changes.  It also doesn't test all the
optional dependency combinations due to the 50 minute time limit.
We expanded the OPAM repository testing in several ways to get around this:
Individual Repositories: Thomas Gazagnaire built 
 centralised Travis scripts 
 that can be used on any OCaml GitHub repository to easily test code before it is
 released into OPAM.  These scripts are sourced from a central repository
 and support external, optional and reverse dependency checking across multiple revisions of the
 compiler.  For instance, it just needs one file to test
 all the supported permutations of the CoHTTP library.
Bulk Builds: Damien Doligez and I independently started doing large-scale bulk builds of
 the repository to ensure that a single snapshot of the package repository can automatically
 build as many packages as possible.  My implementation used the Docker
 container manager to spawn off 1000s of package builds in parallel and commit the results into a filesystem
 This required building a Dockerfile eDSL, and the results
 are now online at https://opam.ocaml.org/builds.
OCamlot: An ongoing piece of infrastructure work is to take the bulk build logs (which are
 around 7GB per daily run), and to store and render them using our Irmin Git store. 
 Expect to see more around this soon; it has the awesome feature of letting any developer clone
 the build logs for their project locally, to make triage of foreign operating systems as simple 
 as possible.


Language Evolution

This ability to do unattended builds of the package repository has also improved the decision
making process within the core compiler team.  Since we now have a large (3000+ package) corpus of
OCaml code, it became a regular occurrence in the 4.02 development cycle to 
"ask OPAM" whether 
a particular feature or new syntax would break any existing code.  This in turn provides an
incentive for commercial users to provide representative samples of their code; for instance, the
Jane Street Core releases in OPAM (with their very modular style) act as an open-source canary
without needing access to any closed source code.
One good example in 2014 was the decoupling of the Camlp4 
macro preprocessor from the main OCaml
distribution.  Since Camlp4 has been used for over a decade and there are some very commonly
used syntax extensions such as type_conv, a simple
removal would break a lot of packages.  We used OPAM to perform a gradual movement that most
users hopefully never noticed by the time OCaml 4.02 was released.  First, we added a 
dummy package in OPAM for earlier versions 
of the compiler that had Camlp4 built-in, and then used the OPAM constraint engine to compile it
as an external tool for the newer compiler revisions.  Then we just had to triage the bulk build
logs to find build failures from packages
that were missing a Camlp4 dependency, and add them to the package metadata.
GitHub Integration

An interesting comment from Vincent Hanquez
about OPAM is that "OCaml's OPAM is a post-GitHub design".  This is very true, as much of the
workflow for pinning git:// URLs emerged out of being early adopters of GitHub for hosting the
MirageOS.  OCaml Labs supported two pieces of infrastructure integration around GitHub in 2014:
OPAM has a compiler switch feature that lets you run simultaneous OCaml installations and
 swap between them easily. I used my GitHub API bindings
 to regularly convert every GitHub pull request 
 into a custom compiler switch.
 This lets users reporting bugs try out a patched compiler almost immediately upon a fix becoming
 available.
The motivation behind this feature was our collaborator Gabriel Scherer's 
 experiment to enable patch review of
 OCaml on GitHub, alongside the venerable Mantis bug tracker.
 We supported this via adding Travis CI support to the main compiler, and also helped to migrate a
 number of support libraries to GitHub, such as camlp4.  These
 can all be found on the ocaml organisation on GitHub.


Codoc Documentation

Leo White, David Sheets, Amir Chaudhry and Thomas Gazagnaire led the charge 
to build a modern documentation generator for OCaml, and 
published
an alpha version of codoc 0.2.0 after a lot of work throughout 2014.
In the 2014 OCaml workshop presentation 
(abstract,
slides,
video),
we mentioned the "module wall" for documentation and this attempts to fix it.
To try it out, simply follow the directions in the README on that repository,
or browse some samples of the current,
default output of the tool. Please do bear in mind codoc and its constituent
libraries are still under heavy development and are not feature complete, but
we're gathering feedback from early
adopters.
codoc's aim is to provide a widely useful set of tools for generating OCaml
documentation. In particular, we are striving to:
Cover all of OCaml's language featuresProvide accurate name resolution and linkingSupport cross-linking between different packagesExpose interfaces to the components we've used to build codocProvide a magic-free command-line interface to the tool itselfReduce external dependencies and default integration with other tools

We haven't yet achieved all of these at all levels of our tool stack but are
getting close, and the patches are all under discussion for integration into
the mainstream OCaml compiler.
codoc 0.2.0 is usable today (if a little rough in some areas like default CSS),
and there is a blog post that outlines the architecture of the new system to
make it easier to understand the design decisions that went into it.
Community Governance

As the amount of infrastructure built around the ocaml.org domain grows (e.g. mailing lists, file hosting, bulk building), it is important to establish a governance framework to ensure that it is being used as best needed by the wider OCaml community.
Amir Chaudhry took a good look at how other language communities organise themself, 
and began putting together a succinct 
governance framework 
to capture how the community around ocaml.org operates, and how to quickly resolve any conflicts 
that may arise in the future.  He took care to ensure it had a well-defined scope, is 
simple and self-contained, and (crucially) documents the current reality.  The result 
of this work is circulating privately through all the existing volunteers for a first 
round of feedback, and will go live in the next few months as a living document 
that explains how our community operates.
Assemblage

One consequence of OCaml's age (close to twenty years old now) is that the tools built
around the compiler have evolved fairly independently.  While OPAM now handles the high-level
package management, there is quite a complex ecosystem of other components that are complex
for new users to get to grips with: OASIS,
ocamlfind,
ocamlbuild, and
Merlin to name a few.
Each of these components (while individually stable) have their own metadata and
namespace formats, further compounding the lack of cohesion of the tools.
Thomas Gazagnaire and Daniel Buenzli embarked on an effort to build an eDSL
that unifies OCaml package descriptions, with the short-term aim of generating the
support files required by the various support tools, and the long-term goal of
being the integration point for the build, test and documentation generation
lifecycle of an OCaml/OPAM package.  This prototype, dubbed Assemblage
has gone through several iterations and design discussions
over the summer of 2014.  Daniel has since been splitting out portions of it
into the Bos OS interaction library.
Assemblage is not released officially yet, but we are committed to resuming work
on it this summer when Daniel visits again, with the intention of unifying much
of our workflow through this tool.   If you are interested in build and packaging
systems, now is the time to make your opinion known!
Core Compiler

We also spent time in 2014 working on the core OCaml language and compiler, with our work
primarily led by Jeremy Yallop and Leo White.  These efforts were not looking to make any
radical changes in the core language; instead, we generally opted for evolutionary
changes that either polish rough edges in the language (such as open type and handler cases),
or new features that fit into the ML style of building programs.
New Features in 4.02.0

The OCaml 4.02 series was primarily developed and released in 2014.
The ChangeLog generated much
user excitement,
and we were also pleased to have contributed several language improvements.
Handler Cases and exceptional syntax

OCaml's try and match constructs are good at dealing with exceptions
and values respectively, but neither constructs can handle both values and exceptions.
Jeremy Yallop investigated 
how to handle success
more elegantly, and an elegant unified syntax emerged.  A simple example is that
of a stream iterator that uses exceptions for control flow:
let rec iter_stream f s =
  match (try Some (MyStream.get s) with End_of_stream -> None) with
  | None -> ()
  | Some (x, s') -> f x; iter_stream f s'This code is not only verbose, but it also has to allocate an option value to ensure
that the iter_stream calls remains tail recursive.  The new syntax in OCaml 4.02
allows the above to be rewritten succinctly:
let rec iter_stream f s =
  match MyStream.get s with
  | (x, s') -> f x; iter_stream f s'
  | exception End_of_stream -> ()Read more about the background of this feature in Jeremy's 
blog post, the associated discussion in the upstream Mantis bug,
and the final manual page in the OCaml 4.02 release.
For an example of its use in a real library, see the Jane Street usage in the s-expression handling library (which they use widely to reify arbitrary OCaml values and exceptions).
Open Extensible Types

A long-standing trick to build universal containers 
in OCaml has been to encode them using the exception exn type.
There is a similar concept of a universal type in Standard ML,
and they were described in the "Open Data Types and Open Functions"
paper by Andres Löh and Ralf Hinze in 2006.
Leo White designed, implemented and upstreamed support for 
extensible variant types in 
OCaml 4.02. Extensible variant types are variant types that can be extended with new variant constructors.
They can be defined as follows:
type attr = ..

type attr += Str of string

type attr +=
  | Int of int
  | Float of floatPattern matching on an extensible variant type requires a default case to handle unknown 
variant constructors, just as is required for pattern matching on exceptions (extensible
types use the exception memory representation at runtime).
With this feature added, the OCaml exn type simply becomes a special case of open extensible
types. Exception constructors can be declared using the type extension syntax:
    type exn += Exc of intYou can read more about the discussion behind open extensible types in the upstream
Mantis bug.  If you'd like to see another
example of their use, they have been adopted by the latest releases of the 
Jane Street Core libraries in the 
Type_equal
module.
Modular Implicits

A common criticism of OCaml is its lack of support for ad-hoc polymorphism. 
The classic example of this is OCaml's separate addition operators for 
integers (+) and floating-point numbers (+.).
Another example is the need for type-specific printing functions (print_int, print_string, etc.)
rather than a single print function which works across multiple types.
Taking inspiration from Scala's 
implicits and 
Modular Type Classes by Dreyer et al.,
Leo White designed a system for ad-hoc polymorphism in OCaml based on using modules as
type-directed implicit parameters.  The design not only supports implicit modules, but also 
implicit functors (that is, modules parameterised by other module types) to permit the expression
of generic modular implicits in exactly the same way that functors are used to build abstract
data structures.
Frederic Bour joined us as a summer intern and dove straight
into the implementation, resulting in an 
online demo and ML Workshop presentation 
(abstract,
 video and
 paper).
Another innovation in how we've been trialling this feature is the use of Andy Ray's
IOCamlJS
to publish an interactive, online notebook that is fully hosted in the browser.  You can follow the
examples of modular implicits online,
or try them out on your own computer via an OPAM switch:
opam switch 4.02.0+modular-implicits
eval `opam config env`
opam install utop 
utopSome of the early feedback on modular implicits from industrial users was interesting.
Jane Street commented that although this would be a big usability leap, it would be dangerous to
lose control over exactly what goes into the implicit environment (i.e. the programmer should always
know what (a + b) represents by locally reasoning about the code).  The current design thus follows
the ML discipline of maintaining explicit control over the namespace, with any ambiguities in resolving
an implicit module type resulting in a type error.
Multicore

In addition to ad-hoc polymorphism, support for parallel execution on multicore CPUs is undoubtedly
the most common feature request for OCaml.  This has been high on our list after improving tooling 
support, and Stephen Dolan and Leo White made solid progress in 2014 on the core runtime plumbing
required.
Stephen initially added thread-local support to the 
OCaml compiler.  This design avoided the need to make the entire OCaml runtime preemptive
(and thus a huge patch) by allocating thread-local state per core.
We are now deep into the design and implementation of the programming abstractions built
over these low-level primitives.  One exciting aspect of our implementation is much of the scheduling
logic for multicore OCaml can be written in (single-threaded) OCaml, making the design very
flexible with respect to heterogenous hardware 
and variable IPC performance.
To get feedback on the overall design of multicore OCaml, we presented at OCaml 2014
(slides,
 video and
 abstract), and 
 Stephen visited INRIA to consult with the development team and Arthur Chargueraud (the author of PASL). 
Towards the end of the year, KC Sivaramakrishnan finished his PhD
studies at Purdue and joined
our OCaml Labs group.  He is the author of MultiMlton,
and is now driving the completion of the OCaml multicore work along with Stephen Dolan, Leo White and
Mark Shinwell. Stay tuned for updates from us when there is more to show later this year!
Ctypes: a Modular Foreign Function Interface

The Ctypes library started as an experiment with
GADTs by Jeremy Yallop, and has since ballooned in a robust, comprehensive library
for safely interacting with the OCaml foreign function interface.   The first release
came out in time to be included in 
Real World OCaml 
in lieu of the low-level FFI (which I was not particularly enamoured with having to explain
in a tight page limit).
Throughout 2014, Jeremy expanded support for a number of features requested by users
(both industrial and academic) who adopted the library in preference to manually writing
C code to interface with the runtime, and issued several updated 
releases.
C Stub Generation

The first release of Ctypes required the use of libffi
to dynamically load shared libraries and dynamically construct function call stack
frames whenever a foreign function is called.  While this works for simple libraries,
it cannot cover all usecases, since interfacing with C demands an understanding
of struct memory layout, C preprocessor macros, and other platform-dependent quirks
which are more easily dealt with by invoking a C compiler.  Finally, the performance
of a libffi-based API will necessarily be slower than writing direct C stub code.
While many other language FFIs provide separate libraries for dynamic and static
FFI libraries, we decided to have a go at building a modular version of Ctypes
that could handle both cases from a single description of the foreign function interface.
The result (dubbed "Cmeleon") remained surprisingly succinct and usable, and now
covers almost every use of the OCaml foreign function interface.  We 
submitted a paper to ICFP 2015 dubbed
"A modular foreign function interface"
that describes it in detail.  Here is a highlight of how simple a generic binding looks:
module Bindings(F : FOREIGN) = struct
  open F
  let gettimeofday = foreign "gettimeofday"
     (ptr timeval @-> ptr timezone @-> returning int)
endThe FOREIGN module type completely abstracts the details of whether or not dynamic
or static binding is used, and handles C complexities such as computing the struct
layout on the local machine architecture.
Inverse Stubs

The other nice result from functorising the foreign function interface emerged
when we tried to invert the FFI and serve a C interface from OCaml code (for
example, by compiling the OCaml code as a 
shared library).  This
would let us begin swapping out C libraries that we don't trust
with safer equivalents written in OCaml.
You can see an example
 of how inverted stubs work via a simple C XML parsing exposed
from the Xmlm library.  We can define a C struct
by:
(* Define a struct of callbacks (C function pointers) *)
let handlers : [`handlers] structure typ = structure "handlers"
let (--) s f = field handlers s (funptr f)
 let on_data      = "on_data"      -- (string @-> returning void)
 let on_start_tag = "on_start_tag" -- (string @-> string @-> returning void)
 let on_end_tag   = "on_end_tag"   -- (void @-> returning void)
 let on_dtd       = "on_dtd"       -- (string @-> returning void) 
 let on_error     = "on_error"     -- (int @-> int @-> string @-> returning void)
let () = seal handlersand then expose this via C functions:
module Stubs(I : Cstubs_inverted.INTERNAL) = struct
  (* Expose the type 'struct handlers' to C. *)
  let () = I.structure handlers

  (* We expose just a single function to C.  The first argument is a (pointer
     to a) struct of callbacks, and the second argument is a string
     representing a filename to parse. *)
  let () = I.internal "parse_xml" 
     (ptr handlers @-> string @-> returning void) parse
endYou can find the full source code to these snippets on the 
ocaml-ctypes-inverted-stubs-example
repository on GitHub.
We'll be exploring this aspect of Ctypes further in 2015 for SSL/TLS with
David Kaloper and Hannes Mehnert, and Microsoft Research has generously funded a 
PhD studentship 
to facilitate the work.
Community Contributions

Ctypes benefited enormously from several external contributions from the OCaml
community.  From a portability perspective, A. Hauptmann contributed
Windows support, and
Thomas Leonard added Xen support
to allow Ctypes bindings to work with MirageOS unikernels (which opens
up the intriguing possibility of accessing shared libraries across virtual machine boundaries
in the future).  C language support was fleshed out by Edwin Torok contributing
typedef support,
Ramkumar Ramachandra adding C99 bools 
and Peter Zotov integrating native strings.
The winner of "most enthusiastic use of OCaml Labs code" goes to Thomas Braibant
of Cryptosense, who used every feature of the
Ctypes library (consider multi-threaded, inverted, staged and marshalled bindings) in their
effort to hack the hackers.  David Sheets comes a close second with his implementation of the
FUSE binary protocol, parameterised by version quirks.
If you're using Ctypes, we would love to hear about your particular use.  A search on
GitHub and OPAM reveals over 20 projects using it already, including industrial use
at Cryptosense and Jane Street,
and ports to Windows, *BSD, MacOS X and even iPhone and Android.  There's a
getting started guide, and
a mailing list available.
Community and Teaching Efforts

In addition to the online community building, we also participated in a number of conferences
and face-to-face events to promote education about functional programming.
 Conferences and Talks

There has been a huge growth in the number of quality conferences in recent years, making 
it tough to choose which ones to attend.
ICFP is the academic meeting point that predates most of them, 
and we participated extensively in 2014 via talks, tutorials and a keynote at the Haskell Symposium.I also served on the program committee and 
industrial relations chair 
and took over as the steering committee 
chair of CUFP.  Jeremy Yallop, Thomas Gazagnaire and Leo White all 
served program committees on workshops, with Jeremy also chairing this year's ML Workshop.
Outside of academic conferences, we participated in a number of non-academic conferences such
as QCon, OSCON, CCC, 
New Directions in OS, FunctionalConf,
FPX and FOSDEM. 
The vast majority of these talks were about the MirageOS, and slides can be found at decks.openmirage.org.
The 2048 Browser Game

Yaron Minsky and I have run OCaml tutorials for ICFP for 
a 
few
years, and 
we finally hung up our boots in favour of a new crowd.
Jeremy Yallop and Leo White stepped up to the mark with their ICFP/CUFP 2014 
Introduction to OCaml
tutorial, which had the additional twist of being taught entirely in a web browser
by virtue of using the js_of_ocaml and IOCamlJS.
They decided that a good practical target was the popular 2048 game that
has wasted many programmer hours here at OCaml Labs.  They hacked on it over the summertime, assisted by our visitor Daniel Buenzli who also released useful libraries such as
Vg, React, Useri, and Gg.
The end result is satisfyingly playable online, with the source code available at ocamllabs/2048-tutorial.
Thomas Gazagnaire got invited to Bangalore for Functional Conf later in the year, and hee extended the interactive tutorial notebook and also ran an OCaml tutorial to a packed room.  We were very happy to support the first functional programming conference in India, and hope to see many more such events spring up!  Amir Chaudhry then went to Belgium to FOSDEM 2015 where he showed off the 2048 game running as an ARM unikernel to a crowd of attendees at the Xen booth.
Graduate Teaching

Jeremy Yallop and Leo White (with assistance from Alan Mycroft and myself) also led the design of
a new graduate course on Advanced Functional Programming
at the Computer Laboratory.  This ran in the Lent Term and
was over-subscribed by three times the number who pre-registered (due to a number of PhD students
and our collaborators from Citrix also attending).
The course materials are freely available online
and cover the theory behind functional programming, and then move onto type inference, abstraction and
parametricity, GADTs, rows, monads, and staging.  We will be running this again in future years, and the
lecture materials are already proving useful to 
answer mailing list questions.
Mentoring Beginners

We also had the pleasure of mentoring up-and-coming functional programmers via several outreach programs,
both face-to-face and remote.
Cambridge Compiler Hacking

We started the Cambridge Compiler Hacking sessions
in a small way towards the end of 2013 in order to provide a local, friendly place to assist people
who wanted to dip their toes into the unnecessarily mysterious world of programming language hacking.
The plan was simple: provide drinks, pizza, network and a bug list of varying difficulty for attendees to choose from and work on for the evening, with mentoring from the
experienced OCaml contributors.
We continued this bi-monthly tradition in 2014, with a regular attendance of 15-30 people, and
even cross-pollinated communities with our local F# and Haskell colleagues.  We rotated locations
from the Cambridge Computer Laboratory to Citrix, Makespace, and the new Cambridge Postdoc Centre.
We posted some highlights
from sessions towards the start of the year, and are very happy with how it's going.  There
has even been uptake of the bug list across the water in France, thanks to Gabriel Scherer.
In 2015, we'd like to branch out further and host some sessions in London. If you have a suggestion
for a venue or theme, please get in touch!
Summer Programs

There has been a laudable rise in summer programs designed to encourage diversity in 
our community, and we of course leap at the opportunity to participate in these when we find them.
The GNOME Outreach Program (now also known as Outreachy)
 had one funded place for Xen and MirageOS.  Mindy Preston did a spectacular
 blog series about her experiences and motivations
 behind learning OCaml.The Google Summer of Code 2014 also had us participating via MirageOS, 
 and Jyotsna Prakash took on the challenging job of building
 OCaml bindings for Amazon EC2, also detailed on her blog.Amir Chaudhry began the Mirage Pioneer Projects
 initiative to give beginners an easier onramp, and this has taken off very effectively as a way
 to advertise interesting projects for beginners at varying levels of difficulties.

Our own students also had the chance to participate in such workshops to get out of Cambridge
in the summer!
Heidi Howard liveblogged her experiences at the PLMW workshop in Mumbai.
Meanwhile, David Sheets got to travel to the slightly less exotic London to liveblog OSIO, and Leonhard Markert covered ICFP 2014 as a student volunteer.
Blogging and Online Activities

Our blog roll maintains the ongoing stream of
activity from the OCaml Labs crew, but there were some particular highlights throughout 2014.
Thomas Leonard began writing about his experiences with switching
 his 0install installation system from 
 Python to OCaml
 and what you gain with OCaml.
 This series led to a bunch of interesting feedback on social networking sites, and Thomas
 joined the group full-time to work on our research into 
 unikernels.Magnus Skjegstad returned from Norway to Cambridge to work on 
 MirageOS, and came up with some crazy experiements,
 as well as helping to build Vagrant images of the OCaml development environment.Amir Chaudhry began his quest to 
 port his website website
 to a Jekyll unikernel.The Mirage 2.0 release in the summer of 2014
 saw a slew of blogs posts about the surge in MirageOS activity.

It wasn't all just blogging though, and Jeremy Yallop and Leo White in particular participated in some
epic OCaml bug threads about new features, and
explanations about OCaml semantics
on the mailing list.
Amir Chaudhry also continued to curate and develop the content on the ocaml.org
website with our external collaborators Ashish Agarwal, Christophe Troestler and Phillippe Wang.
Notably, it is now the recommended site for OCaml (with the INRIA site
being infrequently updated), and also hosts the ACM OCaml Workshop
pages.  One addition that highlighted the userbase of OCaml in the teaching community came from
building a map of all of the universities where the
language is taught, and this was Yan Shvartzshnaider's first contribution to the site.
Visitors and Interns

Finally, a really important part of any community is hanging out with each other to chat over ideas in a friendly environment.
As usual, we had a very steady stream of visitors and interns throughout 2014 to facilitate this.
Frederic Bour, Benjamin Farinier and Matthieu Journault joined us as summer interns from their respective universities in France as part of their Masters programs.  Frederic worked on modular implicits and gave a great talk at the OCaml Users group.  Benjamin and Matthieu worked on Irmin data structures and complexity (and merge-queues and merge-ropes), and Benjamin had his paper on "Mergeable Persistent Data Structures" accepted to JFLA 2015, while Matthieu's work on efficient algorithms for synchronising Irmin DAGs is being integrated into the upstream source code.
Daniel Buenzli repeated his visit from 2013 and spent a productive summer with us, commenting on almost every project we're working on.  In his own words (edited for brevity):
I started by implementing and releasing Uucp, a library to provide efficient access to a selection of the  properties of the latest Unicode Character database (UCD). [...] As a side effect of the previous point I took time to write an absolute minimal introduction to Unicode. [...]
Since I was in this Unicode business I took the opportunity to propose a 31 loc patch to the standard library for a type to represent Unicode scalar values (an Unicode character to be imprecise) to improve interoperability.
The usual yearly update to OpenGL was announced at the Siggraph conference. This prompted me to update the ctypes-based tgls library for supporting the latest entry point of OpenGL 4.5 and OpenGL ES 3.1. Since the bindings are automatically generated from the OpenGL XML registry the work is not too involved but there's always the odd function signature you don't/can't handle automatically yet.
Spend quite a bit (too much) time on useri, a small multi-platform abstraction for setting up a drawing surface and gather user input (not usury) as React events. Useri started this winter as a layer on top of SDL to implement a CT scan app and it felt like this could be the basis for adding interactivity and animation to Vg/Vz visualizations – js viz libraries simply rely on the support provided by the browser or SVG support but Vg/Vz strives for backend independence and clear separations of concern (up to which limit remains an open question). Unfortunately I couldn't bring it to a release and got a little bit lost in browser compatibility issues and trying to reconcile what browser and SDL give us in terms of functionality and way of operating, so that a maximum of client code can be shared among the supported platforms. But despite this non-release it still managed to be useful in some way, see the next point.
Helped Jeremy and Leo to implement the rendering and interaction for their ICFP tutorial 2048 js_of_ocaml implementation. This featured the use of Gg, Vg, Useri and React and I was quite pleased with the result (despite some performance problems in certain browsers, but hey composable rendering and animation without a single assignement in client code). It's nice to see that all these pains at trying to design good APIs eventually fit together [...]


A couple of visitors joined us from sunny Morocco, where Hannes Mehnert and David Kaloper had gone to work on a clean-slate TLS stack.  They found the MirageOS effort online, and got in touch about visiting.  After a very fun summer of hacking, their stack is now the standard TLS option in MirageOS and resulted in the Bitcoin Pinata challenge being issued!  Hannes and David have since moved to Cambridge to work on this stack full-time in 2015, but the internships served as a great way for everyone to get to know each other.
We also had the pleasure of visits from several of our usually remote collaborators. Christophe Troestler, Yaron Minsky, Jeremie Diminio and Andy Ray all visited for the annual OCaml Labs review meeting in Christ's College.
There were also many academic talks from foreign visitors in our SRG seminar series, ranging from Uday Khedkar from IIT to Oleg Kiselyov deliver multiple talks on staging and optimisation (as well as making a celebrity appearance at the compiler hacking session, and Yaron Minsky delivering an Emacs-driven departmental seminar on his experiences with Incremental computation.
Research Efforts

The OCaml Labs are of course based in the Cambridge Computer Laboratory, where 
our day job is to do academic research.  Balancing the demands of open source 
coding, community efforts and top-tier research has be a tricky one, but an effort
that has been worthwhile.
Our research efforts are broadly unchanged from 2013
(it takes time to craft good ideas!), and this will not be an exhaustive recap.  Instead, we'll summarise
them here and point to our papers that describe
the work in detail.
The MirageOS really found its own feet in 2014, with a 
 summer 2.0 release and
 an extensive end-of-year recap.
 The most notable thing has been how well the MirageOS research work has melded
 with the core OCaml Labs efforts, since much of it has been constructing good
 quality OCaml libraries to plug holes in the ecosystem.  It also served to make
 us use OPAM on a day-to-day basis for our own work, thus creating an effective
 feedback loop between open-source and research.
In the Trilogy2 and UCN
 EU projects, we built out MirageOS features such as the
 Jitsu toolstack for the
 "just-in-time" summoning of unikernels in response to DNS requests.  This paper
 will be presented next month at UlSENIX NSDI.
 It also drove the development of the ARMv7 port,
 an architecture for which OCaml has an excellent native code generator, as well
 as more experimental forays into BitCoin incentive schemes
 for distributed systems.
The Irmin Git-like branchable store created by Thomas Gazagnaire matured,
 with Dave Scott prototyping a complex
 port of the XenStore database to Irmin, thus 
 letting us show off debugging systems with Git.
 We had a paper accepted on some early datastructures accepted at JFLA,
 and Thomas Leonard is building the JavaScript backend for running in-browser,
 while Yan Schvartzshnaider is experimenting with graph processing over the DAG representation for privacy-friendly queries.
 KC is investigating how to adapt his PLDI 2015 paper on Quelea into using
 Irmin as a backend as well.
The Higher kinded polymorphism library written
 by Jeremy Yallop and Leo White was published in FLOPS 2014,
 forming a basis for building more complex use-cases that need the flexibility of higher
 kinded types without requiring functorising code.
Our long standing research into personal online privacy 
 led to our next system target that uses unikernels: the Databox paper
 outlines the architecture, and was covered in the Guardian newspaper.  Jon Crowcroft led the establishment of the Cambridge wing of the Microsoft Cloud Computing Research Center to consider the legal aspect of things, and so we have made forays outside of technology into considering the implications of region-specific clouds as well.


Some of the most exciting work done in the group as part of the REMS 
and NaaS projects came towards the end of 2014 and start of 2015, with
multiple submissions going into top conferences.  Unfortunately, due to most of them
being double blind reviewed, we cannot link to the papers yet.  Keep an eye on the
blog and published paper set,
or ask us directly about what's been going on!
Priorities for 2015

As spring breaks and the weather (almost) becomes bearable again, we're setting our work priorities
for the remainder of the year.
Tooling Cohesion: The entire core team is focussed on fusing together
 the individual tools that have been created last year into a cohesive OCaml Platform release that
 covers the lifecycle of documentation, testing and build.
 This is being managed by Amir Chaudhry.  OPAM remains at the heart of this strategy,
 and Louis Gesbert and Thomas Gazagnaire have settled on the 
 OPAM 1.3 roadmap (summary).
Multicore:  KC Sivaramakrishnan has joined the core OCaml Labs
 fulltime to drive the multicore work into a publically testable form. Leo White recently 
 departed after many productive years in Cambridge to head into a career in industry 
 (but still remains very much involved with OCaml development!).
Language Evolution: Jeremy Yallop continues to drive our efforts on staged programming,
 modular implicits, and a macro system for OCaml, all of which are key features that
 make building complex, reliable systems more tractable than ever.


I'd like to thank the entire team
and wider community for a wonderfully enjoyable 2014 and start of 2015, and am very thankful
to the funding and support from Jane Street, Citrix, British Telecom, RCUK, EPSRC, DARPA and
the EU FP7 that made it all possible.
As always, please feel free to contact any of us directly with questions, or reach out to
me personally with any queries, concerns or bars of chocolate
as encouragement.

   Hide
        
      
                    by Anil Madhavapeddy at Apr 02, 2015 
      
      
    
  


       
                  Towards Heroku for Unikernels: Part 1 - Automated deployment
      (Amir Chaudhry)
    
    
                                In my Jekyll to Unikernel post, I described an automated
workflow that would take your static website, turn it into a MirageOS
unikernel, and then store that unikernel in a git repo for later deployment. 
Although it was written from the perspective of a static website, the process
was applicable to any MirageOS project.
This post covers how things have progressed since then and the kind of
automated, end-to-end deployments that we can achieve with unikernels.  

If you’re already familiar with the above-linked post then it should be clear
that this will involve writing a few more scripts and ensuring
they’re in the right place.  The rest of this post will go through a real
world example of such an automated system, which we’ve set up for building and
deploying the unikernel that serves our slide decks — mirage-decks.  Once
you’ve gone though this post, you should be able to recreate such a workflow
for your own needs. In Part 2 of this series I’ll build on this post and
c…Read more...In my Jekyll to Unikernel post, I described an automated
workflow that would take your static website, turn it into a MirageOS
unikernel, and then store that unikernel in a git repo for later deployment. 
Although it was written from the perspective of a static website, the process
was applicable to any MirageOS project.
This post covers how things have progressed since then and the kind of
automated, end-to-end deployments that we can achieve with unikernels.  

If you’re already familiar with the above-linked post then it should be clear
that this will involve writing a few more scripts and ensuring
they’re in the right place.  The rest of this post will go through a real
world example of such an automated system, which we’ve set up for building and
deploying the unikernel that serves our slide decks — mirage-decks.  Once
you’ve gone though this post, you should be able to recreate such a workflow
for your own needs. In Part 2 of this series I’ll build on this post and
consider what the possibilities could be if we extended the system using
some of our other tools — thus arriving at something very much
like our own Heroku for Unikernels.

Standardised build scripts

Almost all of our OCaml projects now use Travis CI for build and testing (and
deployment). In fact, there are so many libraries now that we recently put
together an OCaml Travis Skeleton, which means we don’t
have to manually keep the scripts in sync across all our repos — and fewer
copy/paste/edits means fewer mistakes. 

If you’re familiar with the build scripts from last time, then
you can browse the new scripts and you’ll see that they’re broadly similar. 
In many cases you may well be able to depend on one or other of the scripts
directly and for a handful of scenarios, you can fork and patch them to
suit you (i.e. for MirageOS unikernels).  We can do this because we’ve made it
quick to set up an OCaml environment using an Ubuntu PPA. The rest
of the work is done by the mirage tool itself so once that’s installed, the
build process becomes fairly straightforward. The complexity around secure
keys was also covered last time, which allowed us to commit the
final unikernel to a deployment repo.  That means the remaining step is
to automate the deployment itself.

Automated deployment of unikernels

Committing the unikernel to a deployment repo is where the previous post ended
and a number of people forged ahead and wrote about their
experiences deploying onto AWS and Linode.  Many of these deployments
(understandably) involve a number of quite manual steps. It would be
particularly useful to construct a set of scripts that can be fully automated,
such that a git push to a repo will automatically run through the cycle of
building, testing, storing and activating a new unikernel.  We’ve done
exactly this with some of our repos and this post will talk through those
scripts.  

The deployment options — Xen or *nix

MirageOS unikernels can currently be built for Xen and Unix backends.  This is
a straightforward step and typically the build matrix is already set up to
test that both of them build as expected. For this post, I’ve only considered
the Xen backend as that’s our chosen deployment method but it would be equally
feasible to deploy the unix-based unikernels onto a *nix machine in much the
same way. 
In this sense, you get to choose whether you want to deploy the unikernels
onto a Hypervisor (for isolation and security) or whether running
them as unix-processes better suits your needs. 

The unikernel approach means that both options are open to
you, with little more than a command-line flag between them.

In terms of the deployment machines there are several options to consider. The
most obvious is to set up a dedicated host, where you have full access to the
machine and can install Xen.  Another is to have a machine
running on EC2 and create scripts to deal with unikernels. You
could also build and deploy onto Xen on the Cubieboard2. If you’d
rather test out the complete system first, you could set up an appropriate
machine in Virtualbox to work with.

For our workflow, we use Xen unikernels which we deploy to a dedicated host. 
For the sake of brevity, I won’t go into the details of how to set up
the machine but you can follow the instructions linked above.

The scripts for decks.openmirage.org

Decks is the source repo that holds many of our slides, which
we’ve presented at conferences and events over the years (I admit that I have
yet to add mine).  The repo compiles to a unikernel that can
then serve those slides, as you see at decks.openmirage.org. For
maximum fun-factor, we usually run that unikernel from a Cubieboard2 when
giving talks.



The toolchain for this unikernel includes build, store and deploy.  We’ll
recap the first two steps before going through the final one.

Build — In the root of the decks source repo, you’ll notice the
.travis.yml file, which fetches the standard build script mentioned earlier.
Building the unikernel proceeds according to the options in the build matrix. 

language: c
install: wget https://raw.githubusercontent.com/ocaml/ocaml-travisci-skeleton/master/.travis-mirage.sh
script: bash -ex .travis-mirage.sh
env:
  matrix:
  - OCAML_VERSION=4.02 MIRAGE_BACKEND=unix MIRAGE_NET=socket
  - OCAML_VERSION=4.02 MIRAGE_BACKEND=unix MIRAGE_NET=direct
  - OCAML_VERSION=4.02 MIRAGE_BACKEND=xen
    MIRAGE_ADDR="46.43.42.134" MIRAGE_MASK="255.255.255.128" MIRAGE_GWS="46.43.42.129"
    DEPLOY=1
  global:
  - secure: ".... encrypted data ...."
  - secure: ".... encrypted data ...."
  - secure: ".... encrypted data ...."
  ...

In this case, two builds occur for Unix and one for Xen with different
parameters being used for each.  If you look at the
actual travis file, you’ll notice there are 26 lines of
encrypted data.  This is how we pass the deployment key to Travis CI, so that
it has push access to the separate mirage-decks-deployment
repo.  You can read the section in the previous post to see how we
send Travis a private key.

Store — One of the combinations in the build matrix (configured for Xen),
is intended for deployment.  When that unikernel is completed, an additional
part of the script is triggered that pushes it into the deployment repo. 

Deployment scripts

After the ‘build’ and ‘store’ steps above, we have a
deployment repository with a collection of Xen unikernels. For
this stage, we have a new set of scripts that live in this repo alongside those
unikernels. Specifically, you’ll notice a folder called scripts that
contains four files.  

.
├── Makefile
├── README.md
├── scripts
│   ├── crontab
│   ├── deploy.sh
│   ├── install-hooks.sh
│   └── post-merge.hook
...

A quick summary of the setup is that we clone the repo onto our deployment
machine and install some hooks there.  Then a simple cronjob will perform
git pull at regular intervals.  If a merge event occurs, then it means the
repo has been updated and another script is triggered. That script removes the
currently running unikernel and boots the latest version from the repo.  It’s
fairly straightforward and I’ll explain what each of the files does below.

Makefile - After cloning the repo, run make install.  This will trigger
install-hooks.sh to set things up appropriately. It’s worth remembering that
from this point on, the git repo on the deployment machine will not be
identical to the deployment repo on GitHub.

install-hooks.sh —  The first two lines ensure that the commands
will be run from the root of the git repo.  The third line symlinks the
post-merge.hook file into the appropriate place within the .git directory.
This is the folder where customized git hooks need to be placed in
order to work.  The final line adds the file scripts/crontab to the
deployment machine’s list of cron jobs.

ROOT=$(git rev-parse --show-toplevel)  # obtain path to root of repo
cd $ROOT

# symlink the post-merge.sh file into the .git/hooks folder
ln -sf $ROOT/scripts/post-merge.hook $ROOT/.git/hooks/post-merge
crontab scripts/crontab                # add to list of cron jobs

crontab — This file is a cronjob that sets up the deployment machine to
perform a git pull on the deployment repo at regular intervals. Changing the
file in the repo will ultimately cause it to be updated on the deployment
machine (cf. deploy.sh). At the moment, it’s set to run every 11 minutes.

*/11 * * * * cd $HOME/mirage-decks-deployment && git pull

post-merge.hook — Since we’ve already run the Makefile, this file is
symlinked from the appropriate place on the deployment machine’s copy of the
repo.  When a git pull results in new commits being downloaded and merged,
then this script is triggered immediately afterwards.  In this case, it just
executes the deploy.sh script.

ROOT=$(git rev-parse --show-toplevel)  # obtain path to root of repo
exec $ROOT/scripts/deploy.sh           # execute the deploy script

deploy.sh — This is where the work actually happens and you’ll notice that
there really isn’t much to do!  I’ve commented in the code below to explain
what’s going on.

VM=mir-decks
XM=xm

ROOT=$(git rev-parse --show-toplevel)
cd $ROOT

crontab scripts/crontab     # Update cron scripts

# Identify the latest build in the repo and then use
# the generic Xen config script to construct a
# specific file for this unikernel. Essentially,
# 'sed' just does a find/replace on two elements and
# the result is written to a new file.
#
KERNEL=$ROOT/xen/`cat xen/latest`
sed -e "s,@VM@,$VM,g; s,@KERNEL@,$KERNEL/$VM.xen,g" \
    < $XM.conf.in \
    >| $KERNEL/$XM.conf

# Move into the folder with the latest unikernel.
# Remove any uncompressed Xen images found there
# (since we may be starting a rebuilt unikernel).
# Unzip the compressed unikernel.
#
cd $KERNEL
rm -f $VM.xen
bunzip2 -k $VM.xen.bz2

# Instruct Xen to remove the currently running
# unikernel and then start up the new one we
# just unzipped.
#
sudo $XM destroy $VM || true
sudo $XM create $XM.conf

At this point, we now have a complete system!
Of course, this arrangement isn’t perfect and
there are number of things we could improve.  For example, it depends on a
cron job, which means it may take a while before a new unikernel is live.
Replacing this with something triggered on a webhook could be an improvement,
but it does mean exposing an end-point to the internet.  The scripts will also
redeploy the current unikernel, even if the only change is to the crontab
schedule.  Some extra work in the deploy script, using some git tools, might
work around this. 

Despite these minor issues, we do have a completely end-to-end workflow that
takes us all the way from pushing some new changes to deploying a new
unikernel!  An additional feature is that everything is checked into version
control. Right from the scripts to completed artefacts (including a method of
transmitting secure keys/data, over public systems). 

There is minimal work done outside the code you’ve already seen, though there
is obviously some effort involved in setting up the deployment machine. 
However, as mentioned earlier, you could either use the unix-based unikernels
or experiment with Virtualbox VM with Xen just to test out this
entire toolchain. 

Overall, we’ve only added around 20 lines of code to the initial 50 or so that
we use for the Travis CI build.  So for less than 100 lines of code, we have
a complete end-to-end system that can take a MirageOS project from a
git push, all the way through to a live deployment.  

Fleshing out the backbone

In our current system, if the unikernel builds appropriately then we just
assume it’s ok to deploy to production. Fire and forget! What could
possibly go wrong!  Of course, this is a somewhat naive approach and for any
critical system it would be better to hook in some additional things.

Testing frameworks

One obvious improvement would be to introduce a more thorough testing regimen,
which would include running unit tests as part of the build. Indeed, various
libraries in the MirageOS project are already moving towards this model
(e.g see the notes for links).  

It’s even possible to go beyond unit tests and introduce more
functional/systems/stress testing on the complete unikernel before permitting
deployment.  This would help to surface any wider issues as services interact
and we could even simulate network conditions — achieving something like
‘staging on steroids’.  

Logging and notifications

The scenario we have above also assumes that things work smoothly and nobody
needs to know anything.  It would be useful to hook in some form of logging
and reporting, such that when a new unikernel is deployed a notification can
be sent/stored somewhere. In the short term, there are likely existing tools
and ways of doing this so it would be a matter of putting them together.

Looking ahead

Overall, with the above model, we can easily set up a system where we go from
writing code, to testing it via CI, to deploying it to a staging server for
functional tests, and finally pushing it out into live deployment.  All of
this can be done with a few additional scripts and minimal interaction from
the developer.  We can achieve this because we don’t have to concern ourselves
with large blobs of code, multiple different systems and keeping environments
in sync. Once we’ve built the unikernel, the rest almost becomes trivial. 

This is close enough for me to declare it as a ‘Heroku for unikernels’ but
obviously, there’s much more we can (and should) do with such a system. If we
extrapolate just a little from where we are now, there are a range of
exciting possibilities to consider in terms of automation, scalability and
distributed systems.  Especially if we incorporate other aspects of the
toolstack we’re working towards.  

Part 2 of this series is where I’ll consider these possibilities, which will
be more speculative and less constrained.  It will cover the kinds of systems
we can create once the tools are more mature and will touch on ideas around
hyper-elastic clouds, embedded systems and what this means for the concept of
immutable infrastructure.

Since we already have the ‘backbone’ of the toolchain in place, it’s easier to
see where it can be extended and how.

Edit: The second part of this series is now up -
“Self Scaling Systems”


Thanks to Anil Madhavapeddy and Thomas Leonard for comments on an earlier
draft and Richard Mortier for his work on the deployment toolchain.


Hide
        
      
                    by Amir Chaudhry at Mar 31, 2015 
      
      
    
  


       
          Opam Switch to Multicore OCaml
      (KC Sivaramakrishnan)
    
                                OPAM has a great compiler
switch feature that lets you
simultaneously host several OCaml installations, each with its own compiler
version and a set of installed packages. I wanted to use the power of opam
switch for working with the experimental multicore
OCaml compiler. The key
advantage of doing this is that it lets you easily install packages from the
OPAM repository, while sandboxing it from other OCaml
installations on your system. The post will show how to create OPAM compiler
switch for multicore OCaml.

Install opam-compiler-conf

The first step is to install Gabriel Scherer's opam-compiler-conf
script which lets you do opam
switches on local installations:

$ git clone https://github.com/gasche/opam-compiler-conf
$ cd opam-compiler-conf
$ mkdir -p ~/.local/bin
$ make BINDIR=~/.local/bin install

This installs the opam-compiler-conf script under ~/.local/bin. Make sure
this directory is under your search path. Now, $opam compiler-conf should
give you the list of available comm…Read more...OPAM has a great compiler
switch feature that lets you
simultaneously host several OCaml installations, each with its own compiler
version and a set of installed packages. I wanted to use the power of opam
switch for working with the experimental multicore
OCaml compiler. The key
advantage of doing this is that it lets you easily install packages from the
OPAM repository, while sandboxing it from other OCaml
installations on your system. The post will show how to create OPAM compiler
switch for multicore OCaml.

Install opam-compiler-conf

The first step is to install Gabriel Scherer's opam-compiler-conf
script which lets you do opam
switches on local installations:

$ git clone https://github.com/gasche/opam-compiler-conf
$ cd opam-compiler-conf
$ mkdir -p ~/.local/bin
$ make BINDIR=~/.local/bin install

This installs the opam-compiler-conf script under ~/.local/bin. Make sure
this directory is under your search path. Now, $opam compiler-conf should
give you the list of available commands.

Build multicore OCaml locally

Typing opam switch should list the compilers currently installed in your
system and those that are available. For instance, here is my setup:

$ opam switch
system  C system  System compiler (4.02.1)
4.02.1  I 4.02.1  Official 4.02.1 release
4.02.0  I 4.02.0  Official 4.02.0 release
4.01.0  I 4.01.0  Official 4.01.0 release
--     -- 3.11.2  Official 3.11.2 release
--     -- 3.12.1  Official 3.12.1 release
--     -- 4.00.0  Official 4.00.0 release
--     -- 4.00.1  Official 4.00.1 release
# 66 more patched or experimental compilers, use '--all' to show

You can easily switch between the installations using opam switch
[system-name]. Let us now install multicore OCaml as a new switch:

$ git clone https://github.com/ocamllabs/ocaml-multicore
$ cd ocaml-multicore
$ opam compiler-conf configure
$ make world
$ opam compiler-conf install
$ eval `opam config env`

The multicore compiler is now installed and has been made the current compiler:

$ opam switch
system                      I system                      System compiler (4.02.1)
4.02.1+local-git-multicore  C 4.02.1+local-git-multicore  Local checkout of 4.02.1 at /Users/kc/ocaml-multicore
4.02.1                      I 4.02.1                      Official 4.02.1 release
4.02.0                      I 4.02.0                      Official 4.02.0 release
4.01.0                      I 4.01.0                      Official 4.01.0 release
--                         -- 3.11.2                      Official 3.11.2 release
--                         -- 3.12.1                      Official 3.12.1 release
--                         -- 4.00.0                      Official 4.00.0 release
--                         -- 4.00.1                      Official 4.00.1 release
# 66 more patched or experimental compilers, use '--all' to show

This can be confirmed by:

$ ocamlc -version
4.02.1+multicore-dev0

which shows the current OCaml bytecode compiler version.

Working with the local switch

Every time you change the compiler source, you need to rebuild the compiler and
reinstall the switch:

# Changed compiler source...
$ make world
$ opam compiler-conf reinstall

The local installation can be removed by opam compiler-conf uninstall.
Hide
        
      
                    by KC Sivaramakrishnan at Mar 25, 2015 
      
      
    
  


       
                  A unikernel experiment: A VM for every URL
      (Magnus Skjegstad)
    
    
                                I recently wrote a DNS server that can boot unikernels on demand called Jitsu. The following diagram shows a simplified version of how Jitsu works. The client sends a DNS query to a DNS server (Jitsu). The DNS server starts a unikernel and sends a DNS response back to the client while the unikernel is booting. When the client receives the DNS response it opens a TCP connection to the unikernel, which now has completed booting and is ready to respond to the TCP connection.

The unikernels are built using MirageOS, a library operating system that allows applications to be compiled directly to small Xen VMs. These unikernels only include the operating system components the application needs - nothing else is added. This results in very small VMs with low resource requirements that boot quickly. 
Now, what if I wanted to use Jitsu to boot my unikernel website when someone accesses it? My website is fairly low traffic, so this could potentially save me some resource use and hosting costs. U…Read more...I recently wrote a DNS server that can boot unikernels on demand called Jitsu. The following diagram shows a simplified version of how Jitsu works. The client sends a DNS query to a DNS server (Jitsu). The DNS server starts a unikernel and sends a DNS response back to the client while the unikernel is booting. When the client receives the DNS response it opens a TCP connection to the unikernel, which now has completed booting and is ready to respond to the TCP connection.

The unikernels are built using MirageOS, a library operating system that allows applications to be compiled directly to small Xen VMs. These unikernels only include the operating system components the application needs - nothing else is added. This results in very small VMs with low resource requirements that boot quickly. 
Now, what if I wanted to use Jitsu to boot my unikernel website when someone accesses it? My website is fairly low traffic, so this could potentially save me some resource use and hosting costs. Unfortunately, there are always a few requests per hour to some of the more popular sections, which likely would make my unikernel run most of the time. But what if I could split my unikernel into even smaller unikernels? What if I went to an extreme and had one unikernel per URL? Then I could only boot unikernels for the URLs that are being used and they would only need to know how to serve a single page. This could also have a number of benefits, such as the ability to spin up multiple unikernels for an extremely popular web page and use DNS to direct clients to the unikernel that is closest to them — while keeping the rest of the site inactive (let's ignore web crawlers for now). If I had dynamic sections of my website there could also be security benefits: Every dynamic page would run as a separate VM. An attack on a single page would not have to bring down the rest of the site nor reveal any data stored in other unikernels.


As an experiment, I wanted to see if I could run every URL hosted under my domain as a separate virtual machine. The goal was to map each URL to a domain name, which in turn was directed to a separate VM running a web server that hosts a single page. For example, the URL http://www.skjegstad.com/index.html becomes http://index.html.www.skjegstad.com/.  Each domain is then mapped to a single IPv4 address on my local network.
Here is an abstract from the DNS zone file.
index.html.www.skjegstad.com    IN  A   192.168.56.10
images.profile.jpg.www.skjegstad.com    IN  A   192.168.56.11
ardrone-experiment-1.blog.images.ardrone-test.m4v.www.skjegstad.com IN  A   192.168.56.12
ardrone-experiment-1.blog.images.camera-33.jpg.www.skjegstad.com    IN  A   192.168.56.13
ardrone-experiment-1.blog.images.ardrone-test.png.www.skjegstad.com IN  A   192.168.56.14
[...]



You can see that even individual images have their own DNS entry, and will be served separately.
There is an OCaml web server library, called CoHTTP, which we can use with MirageOS. Using this library, unikernels can be written that host static web pages. The web content itself can either be compiled into the unikernel (as a data structure) or be stored directly on a block storage device. For this experiment, I use the built-in data structure to store a single file per unikernel. 
For the unikernel code, I take the static_webserver example from mirage-skeleton. The code automatically creates a web server that hosts the files located in the ./htdocs subdirectory when the unikernel is built.
The demo code (available here) accepts a directory with static web pages as input. I will use my own website in the examples below, but any site with static web pages should work.
First, mirror (or copy) a static website:
$ wget -m www.skjegstad.com



Then run the create.py script to generate a unikernel with a web server for each file, which will serve that single file from memory. Note that the folder name and domain name must match - otherwise the script will become confused and not rewrite the URLs properly.  My site has 49 pages and I've copied the entire output below.  This step should only take a few seconds on a reasonably modern machine.
# Run create.py on the contents of the www.skjegstad.com folder. Folder name and domain name must match.
$ python create.py www.skjegstad.com
Indexing www.skjegstad.com...
Preparing unikernel for index.html.www.skjegstad.com/index.html in staging/85956378dc89cf3a2729
Preparing unikernel for images.profile.jpg.www.skjegstad.com/profile.jpg in staging/f7fb9c68d5c3701996b3
Preparing unikernel for ardrone-experiment-1.blog.images.ardrone-test.m4v.www.skjegstad.com/ardrone-test.m4v in staging/c286eccb0f59b299cca1
Preparing unikernel for ardrone-experiment-1.blog.images.camera-33.jpg.www.skjegstad.com/camera_33.jpg in staging/5bef80428ab04abb98c3
Preparing unikernel for ardrone-experiment-1.blog.images.ardrone-test.png.www.skjegstad.com/ardrone-test.png in staging/41d9ee421440ceec86a4
Preparing unikernel for ardrone-experiment-1.blog.images.ardrone1.jpg.www.skjegstad.com/ardrone1.jpg in staging/a9fdfed9d054ee4d48de
Preparing unikernel for ardrone-experiment-1.blog.images.camera-28.jpg.www.skjegstad.com/camera_28.jpg in staging/5ca0d87ae96d8648c36d
Preparing unikernel for ardrone-experiment-1.blog.images.screenshot1.png.www.skjegstad.com/screenshot1.png in staging/5b1ce89d1c806c4234f2
Preparing unikernel for mirage-dev.blog.images.ubuntu-in-vm-overview.jpg.www.skjegstad.com/ubuntu_in_vm_overview.jpg in staging/0d368ef42317b28f6243
Preparing unikernel for mirage-dev.blog.images.xen-in-vm-overview.jpg.www.skjegstad.com/xen_in_vm_overview.jpg in staging/2a4e519b0d91b8967b62
Preparing unikernel for uavexperiment.blog.images.MobileNode.png.www.skjegstad.com/MobileNode.png in staging/390a92f0488aa32f17e3
Preparing unikernel for uavexperiment.blog.images.image6.jpg.www.skjegstad.com/image6.jpg in staging/f9a90e263ac465d53558
Preparing unikernel for uavexperiment.blog.images.image1.jpg.www.skjegstad.com/image1.jpg in staging/fc21dfa295c33c6e3bd3
Preparing unikernel for uavexperiment.blog.images.image5.jpg.www.skjegstad.com/image5.jpg in staging/afc6634e807f1bddad22
Preparing unikernel for uavexperiment.blog.images.image4.jpg.www.skjegstad.com/image4.jpg in staging/1f7b97ef7d3a4a1ba0f4
Preparing unikernel for uavexperiment.blog.images.image3.jpg.www.skjegstad.com/image3.jpg in staging/4dedbc5bfcd11fe49a83
Preparing unikernel for uavexperiment.blog.images.image2.jpg.www.skjegstad.com/image2.jpg in staging/8f15fa23ae6563d2d890
Preparing unikernel for software.index.html.www.skjegstad.com/index.html in staging/855d32b8905249554f69
Preparing unikernel for macvim-and-funnel-pl.05.01.2012.blog.index.html.www.skjegstad.com/index.html in staging/cb72fc8606b734fd7322
Preparing unikernel for ardrone-test-flight-1.12.01.2012.blog.index.html.www.skjegstad.com/index.html in staging/570b6f207695db4d2898
Preparing unikernel for mirageos-xen-virtualbox.19.01.2015.blog.index.html.www.skjegstad.com/index.html in staging/9ada75a179f0d6c6cdff
Preparing unikernel for experimenting-with-distributed-chat.02.12.2011.blog.index.html.www.skjegstad.com/index.html in staging/0d0613c7f413609ab402
Preparing unikernel for a-stand-alone-java-bloom-filter.20.10.2011.blog.index.html.www.skjegstad.com/index.html in staging/c11e5af4b1a8ee667bf5
Preparing unikernel for virtualbox.categories.blog.index.html.www.skjegstad.com/index.html in staging/198f10ffb77757798200
Preparing unikernel for bloom-filters.categories.blog.index.html.www.skjegstad.com/index.html in staging/da3c099920c48f77b625
Preparing unikernel for mist.categories.blog.index.html.www.skjegstad.com/index.html in staging/ec590d332b74f3521787
Preparing unikernel for unikernel.categories.blog.index.html.www.skjegstad.com/index.html in staging/befa876942eb63d806d0
Preparing unikernel for experiments.categories.blog.index.html.www.skjegstad.com/index.html in staging/2d14e6010e04324f1fa9
Preparing unikernel for vim.categories.blog.index.html.www.skjegstad.com/index.html in staging/d09920f7a15c2f930686
Preparing unikernel for xen.categories.blog.index.html.www.skjegstad.com/index.html in staging/9bd2e2788f6309bc762c
Preparing unikernel for mirageos.categories.blog.index.html.www.skjegstad.com/index.html in staging/8b08718e16dddf30473f
Preparing unikernel for ardrone.categories.blog.index.html.www.skjegstad.com/index.html in staging/dfc57a0ce3fc7382a001
Preparing unikernel for java.categories.blog.index.html.www.skjegstad.com/index.html in staging/a13890b437ef10df36e4
Preparing unikernel for papers.skjegstad-mistsd-milcom2010.pdf.www.skjegstad.com/skjegstad_mistsd_milcom2010.pdf in staging/2b39a34888b769596fe0
Preparing unikernel for papers.milcom09skjegstad.pdf.www.skjegstad.com/milcom09skjegstad.pdf in staging/09854f35b4a6c9bafe01
Preparing unikernel for papers.milcom11-distributed-chat.pdf.www.skjegstad.com/milcom11_distributed_chat.pdf in staging/759daabffb7ac70f3c82
Preparing unikernel for publications.index.html.www.skjegstad.com/index.html in staging/2434cc8c7a5f966b2bc9
Preparing unikernel for about.index.html.www.skjegstad.com/index.html in staging/6ff5ef4c2d9236d7a398
Preparing unikernel for js.theme.modernizr-2.0.js.www.skjegstad.com/modernizr-2.0.js in staging/1b8ff82e33ce3d777d9e
Preparing unikernel for js.theme.octopress.js.www.skjegstad.com/octopress.js in staging/2e5e43546763b649cd3b
Preparing unikernel for js.theme.ender.js.www.skjegstad.com/ender.js in staging/edf18f0bba513af40346
Preparing unikernel for js.theme.github.js.www.skjegstad.com/github.js in staging/a577cb04bac38034dc95
Preparing unikernel for images.theme.noise.png.www.skjegstad.com/noise.png?1395516324 in staging/e9d12885cf941b2f149c
Preparing unikernel for images.theme.search.png.www.skjegstad.com/search.png?1395516324 in staging/c23144b5f096a13a71bb
Preparing unikernel for images.theme.code-bg.png.www.skjegstad.com/code_bg.png?1395516324 in staging/19e1de022f4a565bc1e2
Preparing unikernel for images.theme.line-tile.png.www.skjegstad.com/line-tile.png?1395516324 in staging/29ff05b603a5d0cdebdc
Preparing unikernel for images.theme.email.png.www.skjegstad.com/email.png?1395516324 in staging/4d4e8d94047cf9c45080
Preparing unikernel for images.theme.rss.png.www.skjegstad.com/rss.png?1395516324 in staging/433115bed52cbea861a0
Preparing unikernel for css.theme.main.css.www.skjegstad.com/main.css in staging/1bacfdf7b71099057d8c

Zone file generated in www.skjegstad.com.zone.

Makefile generated. Type 'make' to build, 'make run' to run and 'make destroy' to stop VMs. Start local DNS server with 'make dns'



When the script completes, a unikernel has been created in ./staging for each file in the www.skjegstad.com directory. To compile the unikernels using the mirage-tool, run make. Note that this requires a working MirageOS development environment and I have written a guide for setting this up here. This step will take some time, depending on how many pages you have. In my case, it was around 4 minutes in Virtualbox on my laptop.
When all the unikernels have been successfully built, make run will start each of them using Xen's xl tool. Use sudo xl list to see the running unikernels. This is the xl output for 49 MirageOS virtual machines hosting www.skjegstad.com on my local network:
$ sudo xl list
Name                                        ID   Mem VCPUsStateTime(s)
Domain-0                                     0   292     1     r-----    2156.1
85956378dc89cf3a2729                        39    32     1     -b----       0.1
f7fb9c68d5c3701996b3                        40    32     1     -b----       0.1
c286eccb0f59b299cca1                        41    32     1     -b----       0.1
5bef80428ab04abb98c3                        42    32     1     -b----       0.1
41d9ee421440ceec86a4                        43    32     1     -b----       0.1
a9fdfed9d054ee4d48de                        44    32     1     -b----       0.1
5ca0d87ae96d8648c36d                        45    32     1     -b----       0.1
5b1ce89d1c806c4234f2                        46    32     1     -b----       0.1
0d368ef42317b28f6243                        47    32     1     -b----       0.1
2a4e519b0d91b8967b62                        48    32     1     -b----       0.1
390a92f0488aa32f17e3                        49    32     1     -b----       0.1
f9a90e263ac465d53558                        50    32     1     -b----       0.1
fc21dfa295c33c6e3bd3                        51    32     1     -b----       0.1
afc6634e807f1bddad22                        52    32     1     -b----       0.1
1f7b97ef7d3a4a1ba0f4                        53    32     1     -b----       0.1
4dedbc5bfcd11fe49a83                        54    32     1     -b----       0.1
8f15fa23ae6563d2d890                        55    32     1     -b----       0.1
855d32b8905249554f69                        56    32     1     -b----       0.1
cb72fc8606b734fd7322                        57    32     1     -b----       0.1
570b6f207695db4d2898                        58    32     1     -b----       0.1
9ada75a179f0d6c6cdff                        59    32     1     -b----       0.1
0d0613c7f413609ab402                        60    32     1     -b----       0.1
c11e5af4b1a8ee667bf5                        61    32     1     -b----       0.1
198f10ffb77757798200                        62    32     1     -b----       0.1
da3c099920c48f77b625                        63    32     1     -b----       0.1
ec590d332b74f3521787                        64    32     1     -b----       0.1
befa876942eb63d806d0                        65    32     1     -b----       0.1
2d14e6010e04324f1fa9                        66    32     1     -b----       0.1
d09920f7a15c2f930686                        67    32     1     -b----       0.1
9bd2e2788f6309bc762c                        68    32     1     -b----       0.1
8b08718e16dddf30473f                        69    32     1     -b----       0.1
dfc57a0ce3fc7382a001                        70    32     1     -b----       0.1
a13890b437ef10df36e4                        71    32     1     -b----       0.1
2b39a34888b769596fe0                        72    32     1     -b----       0.1
09854f35b4a6c9bafe01                        73    32     1     -b----       0.0
759daabffb7ac70f3c82                        74    32     1     -b----       0.0
2434cc8c7a5f966b2bc9                        75    32     1     -b----       0.0
6ff5ef4c2d9236d7a398                        76    32     1     -b----       0.0
1b8ff82e33ce3d777d9e                        77    32     1     -b----       0.0
2e5e43546763b649cd3b                        78    32     1     -b----       0.0
edf18f0bba513af40346                        79    32     1     -b----       0.0
a577cb04bac38034dc95                        80    32     1     -b----       0.0
e9d12885cf941b2f149c                        81    32     1     -b----       0.0
c23144b5f096a13a71bb                        82    32     1     -b----       0.0
19e1de022f4a565bc1e2                        83    32     1     -b----       0.0
29ff05b603a5d0cdebdc                        84    32     1     -b----       0.0
4d4e8d94047cf9c45080                        85    32     1     -b----       0.0
433115bed52cbea861a0                        86    32     1     -b----       0.0
1bacfdf7b71099057d8c                        87    32     1     -b----       0.0



The create.py-script also generated a DNS zone file that maps the domain names to IP addresses. This is the generated zone file for www.skjegstad.com:
$ cat www.skjegstad.com.zone
@   IN  SOA     ns.www.skjegstad.com.  postmaster.www.skjegstad.com. (1 600 600 600 60)
www.skjegstad.com.  IN  CNAME   index.html.www.skjegstad.com.
index.html.www.skjegstad.com    IN  A   192.168.56.10
images.profile.jpg.www.skjegstad.com    IN  A   192.168.56.11
ardrone-experiment-1.blog.images.ardrone-test.m4v.www.skjegstad.com IN  A   192.168.56.12
ardrone-experiment-1.blog.images.camera-33.jpg.www.skjegstad.com    IN  A   192.168.56.13
ardrone-experiment-1.blog.images.ardrone-test.png.www.skjegstad.com IN  A   192.168.56.14
ardrone-experiment-1.blog.images.ardrone1.jpg.www.skjegstad.com IN  A   192.168.56.15
ardrone-experiment-1.blog.images.camera-28.jpg.www.skjegstad.com    IN  A   192.168.56.16
ardrone-experiment-1.blog.images.screenshot1.png.www.skjegstad.com  IN  A   192.168.56.17
mirage-dev.blog.images.ubuntu-in-vm-overview.jpg.www.skjegstad.com  IN  A   192.168.56.18
mirage-dev.blog.images.xen-in-vm-overview.jpg.www.skjegstad.com IN  A   192.168.56.19
uavexperiment.blog.images.MobileNode.png.www.skjegstad.com  IN  A   192.168.56.20
uavexperiment.blog.images.image6.jpg.www.skjegstad.com  IN  A   192.168.56.21
uavexperiment.blog.images.image1.jpg.www.skjegstad.com  IN  A   192.168.56.22
uavexperiment.blog.images.image5.jpg.www.skjegstad.com  IN  A   192.168.56.23
uavexperiment.blog.images.image4.jpg.www.skjegstad.com  IN  A   192.168.56.24
uavexperiment.blog.images.image3.jpg.www.skjegstad.com  IN  A   192.168.56.25
uavexperiment.blog.images.image2.jpg.www.skjegstad.com  IN  A   192.168.56.26
software.index.html.www.skjegstad.com   IN  A   192.168.56.27
macvim-and-funnel-pl.05.01.2012.blog.index.html.www.skjegstad.com   IN  A   192.168.56.28
ardrone-test-flight-1.12.01.2012.blog.index.html.www.skjegstad.com  IN  A   192.168.56.29
mirageos-xen-virtualbox.19.01.2015.blog.index.html.www.skjegstad.com    IN  A   192.168.56.30
experimenting-with-distributed-chat.02.12.2011.blog.index.html.www.skjegstad.com    IN  A   192.168.56.31
a-stand-alone-java-bloom-filter.20.10.2011.blog.index.html.www.skjegstad.com    IN  A   192.168.56.32
virtualbox.categories.blog.index.html.www.skjegstad.com IN  A   192.168.56.33
bloom-filters.categories.blog.index.html.www.skjegstad.com  IN  A   192.168.56.34
mist.categories.blog.index.html.www.skjegstad.com   IN  A   192.168.56.35
unikernel.categories.blog.index.html.www.skjegstad.com  IN  A   192.168.56.36
experiments.categories.blog.index.html.www.skjegstad.com    IN  A   192.168.56.37
vim.categories.blog.index.html.www.skjegstad.com    IN  A   192.168.56.38
xen.categories.blog.index.html.www.skjegstad.com    IN  A   192.168.56.39
mirageos.categories.blog.index.html.www.skjegstad.com   IN  A   192.168.56.40
ardrone.categories.blog.index.html.www.skjegstad.com    IN  A   192.168.56.41
java.categories.blog.index.html.www.skjegstad.com   IN  A   192.168.56.42
papers.skjegstad-mistsd-milcom2010.pdf.www.skjegstad.com    IN  A   192.168.56.43
papers.milcom09skjegstad.pdf.www.skjegstad.com  IN  A   192.168.56.44
papers.milcom11-distributed-chat.pdf.www.skjegstad.com  IN  A   192.168.56.45
publications.index.html.www.skjegstad.com   IN  A   192.168.56.46
about.index.html.www.skjegstad.com  IN  A   192.168.56.47
js.theme.modernizr-2.0.js.www.skjegstad.com IN  A   192.168.56.48
js.theme.octopress.js.www.skjegstad.com IN  A   192.168.56.49
js.theme.ender.js.www.skjegstad.com IN  A   192.168.56.50
js.theme.github.js.www.skjegstad.com    IN  A   192.168.56.51
images.theme.noise.png.www.skjegstad.com    IN  A   192.168.56.52
images.theme.search.png.www.skjegstad.com   IN  A   192.168.56.53
images.theme.code-bg.png.www.skjegstad.com  IN  A   192.168.56.54
images.theme.line-tile.png.www.skjegstad.com    IN  A   192.168.56.55
images.theme.email.png.www.skjegstad.com    IN  A   192.168.56.56
images.theme.rss.png.www.skjegstad.com  IN  A   192.168.56.57
css.theme.main.css.www.skjegstad.com    IN  A   192.168.56.58



The zone file can be loaded in your favorite DNS server (e.g. bind or unbound). It is also relatively easy to build a quick DNS server in OCaml based on ocaml-dns:
open Dns
open Lwt

let () =
    Lwt_main.run (
        let address = "0.0.0.0" in
        let port = 53 in
        Dns_server_unix.serve_with_zonefile ~address ~port "www.skjegstad.com.zone"
    )



I have written an extended version with support for forwarding unknown DNS requests to another DNS server. It is available for download here and is included in the demo code. In fact, you could also run the DNS server as a unikernel, but I will leave that as an exercise for the reader (this blog post by Heidi Howard is a good starting point with lots of examples).
Point your DNS server to the IP of the DNS server that hosts the zone file to start browsing the static website. Each static page should map to a separate domain name which is run by a single MirageOS unikernel!
Conclusion / discussion
Currently this is just a quick experiment and is not very practical. I would still need a unikernel or Linux VM to run the DNS server and I have not been able to find a hosting provider that has a minimum configuration which would be appropriate for large or small unikernels (e.g. 8MB vs 64MB RAM). Each VM/URL would also require a unique public IP address unless they are placed behind another service (NAT) that can route the HTTP requests to an internal network. 
If you followed along, then you would also notice that the compiled unikernels in ./staging take up around 175MB (49 unikernels). When running, they collectively consume around 1.6GB of memory. However, it was a lot easier to try this experiment using unikernels than it would have been with a traditional OS stack.
A more practical architecture would perhaps be to run a website and DNS server from a single host initially. As each web page is mapped to a URL, the DNS server could then be configured to automatically boot new unikernels only if a subdomain experienced high load. It could, for example, deploy unikernels automatically to Amazon EC2 that serve a single file and start returning its IP address in DNS queries in periods with high load. Once the website has been 'disaggregated' this way, it would even be possible to start VMs in availability zones that are geographically close to the where the spike in traffic is originating!  This could all be configured to happen automatically, with DNS entries being updated as new pages are added and with unikernels being deployed to places where they are in demand.
(Thanks to Amir Chaudry for commenting on a draft of this post)Hide
        
      
                    by Magnus Skjegstad at Mar 25, 2015 
      
      
    
  


       
                  Part 3: Running your own DNS Resolver with MirageOS
      (Heidi Howard)
    
    
                                This article is the third in the “Running your own DNS Resolver with MirageOS” series. In the first part, we used the ocaml-dns library to lookup the hostname corresponding with an IP address using its Dns_resolver_mirage module. In the second part, we wrote a simple DNS server, which serves RRs from a zone file using the Dns_server_mirage module.
Today in the third part, we will combine the above to write a simple DNS resolver, which relays queries to another DNS resolver. Then we will compose this with our simple DNS server from last week, to build a resolver which first looks up queries in the host file and if unsuccessful will relay the query to another DNS resolver.
As always, the complete code for these examples is in ocaml-dns-examples.
3.1 DNS FoRwarder
When writing our simple DNS server, we used a function called serve_with_zonefile in Dns_server_mirage to service incoming DNS queries. Now we are going remove a layer of abstraction and instead use serve_with_proces…Read more...This article is the third in the “Running your own DNS Resolver with MirageOS” series. In the first part, we used the ocaml-dns library to lookup the hostname corresponding with an IP address using its Dns_resolver_mirage module. In the second part, we wrote a simple DNS server, which serves RRs from a zone file using the Dns_server_mirage module.
Today in the third part, we will combine the above to write a simple DNS resolver, which relays queries to another DNS resolver. Then we will compose this with our simple DNS server from last week, to build a resolver which first looks up queries in the host file and if unsuccessful will relay the query to another DNS resolver.
As always, the complete code for these examples is in ocaml-dns-examples.
3.1 DNS FoRwarder
When writing our simple DNS server, we used a function called serve_with_zonefile in Dns_server_mirage to service incoming DNS queries. Now we are going remove a layer of abstraction and instead use serve_with_processor:
val serve_with_processor: t -> port:int -> processor:(module PROCESSOR) -> unit Lwt.t
val serve_with_zonefile : t -> port:int -> zonefile:string -> unit Lwt.t

Now instead of passing the function a simple string, representing the filename of zonefile, we pass a first class module, satisfying the PROCESSOR signature. We can generate such a module by writing a process and using processor_of_process:
type ip_endpoint = Ipaddr.t * int

type 'a process = src:ip_endpoint -> dst:ip_endpoint -> 'a -> Dns.Query.answer option Lwt.t

module type PROCESSOR = sig
  include Dns.Protocol.SERVER

  (** DNS responder function.
      @param src Server sockaddr
      @param dst Client sockaddr
      @param Query packet
      @return Answer packet
  *)
  val process : context process
end

type 'a processor = (module PROCESSOR with type context = 'a)

val processor_of_process : Dns.Packet.t process -> Dns.Packet.t processor
So given a Dns.Packet.t process, which is a function of type:
src:ip_endpoint -> dst:ip_endpoint -> Dns.Packet.t -> Dns.Query.answer option Lwt.t
We can now service DNS packets. If we assume that myprocess is a function of this type, we can service DNS queries with the following unikernel
open Lwt
open V1_LWT
open Dns
open Dns_server

let port = 53

module Main (C:CONSOLE) (K:KV_RO) (S:STACKV4) = struct

  module U = S.UDPV4
  module DS = Dns_server_mirage.Make(K)(S)

  let myprocess ~src ~dst packet = ...

  let start c k s =
    let server = DS.create s k in
    let processor = ((Dns_server.processor_of_process myprocess) :> (module Dns_server.PROCESSOR)) in 
    DS.serve_with_processor server ~port ~processor
end


Now we will write an implementation of myprocess which will service DNS packets by forwarding them to another DNS resolver and then relaying the response.
Recall from part 1, that you can use the resolve function in Dns_resolver_mirage to do this. All that remains is to wrap invocation of resolve, in a function of type Dns.Packet.t process, which can be done as follows:
 
let process resolver ~src ~dst packet =
      let open Packet in
      match packet.questions with
      | [] -> (* we are not supporting QDCOUNT = 0  *)
          return None 
      | [q] -> 
         DR.resolve (module Dns.Protocol.Client) resolver 
         resolver_addr resolver_port q.q_class q.q_type q.q_name 
          >>= fun result ->
          return (Some (Dns.Query.answer_of_response result))) 
      | _ -> (* we are not supporting QDCOUNT > 1 *)
          return None
3.2 DNS server & forwarder
[this part requires PR 58 on ocaml-dns until it is merged in]
We will extend our DNS forwarded to first check a zonefile, this is achieve with just 3 extra lines:
...
DS.eventual_process_of_zonefiles server [zonefile]
>>= fun process ->
let processor = (processor_of_process (compose process (forwarder resolver)) :> (module Dns_server.PROCESSOR)) in
...

Here we are using compose to use two processes: one called process generated from the zonefile and one called forwarder, from the forwarding code in the last section.
Next time, we will extend our DNS resolver to include a cache.
 
 
Hide
        
      
                    by Heidi Howard at Mar 19, 2015 
      
      
    
  


       
                  Part 2: Running your own DNS Resolver with MirageOS
      (Heidi Howard)
    
    
                                Last time, we wrote a simple “dig like” unikernel. Given a domain and the address of a nameserver, the unikernel resolved the domain by asking the nameserver and returned the return to the console.
Today, we will look at another way to resolve a DNS query, being a DNS server. This is useful in its own right but also allows us to cool things with our local DNS resolver such as locally overwriting DNS names and resolving .local names, both of which we will add to our DNS resolver another day.
Today we use features only added to ocaml-dns library in version 0.15 (currently PR #52), so if you do not have this version or later, then update OPAM or pin the master branch on github.
Building a DNS server with MirageOS is simple, look at the following code:
open Lwt
open V1_LWT
open Dns
open Dns_server

let port = 53
let zonefile = "test.zone"

module Main (C:CONSOLE) (K:KV_RO) (S:STACKV4) = struct

  module U = S.UDPV4
  module DNS = Dns_server_mirage.Make(K)(S)

  let start c k s =
    …Read more...Last time, we wrote a simple “dig like” unikernel. Given a domain and the address of a nameserver, the unikernel resolved the domain by asking the nameserver and returned the return to the console.
Today, we will look at another way to resolve a DNS query, being a DNS server. This is useful in its own right but also allows us to cool things with our local DNS resolver such as locally overwriting DNS names and resolving .local names, both of which we will add to our DNS resolver another day.
Today we use features only added to ocaml-dns library in version 0.15 (currently PR #52), so if you do not have this version or later, then update OPAM or pin the master branch on github.
Building a DNS server with MirageOS is simple, look at the following code:
open Lwt
open V1_LWT
open Dns
open Dns_server

let port = 53
let zonefile = "test.zone"

module Main (C:CONSOLE) (K:KV_RO) (S:STACKV4) = struct

  module U = S.UDPV4
  module DNS = Dns_server_mirage.Make(K)(S)

  let start c k s =
    let t = DNS.create s k in
    DNS.serve_with_zonefile t ~port ~zonefile
end

The above code will serve DNS requests to port 53, responding with the resource records (RR) in test.zone. We have provided an example zone file in the repo with the code from this guide. To use this unikernel, we also need to edit the config.ml file from yesterday.
open Mirage

let data = crunch "./data"

let handler =
  foreign "Unikernel.Main" (console @-> kv_ro @-> stackv4 @-> job)

let ip_config:ipv4_config = {
  address= Ipaddr.V4.make 192 168 1 2;
  netmask= Ipaddr.V4.make 255 255 255 0;
  gateways= [Ipaddr.V4.make 192 168 1 1];
}

let direct =
  let stack = direct_stackv4_with_static_ipv4 default_console tap0 ip_config  in
  handler $ default_console $ data $ stack

let () =
  add_to_ocamlfind_libraries ["dns.mirage";"dns.lwt-core"];
  add_to_opam_packages ["dns"];
  register "dns" [direct]

We are using crunch to access the zone file in the data directory. As explain in part 1, this config file is specific to my network setup for xen backends and can easily be generalised.
You can now test your DNS server and see it work
$ dig @192.168.1.2 ns0.d1.signpo.st.
 
Hide
        
      
                    by Heidi Howard at Mar 02, 2015 
      
      
    
  


       
                  Part 1: Running your own DNS Resolver with MirageOS
      (Heidi Howard)
    
    
                                The following is the first part in a step-by-step guide to setting up your own DNS resolver using MirageOS. I will be running this on a low power, low cost ARM device called the Cubieboard 2. Up to date code for each version of the DNS resolver is on Github. This guide assumes some basic experience of lwt and MirageOS, up to the level of the Hello World Tutorial.
Feedback on this article and pull requests to the demo code are welcome.
Part 1.1 – Setting up the cubieboard with MirageOS
Plenty of information on setting up a cubieboard with Xen and MirageOS is available elsewhere, most notability:
MirageOS Hello World Tutorial & Code
Cubieboard2 Xen Binaries

For debugging I am a big fan for wireshark. I run a full wireshark sesson on the machine which is connection sharing to my cubieboard network, to check all external traffic.
For this guide, I will always be compiling for Xen ARM backend, with direct network connection via br0 and a static IP for all unikernels. My test networ…Read more...The following is the first part in a step-by-step guide to setting up your own DNS resolver using MirageOS. I will be running this on a low power, low cost ARM device called the Cubieboard 2. Up to date code for each version of the DNS resolver is on Github. This guide assumes some basic experience of lwt and MirageOS, up to the level of the Hello World Tutorial.
Feedback on this article and pull requests to the demo code are welcome.
Part 1.1 – Setting up the cubieboard with MirageOS
Plenty of information on setting up a cubieboard with Xen and MirageOS is available elsewhere, most notability:
MirageOS Hello World Tutorial & Code
Cubieboard2 Xen Binaries

For debugging I am a big fan for wireshark. I run a full wireshark sesson on the machine which is connection sharing to my cubieboard network, to check all external traffic.
For this guide, I will always be compiling for Xen ARM backend, with direct network connection via br0 and a static IP for all unikernels. My test network router is configured to give out static IP of the form 192.168.1.x to hosts with the MAC address 00:00:00:00:00:0x. As a result, my config.ml file look like:
open Mirage

let ip_config:ipv4_config = {
  address= Ipaddr.V4.make 192 168 1 2;
  netmask= Ipaddr.V4.make 255 255 255 0;
  gateways= [Ipaddr.V4.make 192 168 1 1];
}

let client =
  foreign "Unikernel.Client" @@ console @-> stackv4 @-> job

let () =
  add_to_ocamlfind_libraries [ "dns.mirage"; ];
  register "dns-client" 
[ client $ default_console $ direct_stackv4_with_static_ipv4 default_console tap0 ip_config]

Since the IP address of the unikernel is 192.168.1.2, before launching the unikernel, I do:
echo "vif = [ 'mac=00:00:00:00:00:02,bridge=br0' ]" >> dns-client.xl

I build unikernel using the usual commands:
mirage configure --xen
make depend; make; make run
# edit file.xl
sudo xl create -c file.xl

Part 1.2 – Getting Started
The following is the complete code for a unikernel which queries a DNS server for a DNS domain and prints to console the IP address returned.
open Lwt
open V1_LWT

let domain = "google.com"
let server = Ipaddr.V4.make 8 8 8 8

module Client (C:CONSOLE) (S:STACKV4) = struct

  module U = S.UDPV4
  module DNS = Dns_resolver_mirage.Make(OS.Time)(S)

  let start c s =
    let t = DNS.create s in
    OS.Time.sleep 2.0 
    >>= fun () ->
    C.log_s c ("Resolving " ^ domain)
    >>= fun () ->
    DNS.gethostbyname t ~server domain
    >>= fun rl ->
    Lwt_list.iter_s
      (fun r ->
         C.log_s c ("Answer " ^ (Ipaddr.to_string r))
      ) rl

end

This unikernel will query a DNS server at 8.8.8.8 (google public DNS resolver) for a domain google.com. Here we are using the simple function, DNS.gethostbyname, with the following type sig:
  val gethostbyname : t ->
    ?server:Ipaddr.V4.t -> ?dns_port:int ->
    ?q_class:Dns.Packet.q_class ->
    ?q_type:Dns.Packet.q_type ->
    string -> Ipaddr.t list Lwt.t

This returns a list of IP’s, which we then iterative over with Lwt_list.iter_s and print to the console.
Part 1.3 – Boot time parameters
Hardcoding the server and domain is far from ideal, instead we will provide them at boot time with Bootvar, the interface for bootvar is below:
type t
(* read boot parameter line and store in assoc list - expected format is "key1=val1 key2=val2" *)
val create: unit -> t Lwt.t

(* get boot parameter *)
val get: t -> string -> string option

(* get boot parameter, throws Not Found exception *)
val get_exn: t -> string -> string

We can now use this to provide domain and server at boot time instead of compile time
let start c s =
    Bootvar.create () >>= fun bootvar ->
    let domain = Bootvar.get_exn bootvar "domain" in
    let server = Ipaddr.V4.of_string_exn (Bootvar.get_exn bootvar "server") in
    ...

Part 1.4 – Using Resolve
Now, a real DNS resolver will need to make many more parameters (any DNS query) and return full DNS responses not just IP address. Thus we need to move on from DNS.hostbyname to using the less abstract resolve function, resolve:
  val resolve :
    (module Dns.Protocol.CLIENT) ->
    t -> Ipaddr.V4.t -> int ->
    Dns.Packet.q_class ->
    Dns.Packet.q_type ->
    Dns.Name.domain_name ->
    Dns.Packet.t Lwt.t 

We can achieve same result of hostbyname as follows:
...
    DNS.resolve (module Dns.Protocol.Client) t server 53 Q_IN Q_A (string_to_domain_name domain)
    >>= fun r ->
    let ips =
    List.fold_left (fun a x ->
      match x.rdata with
      | A ip -> (Ipaddr.V4 ip) :: a
      | _ -> a ) [] r.answers in
...

We are now explicit about parameters such as port, class and type. Note that we have opened the Dns.Name and Dns.Packet.t modules. The return value of resolve is a Dns.Packet.t, we fold over answers in the produce an IPaddr.V4 list as with hostbyname. We can also use the to_string function in Packet to print
I’ve taken a break to do some refactoring work on the ocaml-dns library. In the next post, Part 2, we will expand our code to a DNS stub resolver.
 
Hide
        
      
                    by Heidi Howard at Feb 18, 2015 
      
      
    
  


       
                  Smash the Bitcoin Pinata for fun and profit!
      (Mirage OS)
    
    
                                
      
Last summer we announced the beta release of a clean-slate implementation of
TLS in pure OCaml, alongside a series of blog posts that described
the libraries and the thinking behind them.  It took two hackers six months
— starting on the beach —  to get the stack to that point and
their demo server is still going strong. Since then, the team has
continued working and recently presented at the 31st Chaos
Communication Congress.
The authors are putting their stack to the test again and this time they've
built a Bitcoin Piñata! Essentially, they've hidden a
private key to a bitcoin address within a Unikernel running on Xen. If you're
able to smash your way in, then you get to keep the spoils.
There's more context around this in my Piñata post and you can see
the details on the site itself. Remember that the codebase is
all open (as well as issues) so there's nothing to
reverse engineer. Have fun!

   
        
      
                    by Amir Chaudhry at Feb 10, 2015 
      
      
    
  


       
                  South of England Regional Programming Language Seminar (S-REPLS)
      (Compiler Hacking)
    
    
                                If you're within travelling distance of Cambridge and interested in
programming language theory then you might like to
sign up to come along to the first
South of England Regional Programming Language Seminar
at Wolfson College on the 30th April.  We
have an exciting programme, starting with an invited talk from Conor McBride:

The dependent lollipop (Conor McBride, Strathclyde)
Improving implicit parallelism (Jose Calderon, York)
Many-core compiler fuzzing (Alastair Donaldson, Imperial)
Fine-grained language composition (Laurence Tratt, King's College London)
Graphical models of concurrent program execution (Tony Hoare, Microsoft)


Further information is available on the
S-REPLS website.

        
      
                    by Compiler Hacking at Feb 05, 2015 
      
      
    
  


       
                  Ninth OCaml compiler hacking evening (back in the lab, with a talk from Oleg)
      (Compiler Hacking)
    
    
                                We'll be meeting in the Computer Lab next Tuesday (10th February 2015) for another evening of compiler hacking.  All welcome!  Please add yourself to the Doodle poll if you're planning to come along, and sign up to the mailing list to receive updates.

Talk: Generating code with polymorphic let (Oleg Kiselyov)

This time we'll be starting with a talk from Oleg Kiselyov:

Generating code with polymorphic let

One of the simplest ways of implementing staging is source-to-source
translation from the quotation-unquotation code to code-generating
combinators. For example, MetaOCaml could be implemented as a
pre-processor to the ordinary OCaml. However simple, the approach is
surprising productive and extensible, as Lightweight Modular Staging
(LMS) in Scala has demonstrated. However, there is a fatal flaw:
handling quotations that contain polymorphic let. The translation to
code-generating combinators represents a future-stage let-binding with
the present-staging lambda-binding, which is m…Read more...We'll be meeting in the Computer Lab next Tuesday (10th February 2015) for another evening of compiler hacking.  All welcome!  Please add yourself to the Doodle poll if you're planning to come along, and sign up to the mailing list to receive updates.

Talk: Generating code with polymorphic let (Oleg Kiselyov)

This time we'll be starting with a talk from Oleg Kiselyov:

Generating code with polymorphic let

One of the simplest ways of implementing staging is source-to-source
translation from the quotation-unquotation code to code-generating
combinators. For example, MetaOCaml could be implemented as a
pre-processor to the ordinary OCaml. However simple, the approach is
surprising productive and extensible, as Lightweight Modular Staging
(LMS) in Scala has demonstrated. However, there is a fatal flaw:
handling quotations that contain polymorphic let. The translation to
code-generating combinators represents a future-stage let-binding with
the present-staging lambda-binding, which is monomorphic. Even if
polymorphic lambda-bindings are allowed, they require type
annotations, which precludes the source-to-source translation.

We show the solution to the problem, using a different translation. It
works with the current OCaml. It also almost works in theory,
requiring a small extension to the relaxed value
restriction. Surprisingly, this extension seems to be exactly the one
needed to make the value restriction sound in a staged language with
reference cells and cross-stage-persistence.

The old, seems completely settled question of value restriction is
thrown deep-open in staged languages. We gain a profound problem to
work on.


(Approximate) schedule

6pm Start, set up6.30pm Talk7pm Pizza7.30pm-10pm Compiler hacking  

Further details

Where:
  Room FW11, Computer Laboratory, Madingley Road

When: 6pm, Tuesday 10th February

Who: anyone interested in improving OCaml. Knowledge of OCaml programming will obviously be helpful, but prior experience of working on OCaml internals isn't necessary.

What: fixing bugs, implementing new features, learning about OCaml internals.

Wiki: https://github.com/ocamllabs/compiler-hacking/wiki

We're defining "compiler" pretty broadly, to include anything that's part of the standard distribution, which means at least the standard library, runtime, tools (ocamldep, ocamllex, ocamlyacc, etc.), ocamlbuild, the documentation, and the compiler itself. We'll have suggestions for mini-projects for various levels of experience (see also some things we've done on previous evenings), but feel free to come along and work on whatever you fancy.

We'll be ordering pizza, so if you want to be counted for food you should aim to arrive by 6pm.
Hide
        
      
                    by Compiler Hacking at Feb 05, 2015 
      
      
    
  


       
                  Securing the Unikernel
      (Thomas Leonard)
    
    
                                Back in July, I used MirageOS to create my first unikernel, a simple REST service for queuing file uploads, deployable as a virtual machine.
While a traditional VM would be a complete Linux system (kernel, init system, package manager, shell, etc), a Mirage unikernel is a single OCaml program which pulls in just the features (network driver, TCP stack, web server, etc) it needs as libraries.
Now it’s time to look at securing the system with HTTPS and access controls, ready for deployment.



Table of Contents

  Introduction
  OCaml
  Xen
  Transport Layer Security          C stubs for Xen
      Ethernet frame alignment
      HTTP API
      The private key
      The partition code
      Entropy
    
  
  Access control          Python client
    
  
  Conclusions


( this post also appeared on Hacker News and
  Reddit )

Introduction

As a quick reminder, the service (“Incoming queue”) accepts uploads from various contributors and queues them until the (firewalled) repository …Read more...Back in July, I used MirageOS to create my first unikernel, a simple REST service for queuing file uploads, deployable as a virtual machine.
While a traditional VM would be a complete Linux system (kernel, init system, package manager, shell, etc), a Mirage unikernel is a single OCaml program which pulls in just the features (network driver, TCP stack, web server, etc) it needs as libraries.
Now it’s time to look at securing the system with HTTPS and access controls, ready for deployment.



Table of Contents

  Introduction
  OCaml
  Xen
  Transport Layer Security          C stubs for Xen
      Ethernet frame alignment
      HTTP API
      The private key
      The partition code
      Entropy
    
  
  Access control          Python client
    
  
  Conclusions


( this post also appeared on Hacker News and
  Reddit )

Introduction

As a quick reminder, the service (“Incoming queue”) accepts uploads from various contributors and queues them until the (firewalled) repository software downloads them, checks the GPG signatures, and merges them into the public software repository, signed with the repository’s key:



Although the queue service isn’t security critical, since the GPG signatures are made and checked elsewhere, I would like to ensure it has a few properties:

  Only the repository can fetch items from the queue.
  Only authorised users can upload to it.
  I can see where an upload came from and revoke access if necessary.
  An attacker cannot take control of the system and use it to attack other systems.


We often think of security as a set of things we want to prevent - taking away possible actions from a fundamentally vulnerable underlying system (such as my original implementation, which had no security features).
But ideally I’d like every component of the system to be isolated by default, with allowed interactions (shown here by arrows) specified explicitly.
We should then be able to argue (informally) that the system will meet the goals above without having to verify that every line of code is correct.

My unikernel is written in OCaml and runs as a guest OS under the Xen hypervisor, so let’s look at how well those technologies support isolation first…

OCaml

I want to isolate the components of my unikernel, giving each just the access it requires.
When writing an OS, some unsafe code will occasionally be needed, but it should be clear which components use unsafe features (so they can be audited more carefully), and unsafe features shouldn’t be needed often.

For example, the code for handling an HTTP upload request should only be able to use our on-disk queue’s Uploader interface and its own HTTP connection.
Then we would know that an attacker with upload permission can only cause new items to be added to the queue, no matter how buggy that code is.
It should not be able to read the web server’s private key, establish new out-bound connections, corrupt the disk, etc.

Like most modern languages, OCaml is memory-safe, so components can’t interfere with each other through buggy pointer arithmetic or unsafe casts of the kind commonly found in C code.

But we also need to avoid global variables, which would allow two components to communicate without us explicitly connecting them.
I can’t reason about the security of the system by looking at arrows in the architecture diagrams if unconnected components can magically create new arrows by themselves!
I’ve seen a few interesting approaches to this problem (please correct me if I’ve got this wrong):

  Haskell
  Haskell avoids all side-effects, which makes global variables impossible (without using “unsafe” features), since updating them would be a side-effect.
  Rust
  Rust almost avoids the problem of globals through its ownership types.
Since only one thread can have a pointer to a mutable value at a time, mutable values can’t normally be global.
Rust does allow “mutable statics” (which contain only pointers to fixed data), but requires an explicit “unsafe” block to use them, which is good.
  E
  E allows modules to declare top-level variables, but each time the module is imported it creates a fresh copy.


OCaml does allow global variables, but by convention they are generally not used.

A second problem is controlling access to the outside world, including the network and disks (which you could consider to be more global variables):

  Haskell
  Haskell doesn’t allow functions to access the outside world at all, but they can return an IO type if they want to do something (the caller must then pass this value up to the top level).
This makes it easy to see that e.g. evaluating a function “uriPath :: URI -> String” cannot access the network.
However, it appears that all IO gets lumped in together: a value of type IO String may cause any side-effects at all (disk, network, etc), so the entire side-effecting part of the program needs to be audited.
  Rust
  Rust allows all code full access to the system via its standard library.
For example, any code can read or write any file.
  E
  E passes all access to the outside world to the program’s entry point.
For example, <file> grants access to the file system and <unsafe> grants access to all unsafe features.
These can be passed to libraries to grant them access, and can be attenuated (wrapped) to provide limited access.


For example:

main.e 
1
2
def queue := <import:makeQueue>(<file:/var/myprog/queue>)
...


Here, queue has read-write access to the /var/myprog/queue sub-tree (and nothing else).
It also has no way to share data with any other parts of the program, including other queues.

Like Rust, OCaml does not limit access to the outside world.
However, Mirage itself uses E-style dependency injection everywhere, with the unikernel’s start function being passed all external resources as arguments:

unikernel.ml 
1
2
3
4
5
6
7
8
9
10
module Main (C : V1_LWT.CONSOLE)
            (B : V1_LWT.BLOCK)
            (H : Cohttp_lwt.Server) =
  struct
    module Q = Upload_queue.Make(B)
    let start console block http =
      lwt queue = Q.create block in
      ...
  end


Because everything in Mirage is defined using abstract types, libraries always expect to be passed the things they need explicitly.
We know that Upload_queue above won’t access a block device directly because it needs to support different kinds of block device.

OCaml does enforce its abstractions.
There’s no way for the Upload_queue to discover that block is really a Xen block device with some extra functionality
(as a Java program might do with if (block instanceof XenBlock), for example).
This means that we can reason about the limits of what functions may do by looking only at their type signatures.

The use of functors means you can attenuate access as desired.
For example, if we want to grant just part of block to the queue then we can create our own module implementing the BLOCK type that exposes just some partition of the device, and pass that to the Upload_queue.Make functor.

In summary then, we can reason about security fairly well in Mirage if we assume the libraries are not malicious, but we cannot make hard guarantees.
It should be possible to check with automatic static analysis that we’re not using any “unsafe” features such as global variables, direct access to devices, or allocating uninitialised memory, but I don’t know of any tools to do that (except Emily, but that seems to be just a proof-of-concept).
But these issues are minor: any reasonably safe modern language will be a huge improvement over legacy C or C++ systems!

Xen

The Xen hypervisor allows multiple guest operating systems to run on a single physical machine.
It is used by many cloud hosting providers, including Amazon’s AWS.
I run it on my CubieTruck - a small ARM board.
Xen allows me to run my unikernel on the same machine as other services, but ideally with the same security properties as if it had its own dedicated machine.
If some other guest on the machine is compromised, it shouldn’t affect my unikernel, and if the unikernel is compromised then it shouldn’t affect other guests.

A typical Xen deployment, running four domains.

The diagram above shows a deployment with Linux and Mirage guests. Only dom0 has access to the physical hardware; the other guests only see virtual devices, provided by dom0.

How secure is Xen?
The Xen security advisories page shows that there are about 3 new Xen advisories each month.
However, it’s hard to compare programs this way because the number of vulnerabilities reported depends greatly on the number of people using the program, whether they use it for security-critical tasks, and how seriously the project takes problems (e.g. whether a denial-of-service attack is considered a security bug).

I started using Xen in April 2014.
These are the security problems I’ve found myself so far:

  XSA-93 Hardware features unintentionally exposed to guests on ARM
  While trying to get Mini-OS to boot, I tried implementing ARM’s recommended boot code for invalidating the cache
(at startup, you should invalidate the cache, dropping any previous contents).
When running under a hypervisor this should be a null-op since the cache is always valid by the time a guest is running, but Xen allowed the operation to go ahead, discarding pending writes by the hypervisor and other guests and crashing the system.
This could probably be used for a successful attack.
  XSA-94 : Hypervisor crash setting up GIC on arm32
  I tried to program an out-of-range register, causing the hypervisor to dereference a null pointer and panic.
This is just a denial of service (the host machine needs to be rebooted); it shouldn’t allow access to other VMs.
  XSA-95 : Input handling vulnerabilities loading guest kernel on ARM
  I got the length wrong when creating the zImage for the unikernel (I included the .bss section in the length).
The xl create tool didn’t notice and tried to read the extra data, causing the tool to segfault.
You could use this to read a bit of private data from the xl process, but it’s unlikely there would be anything useful there.


Although that’s more bugs than you might expect, note that they’re all specific to the relatively new ARM support.
The second and third are both due to using C, and would have been avoided in a safer language.
I’m not really sure why the “xl” tool needs to be in C - that seems to be asking for trouble.

To drive the physical hardware, Xen runs the first guest (dom0) with access to everything.
This is usually Linux, and I had various problems with that. For example:

  Bug sharing the same page twice
  Linux got confused when the unikernel shared the same page twice (it split a single page of RAM into multiple TCP segments).
This wasn’t a security bug (I think), but after it was fixed, I then got:
  Page still granted
  If my unikernel sent a network packet and then exited quickly, dom0 would get stuck and I’d have to reboot.
  Oops if network is used too quickly
  The Linux dom0 initialises the virtual network device while the VM is booting.
My unikernel booted fast enough to send packets before the device structure had been fully filled in, leading to an oops.
  Linux Dom0 oops and reboot on indirect block requests
  Sending lots of block device requests to Linux dom0 from the unikernel would cause Linux to oops in swiotlb_tbl_unmap_single and reboot the host.
I wasn’t the first to find this though, and backporting the patch from Linux 3.17 seemed to fix it (I don’t actually know what the problem was).


So it might seem that using Xen doesn’t get us very far.
We’re still running Linux in dom0, and it still has full access to the machine.
For example, a malicious network packet from outside or from a guest might still give an attacker full control of the machine.
Why not just use KVM and run the guests under Linux directly?

The big (potential) advantage of Xen here is Dom0 Disaggregation.
With this, Dom0 gives control of different pieces of physical hardware to different VMs rather than driving them itself.
For example, Qubes (a security-focused desktop OS using Xen) runs a separate “NetVM” Linux guest just to handle the network device.
This is connected only to the FirewallVM - another Linux guest that just routes packets to other VMs.

This is interesting for two reasons.
First, if an attacker exploits a bug in the network device driver, they’re still outside your firewall.
Secondly, it provides a credible path to replacing parts of Linux with alternative implementations, possibly written in safer languages.
You could, for example, have Linux running dom0 but use FreeBSD to drive the network card, Mirage to provide the firewall, and OpenBSD to handle USB.

Finally, it’s worth noting that Mirage is not tied to Xen, but can target various systems (mainly Unix and Xen currently, but there is some JavaScript support too).
If it turns out that e.g. Genode on seL4 (a formally verified microkernel) provides better security, we should be able to support that too.

Transport Layer Security

We won’t get far securing the system while attackers can read and modify our communications.
The ocaml-tls project provides an OCaml implementation of TLS (Transport Layer Security), and
in September Hannes Mehnert showed it running on Mirage/Xen/ARM devices.
Given the various flaws exposed recently in popular C TLS libraries, an OCaml implementation is very welcome.
Getting the Xen support in a state where it could be widely used took a bit of work, but
I’ve submitted all the patches I made, so it should be easier for other people now - see https://github.com/mirage/mirage-dev/pull/52.

C stubs for Xen

TLS needs some C code for the low-level cryptographic functions, which have to be constant time to avoid leaking information about the key, so first I had to make packages providing versions of libgmp, ctypes, zarith and nocrypto compiled to run in kernel mode.

The reason you need to compile C programs specially to run in kernel mode is because on x86 processors user mode code can assume the existence of a red zone, which allows some optimisations that aren’t safe in kernel mode.

Ethernet frame alignment

The Mirage network driver sends Ethernet frames to dom0 by sharing pages of memory.
Each frame must therefore be contained in a single page.
The TLS code was (correctly) passing a large buffer to the TCP layer, which incorrectly asked the network device to send each TCP-sized chunk of it.
Chunks overlapping page boundaries then got rejected.

My previous experiments with tracing the network layer had shown that we actually share two pages for each packet: one for the IP header and one for the payload.
Doing this avoids the need to copy the data to a new buffer, but adds the overhead of granting and revoking access to both pages.
I modified the network driver to copy the data into a single block inside a single page and got a large speed boost.
Indeed, it got so much faster that it triggered a bug handling full transmit buffers - which made it initially appear slower!

In addition to fixing the alignment problem when using TLS, and being faster, this has a nice security benefit:
the only data shared with the network driver domain is data explicitly sent to it.
Before, we had to share the entire page of memory containing the application’s buffer, and there was no way to know what else might have been there.
This offers some protection if the network driver domain is compromised.

HTTP API

My original code configured a plain HTTP server on port 8080 like this:

config.ml 
1
2
3
4
5
6
7
8
9
...
let server =
  http_server (`TCP (`Port 8080)) (conduit_direct (stack default_console))
let () =
  register "queue" [
    queue $ default_console $ storage $ server
  ]


stack creates TCP/IP stack.
conduit_direct can dynamically select different transports (e.g. http or vchan).
http_server applies the configuration to the conduit to get an HTTP server using plain HTTP.

I added support to Conduit_mirage to let it wrap any underlying conduit with TLS.
However, the configuration needed for TLS is fairly complicated, and involves a secret key which must be protected.
Therefore, I switched to creating only the conduit in config.ml and having the unikernel itself load the key and certificate by copying a local “keys” directory into the unikernel image as a “crunch” filesystem:

config.ml 
1
2
3
4
5
6
7
8
let conduit = conduit_direct
    ~tls:(tls_over_conduit default_entropy)
    (stack default_console)
let () =
  register "queue" [
    queue $ default_console $ storage $ conduit $ crunch "keys"
  ]


unikernel.ml 
1
2
3
4
5
6
7
8
    let start c b conduit kv =
      lwt certificate = X509.certificate kv `Default in
      let tls_config = Tls.Config.(of_server (server ~certificate ())) in
      let http spec =
        let ctx = conduit in
        let mode = `TLS (tls_config, `TCP (`Port 8443)) in
        Conduit.serve ~ctx ~mode (H.listen spec) in
      ...


This runs a secure HTTPS server on port 8443.
The rest of the code is as before.

The private key

The next question is where to store the real private key.
The examples provided by the TLS package compile it into the unikernel image using crunch, but it’s common to keep unikernel binaries in Git repositories and people don’t expect kernel images to contain secrets.
In a traditional Linux system, we’d store the private key on the disk, so I decided to try the same here.
(I did think about storing the key encrypted in the unikernel and storing the password on the disk so you’d need both to get the key, but the TLS library doesn’t support encrypted keys yet.)

I don’t use a regular filesystem for my queuing service, and I wouldn’t want to share it with the key if I did, so instead I reserved a separate 4KB partition of the disk for the key.
It turned out that Mirage already has partitioning support in the form of the ocaml-mbr library.
I didn’t actually create an MBR at the start, but just used the Mbr_partition functor to wrap the underlying block device into two parts.
The configuration looks like this:



How safe is this?
I don’t want to audit all the code for handling the queue, and I shouldn’t have to: we can see from the diagram that the only components with access to the key are the disk, the partitions and the TLS library.
We need to trust that the TLS library will protect the key (not easy, but that’s its job) and that queue_partition won’t let queue access the part of the disk with the key.

We also need to trust the disk, but if the partitions are only allowing correct requests through, that shouldn’t be too much to ask.

The partition code

Before relying on the partition code, we’d better take a look at it because it may not be designed to enforce security.
Indeed, a quick look at the code shows that it isn’t:

1
2
3
4
5
6
7
8
9
10
11
  let read t start_sector buffers =
    let length = Int64.add start_sector (length buffers) in
    if length > t.length_sectors
    then return (`Error (`Unknown (Printf.sprintf "read %Ld %Ld out of range" start_sector length)))
    else B.read t.b (Int64.add start_sector t.start_sector) buffers
  let write t start_sector buffers =
    let length = Int64.add start_sector (length buffers) in
    if length > t.length_sectors
    then return (`Error (`Unknown (Printf.sprintf "write %Ld %Ld out of range" start_sector length)))
    else B.write t.b (Int64.add start_sector t.start_sector) buffers


It checks only that the requested start sector plus the length of the results buffer is less than the length of the partition.
To (hopefully) make this bullet-proof, I:

  moved the checks into a single function so we don’t have to check two copies,
  added a check that the start sector isn’t negative,
  modified the end check to avoid integer overflow, and
  added some unit-tests.


1
2
3
4
5
6
7
8
9
10
  let adjust_start name op t start_sector buffers =
    let buffers_len_sectors = length t buffers in
    if start_sector < 0L ||
       start_sector > t.id.length_sectors -- buffers_len_sectors
    then return (`Error (`Unknown (Printf.sprintf
      "%s %Ld+%Ld out of range" name start_sector buffers_len_sectors)))
    else op t.id.b (Int64.add start_sector t.id.start_sector) buffers
  let read = adjust_start "read" B.read
  let write = adjust_start "write" B.write


The need to protect against overflow is an annoyance.
OCaml’s Int64.(add max_int one) doesn’t abort, but returns Int64.min_int.
That’s disappointing, but not surprising.
I wrote a unit-test that tried to read sector Int64.max_int and ran it (before updating the code) to check it detected the problem.
I was expecting the partition code to pass the request to the underlying block device, which I expected to return an error about the invalid sector, but it didn’t!
It turns out, Int64.to_int (used by my in-memory test block device) silently truncates out-of-range integers:

# Int64.max_int;;
- : int64 = 9223372036854775807L
# Int64.(add max_int one);;
- : int64 = -9223372036854775808L
# Int64.min_int;;
- : int64 = -9223372036854775808L
# Int64.(to_int min_int);;
- : int = 0


So, if the queue can be tricked into asking for sector 9223372036854775807 then the partition would accept it as valid and the block device would truncate it and give access to sector 0 - the sector with the private key!

Still, this is a nice demonstration of how we can add security in Mirage by inserting a new module (Mbr_partition) between two existing ones.
Rather than having some complicated fixed policy language (e.g. SELinux), we can build whatever security abstractions we like.
Here I just limited which parts of the disk the queue could access, but we could do many other things: make a partition read-only, make it readable only until the unikernel finishes initialising, apply rate-limiting on reads, etc.

Here’s the final code. It:

  Takes a block device, a conduit, and a KV store as inputs.
  Creates two partitions (views) onto the block device.
  Creates a queue on one partition.
  Reads the private key from the other, and the certificate from the KV store.
  Begins serving HTTPS requests.


unikernel.ml 
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
module Main (C : V1_LWT.CONSOLE)
            (B : V1_LWT.BLOCK)
            (Conduit : Conduit_mirage.S)
            (CertStore : V1_LWT.KV_RO) = struct
  module Part = Mbr_partition.Make(B)
  module Q = Upload_queue.Make(Part)
  module Http = HTTP.Make(Conduit)
  [...]
  let start c b conduit cert_store =
    B.get_info b >>= fun info ->
    let key_sectors = (key_partition_size + info.B.sector_size - 1) /
                      info.B.sector_size |> Int64.of_int in
    let queue_sectors = info.B.size_sectors -- key_sectors in
    Part.(connect {b; start_sector = 0L;
                      length_sectors = key_sectors})
    >>|= fun key_partition ->
    Part.(connect {b; start_sector = key_sectors;
                      length_sectors = queue_sectors})
    >>|= fun queue_partition ->
    Q.create queue_partition >>= fun q ->
    lwt certs =
      read_from_kv cert_store "tls/server.pem"
      >|= X509.Cert.of_pem_cstruct in
    lwt pk =
      read_from_partition key_partition ~len:private_key_len
      >|= X509.PK.of_pem_cstruct1 in
    let tls_config = Tls.Config.(of_server
      (server ~certificate:(certs, pk) ())) in
    let spec = Http.Server.make ~callback:(callback q) ~conn_closed () in
    let mode = `TLS (tls_config, `TCP (`Port 8443)) in
    Conduit.serve ~ctx:conduit ~mode (Http.Server.listen spec)
end


Entropy

Another issue is getting good random numbers, which is required for the cryptography.
On start-up, the unikernel displayed:

Entropy_xen_weak: using a weak entropy source seeded only from time.


To fix this, you need to use Dave Scott’s version (with a slight patch from me):

opam pin add mirage-entropy-xen 'https://github.com/talex5/mirage-entropy.git#handshake'


You should then see:

Entropy_xen: attempting to connect to Xen entropy source org.openmirage.entropy.1
Console.connect org.openmirage.entropy.1: doesn't currently exist, waiting for hotplug


Now run xentropyd in dom0 to share the host’s entropy with guests.

The interesting question here is what Linux guests do for entropy, especially on ARM where there’s no RdRand instruction.

Access control

Traditional means of access control involve issuing users with passwords or X.509 client certificates, which they share with the software they’re running.
All requests sent by the client can then be authenticated as coming from them and approved based on some access control policy.
This approach leads to all the well-known problems with traditional access control: the confused deputy problem, Cross-Site Request Forgery, Clickjacking, etc, so I want to avoid that kind of “ambient authority”.

The previous diagram let us reason about how the different components within the unikernel could interact with each other, showing the possible (initial) interactions with arrows.
Now I want to stretch arrows across the Internet, so I can reason in the same way about the larger distributed system that includes my queue service with the uploaders and downloaders.

Like C pointers, traditional web URLs do not give us what we want: a compromised CA anywhere in the world will allow an attacker to impersonate our service, and our URLs may be guessable.
Instead, I decided to try a YURL:

  ”[…] the identifier MUST provide enough information to: locate the target site; authenticate the target site; and, if required, establish a private communication channel with the target site. A URL that meets these requirements is a YURL.”


The latest version of this (draft) scheme I could find was some brief notes in HTTPSY (2014), which uses the format:

httpsy://algorithm:fingerprint@domain:port/path1/!redactedPath2/…


There are two parts we need to consider: how the client determines that it is connected to the real service, and how the service determines what the client can do.

To let the client authenticate the server without relying on the CA system, YURLs include a hash (fingerprint) of the server’s public key.
You can get the fingerprint of an X509 certificate like this:

$ openssl x509 -in server.pem -fingerprint -sha256 -noout
SHA256 Fingerprint=3F:27:2D:E6:D6:3D:7C:08:E0:E3:EF:02:A8:DA:9A:74:62:84:57:21:B4:72:39:FD:D0:72:0E:76:71:A5:E9:94


Base32-encoding shortens this to h4ts3zwwhv6aryhd54bkrwu2orriivzbwrzdt7oqoihhm4nf5gka.

Alternatively, to get the value with OCaml, use:

1
Certificate.cs_of_cert cert |> Nocrypto.Hash.digest `SHA256


To control what each user of the service can do, we give each user a unique YURL containing a Swiss number, which is like a password except that it applies only to a specific resource, not to the whole site.
The Swiss number comes after the ! in the URL, which indicates to browsers that it shouldn’t be displayed, included in Referer headers, etc.
You can use any unguessably long random string here (I used pwgen 32 1).
After checking the server’s fingerprint, the client requests the path with the Swiss number included.

Putting it all together, then, a sample URL to give to the downloader looks like this:

httpsy://sha256:h4ts3zwwhv6aryhd54bkrwu2orriivzbwrzdt7oqoihhm4nf5gka@10.0.0.2:8443/downloader/!eequuthieyexahzahShain0abeiwaej4


The old code for handling requests looked like this:

1
2
3
match Uri.path request.Cohttp.Request.uri with
| "/uploader" -> handle_uploader q request body
| "/downloader" -> handle_downloader q request


This becomes:

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
let re_endpoint = Str.regexp "^/\\(uploader\\|downloader\\)/!\\(.*\\)$" in
let path = Uri.path request.Cohttp.Request.uri in
if Str.string_match re_endpoint path 0 then (
  let label = Str.matched_group 1 path in
  let swiss_hash = Str.matched_group 2 path |> Cstruct.of_string
    |> Nocrypto.Hash.digest `SHA256 |> Cstruct.to_string
    |> B64.encode in
  match label, swiss_hash with
  | "uploader", "kW8VOKYP1eu/cWInpJx/jzYDSJzo1RUR14GoxV/CImM=" ->
      handle_uploader q request body ~user:"Alice"
  | "downloader", "PEj3nuboy3BktVGzi9y/UBmgkrGuhHD1i6WsXDw1naI=" ->
      handle_downloader q request ~user:"0repo"
  | _ ->
      bad_path path
) else bad_path path


I hashed the Swiss number here so that the unikernel doesn’t have to contain any secrets and I therefore don’t have to worry about timing attacks.
Even if the attacker knows the hash we’re looking for, they still shouldn’t be able to generate a URL which hashes to that value.

By giving each user of the service a different Swiss number we can keep records of who authorised each request and revoke access individually if needed (here the ~user:"Alice" indicates this is the uploader URL we gave to Alice).

Of course, the YURLs need to be sent to users securely too.
In my case, the users already have known GPG keys, so I can just email them an encrypted version.

Python client

The downloader (0repo) is written in Python, so the next step was to check that it could still access the service.
The Python SSL API was rather confusing, but this seems to work:

client.py 
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
#!/usr/bin/python3
from urllib.parse import urlparse
from http import client
import ssl
import hashlib
import base64
# Note: MUST check fingerprint BEFORE sending URL path with Swiss number
class FingerprintContext:
    required_fingerprint = None
    verify_mode = ssl.CERT_REQUIRED
    check_hostname = False
    def __init__(self, fingerprint):
        self.required_fingerprint = fingerprint
    def wrap_socket(self, sock, server_hostname, **kwargs):
        wrapped = ssl.wrap_socket(sock, **kwargs)
        cert = wrapped.getpeercert(binary_form = True)
        actual_fingerprint = base64.b32encode(hashlib.sha256(cert).digest()).lower().decode('ascii').strip('=')
        if actual_fingerprint != self.required_fingerprint:
            raise Exception("Expected server certificate to have fingerprint:\n%s but got:\n%s" % (self.required_fingerprint, actual_fingerprint))
        return wrapped
# Testing...
url = "httpsy://sha256:h4ts3zwwhv6aryhd54bkrwu2orriivzbwrzdt7oqoihhm4nf5gka@localhost:8443/downloader/!eequuthieyexahzahShain0abeiwaej4"
parsed = urlparse(url)
assert parsed.scheme == "httpsy"
assert parsed.username == "sha256"
conn = client.HTTPSConnection(
    host = parsed.hostname,
    port = parsed.port,
    context = FingerprintContext(parsed.password),
    check_hostname = False)
conn.request("GET", parsed.path)
resp = conn.getresponse()
print("Fetching item of size %s..." % resp.getheader('Content-Length'))
d = resp.read()
print("Fetched %d bytes" % len(d))


Conclusions

MirageOS should allow us to build systems that are far more secure than traditional operating systems.
By starting with isolated components and then connecting them together in a controlled way we can feel some confidence that our security goals will be met.

At the language level, OCaml’s abstract types and functors make it easy to reason informally about how the components of our system will interact. Mirage passes values granting access to the outside world (disks, network cards, etc) to our unikernel’s start function.
Our code can then delegate these permissions to the rest of the code in a controlled fashion.
For example, we can grant the queuing code access only to its part of the disk (and not the bit containing the TLS private key) by wrapping the disk in a partition functor.
Although OCaml doesn’t actually prevent us from bypassing this system and accessing devices directly, code that does so would not be able to support the multiple different back-ends (e.g. Unix and Xen) that Mirage requires and so could not be written accidentally.
It should be possible for a static analysis tool to verify that modules don’t do this.

Moving up a level from separating the components of our unikernel, Xen allows us to isolate multiple unikernels and other VMs running on a single physical host.
Just as we interposed a disk partition between the queue and the disk within the unikernel, we can use Xen to interpose a firewall VM between the physical network device and our unikernel.

Finally, the use of transport layer security and YURLs allows us to continue this pattern of isolation to the level of networks, so that we can reason in the same way about distributed systems.
My current code mixes the handling of YURLs with the existing application logic, but it should be possible to abstract this and make it reusable, so that remote services appear just like any local service.
In many systems this is awkward because local APIs are used synchronously while remote ones are asynchronous, but in Mirage everything is non-blocking anyway, so there is no difference.

I feel I should put some kind of warning here about these very new security features not being ready for real use and how you should instead use mature, widely deployed systems such as Linux and OpenSSL.
But really, can it be any worse?

If you’ve spotted any flaws in my reasoning or code, please add comments!
The code for this unikernel can be found on the tls branch at https://github.com/0install/0repo-queue/tree/tls.

Hide
        
      
                    by Thomas Leonard at Jan 21, 2015 
      
      
    
  


       
                  Local MirageOS development with Xen and Virtualbox
      (Magnus Skjegstad)
    
    
                                MirageOS is a library operating system. An application written for MirageOS is compiled to an operating system kernel that only contains the specific functionality required by the application - a unikernel. The MirageOS unikernels can be compiled for different targets, including standalone VMs that run under Xen. The Xen unikernels can be deployed directly to common cloud services such as Amazon EC2 and Linode.
I have done a lot of MirageOS development for Xen lately and it can be inconvenient to have to rely on an external server or service to be able to run and debug the unikernel. As an alternative I have set up a VM in Virtualbox with a Xen server. The MirageOS unikernels then run as VMs in Xen, which itself runs in a VM in Virtualbox. With the "Host-only networking" feature in Virtualbox the unikernels are accessible from the host operating system, which can be very useful for testing client/server applications. A unikernel that hosts a web page can for example be tested in a web …Read more...MirageOS is a library operating system. An application written for MirageOS is compiled to an operating system kernel that only contains the specific functionality required by the application - a unikernel. The MirageOS unikernels can be compiled for different targets, including standalone VMs that run under Xen. The Xen unikernels can be deployed directly to common cloud services such as Amazon EC2 and Linode.
I have done a lot of MirageOS development for Xen lately and it can be inconvenient to have to rely on an external server or service to be able to run and debug the unikernel. As an alternative I have set up a VM in Virtualbox with a Xen server. The MirageOS unikernels then run as VMs in Xen, which itself runs in a VM in Virtualbox. With the "Host-only networking" feature in Virtualbox the unikernels are accessible from the host operating system, which can be very useful for testing client/server applications. A unikernel that hosts a web page can for example be tested in a web browser in the host OS. I am hoping that this setup may be useful to others so I am documenting it in this blog post.


My current VM is based on Ubuntu Server 14.04 LTS with Xen hypervisor 4.4 installed. The steps described in this post should be transferrable to other distributions if they support newer versions of the Xen hypervisor (4.4+). I have also included a list of alternative development environments for Mirage near the end.
Install the Ubuntu VM
First, create a new Virtualbox VM with at least 1 GB RAM and 20 GB disk and start the Ubuntu Server installation. How to install Ubuntu in Virtualbox is covered in detail elsewhere, so I will only briefly describe the most relevant steps. 
To keep the VM lightweight, install as few features as possible. We will use SSH to login to the server so select "OpenSSH Server". You may want to install a desktop environment later, but keep in mind that the graphics support will be limited under two layers of virtualization (Virtualbox + Xen). 
Give the Linux VM a hostname that is unique on your network as we will use this to access it with SSH later. I use "virtualxen". 
Add a user you want to use for development, for example "mirage". 
You may also want to reserve some of the disk space for Mirage if you plan to run applications that use block storage. During guided partitioning in the Ubuntu installer, if you choose to use the entire disk, the next question will allow you to specify a percentage of the disk that you want to use. If you plan to use Xen VMs that need direct disk access you should leave some of it for this purpose, for example 50%.
After completing the installation, run apt-get update/upgrade and install the Virtualbox guest utilities, then reboot:
sudo apt-get update
sudo apt-get upgrade
sudo apt-get install virtualbox-guest-utils 
sudo reboot



Install the Xen Hypervisor
After installing the Ubuntu Server VM, your configuration will be as in the following figure. Ubuntu runs in Virtualbox which runs under the main operating system (OS X in my case). 

We are now going to install the Xen hypervisor, which will become a thin layer between Virtualbox and the Ubuntu Server installation. The Xen hypervisor will be able to run VMs within the Virtualbox VM and we can use the Ubuntu installation to control Xen. This is the new configuration with Xen:

Dom0 is the original Ubuntu Server installation and the DomU's will be our future Mirage applications.
To install Xen, log in to Ubuntu and install the Xen hypervisor with the following command. We will also need bridge-utils (for configuring networking), build-essential (development tools) and git (version control):
# install hypervisor and other tools
sudo apt-get install xen-hypervisor-4.4-amd64 bridge-utils build-essential git



After the installation is complete, reboot the Virtualbox VM to start into Xen and the Ubuntu Server installation (which now has become dom0). 
Xen can be controlled from dom0 with the xl command. To verify that Ubuntu is running under Xen, log in and run sudo xl list to show a list of running domains. The output should look similar to this:
$ sudo xl list
Name                                        ID   Mem VCPUs       State      Time(s)
Domain-0                                     0  1896     1      r-----         31.7



The only Xen domain running at this point should be Dom0.
We are now ready to set up networking. 
Networking
Internet access should work out of the box for dom0, but to enable network access from the domU's we have set up a bridge device that they can connect to. We will call this device br0. Since this is a development environment we also want the unikernels to be accessible from the host operating system (so we can test them), but not from the local network. Virtualbox has a feature that allows this called a host-only network. 
To set up the host-only network in Virtualbox we have to shutdown the VM (sudo shutdown -h now). Then go to Preferences in Virtualbox and select the "Network" tab and "Host-only Networks". Create a new network. Make sure that the built-in DHCP server is disabled - I have not managed to get the built-in DHCP server to work with Mirage, so we will install a DHCP server in dom0 instead. If you already have an existing host-only network and you disabled the DHCP server in this step, remember to restart Virtualbox to make sure that the DHCP server is not running. 
After setting up the host-only network, exit preferences and open the settings for the VM. Under the "Network" tab, go to "Adapter 2", enable it and choose to attach to "Host-only Adapter". Select the name of the network that you just created in Preferences. Under advanced, select "Allow All" for "Promiscuous mode". Exit and save.
You can now start the VM with the new network configuration. After booting, edit /etc/network/interfaces to setup up the host-only adapter (eth1) and add it to the bridge (br0). The configuration below is based on the default IP range (192.168.56.x) for host-only networking in Virtualbox - if you have made changes to the default network configuration you may have to update the configuration here as well. 
# /etc/network/interfaces
auto lo
iface lo inet loopback

auto eth0
iface eth0 inet dhcp

auto eth1
iface eth1 inet manual
    pre-up ifconfig $IFACE up
    post-down ifconfig $IFACE down

auto br0
iface br0 inet static
    bridge_ports eth1
    address 192.168.56.5
    broadcast 192.168.56.255
    netmask 255.255.255.0
    # disable ageing (turn bridge into switch)
    up /sbin/brctl setageing br0 0
    # disable stp
    up /sbin/brctl stp br0 off



Next, we install dnsmasq to setup a DHCP server in dom0. This DHCP server will be responsible for assigning IP addresses to the unikernels that attach to br0.
$ sudo apt-get install dnsmasq



To enable the DHCP server on br0, edit /etc/dnsmasq.conf and add the following lines:
interface=br0
dhcp-range=192.168.56.150,192.168.56.200,1h



This configures the DHCP server to run on br0 and to dynamically assign IP addresses in the range 192.168.56.150 to 192.168.56.200 with a lease time of 1 hour. 
To be able to access dom0 via SSH from the host operating system (outside Virtualbox) we install avahi-daemon. Avahi-daemon enables mDNS, which will allow you to connect to "virtualxen.local" from the host operating system:
$ sudo apt-get install avahi-daemon



Reboot dom0 to activate the changes (sudo reboot).
You should now be able to connect to dom0 with SSH from the host OS:
$ ssh mirage@virtualxen.local



Dom0 can also be accessed from the host by using the IP address for br0 that we set above: 192.168.56.5.
Installing MirageOS in dom0
Before we can compile MirageOS unikernels we have to install OCaml.
# install ocaml and friends
sudo apt-get install ocaml-compiler-libs ocaml-interp ocaml-base-nox ocaml-base ocaml ocaml-nox ocaml-native-compilers camlp4 camlp4-extra m4



We also need the OCaml package manager, opam. A new version of opam was recently release and the version that comes with many Linux distros is outdated. To get the latest version (currently 1.2.x) I use 0install to install opam directly from the installation script provided on the ocaml.org web page. If you don't want to use 0install, a list of other options is available here (including PPA's). 
# install 0install
sudo apt-get install zeroinstall-injector
# install opam
0install add opam http://tools.ocaml.org/opam.xml



0install installs applications in ~/bin. To add this directory to your path logout and back in. 
After installing opam, run opam init and follow the instructions to complete the installation. Note that the opam commands should not be run with sudo, as it installs everything in ~/.opam in your home directory.
If you want to run the development version of Mirage, you can add mirage-dev as an opam repository. Keep in mind that this repository contains the latest changes to Mirage, which may not always work. The safest option is to skip this step.
# optional - add mirage-dev to opam 
opam remote add mirage-dev git://github.com/mirage/mirage-dev



We can now install Mirage:
# install libs required to build many mirage apps
sudo apt-get install libssl-dev pkg-config
# install mirage
opam install mirage -v



After Mirage has been installed you should be able to run the Mirage configuration tool mirage.
$ mirage --version
2.0.0



If you use emacs or vim I also recommend installing Merlin, which provides tab completion, type lookup and many other useful IDE features for OCaml. 
Creating a Mirage VM
To verify that everything works, we will now download the Mirage examples and compile the static website example. This example will start a web server hosting a "Hello world" page that we should be able to access from the host OS. The IP-address will be assigned with DHCP. 
First, clone the Mirage examples:
# clone mirage-skeleton
git clone http://github.com/mirage/mirage-skeleton.git



Then go to the mirage-skeleton/static-webpage folder and run env DHCP=true mirage configure --xen. This command will download and install all the required dependencies and then create a Makefile. When the command completes, run make to compile.
If make completes successfully, there will be a file called www.xl that contains the Xen DomU configuration file for the unikernel. By default the line that contains the network interface configuration is commented out. Remove the # in front of the line that begins with 'vif = ...' to enable network support. The www.xl file should look similar to this:
name = 'www'
kernel = '/home/mirage/mirage-skeleton/static_website/mir-www.xen'
builder = 'linux'
memory = 256
on_crash = 'preserve'

# You must define the network and block interfaces manually.

# The disk configuration is defined here:
# http://xenbits.xen.org/docs/4.3-testing/misc/xl-disk-configuration.txt
# An example would look like:
# disk = [ '/dev/loop0,,xvda' ]

# The network configuration is defined here:
# http://xenbits.xen.org/docs/4.3-testing/misc/xl-network-configuration.html
# An example would look like:
vif = [ 'mac=c0:ff:ee:c0:ff:ee,bridge=br0' ]



The memory is set to 256 MB by default, but most of the example unikernels require much less than this. The static webserver example runs fine with 16 MB.
You should now be able to start the unikernel using the command sudo xl create www.xl -c: 
$ sudo xl create www.xl -c
Parsing config from www.xl
Xen Minimal OS!
  start_info: 0000000000332000(VA)
    nr_pages: 0x10000
  shared_inf: 0x5457d000(MA)
     pt_base: 0000000000335000(VA)
nr_pt_frames: 0x5
    mfn_list: 00000000002b2000(VA)
   mod_start: 0x0(VA)
     mod_len: 0
       flags: 0x0
    cmd_line:
       stack: 0000000000291b40-00000000002b1b40
Mirage: start_kernel
MM: Init
      _text: 0000000000000000(VA)
     _etext: 000000000015cc0f(VA)
   _erodata: 0000000000197000(VA)
     _edata: 0000000000258220(VA)
stack start: 0000000000291b40(VA)
       _end: 00000000002b1b40(VA)
  start_pfn: 33d
    max_pfn: 10000
Mapping memory range 0x400000 - 0x10000000
setting 0000000000000000-0000000000197000 readonly
skipped 1000
MM: Initialise page allocator for 3bb000(3bb000)-10000000(10000000)
MM: done
Demand map pfns at 10001000-0000002010001000.
Initialising timer interface
Initialising console ... done.
gnttab_table mapped at 0000000010001000.
xencaml: app_main_thread
getenv(OCAMLRUNPARAM) -> null
getenv(CAMLRUNPARAM) -> null
Unsupported function lseek called in Mini-OS kernel
Unsupported function lseek called in Mini-OS kernel
Unsupported function lseek called in Mini-OS kernel
getenv(OCAMLRUNPARAM) -> null
getenv(CAMLRUNPARAM) -> null
getenv(TMPDIR) -> null
getenv(TEMP) -> null
Netif: add resume hook
Netif.connect 0
Netfront.create: id=0 domid=0
MAC: c0:ff:ee:c0:ff:ee
Manager: connect
Attempt to open(/dev/urandom)!
Manager: configuring
DHCP: start discovery

Sending DHCP broadcast (length 552)
DHCP response:
input ciaddr 0.0.0.0 yiaddr 192.168.56.178
siaddr 192.168.56.5 giaddr 0.0.0.0
chaddr c0ffeec0ffee00000000000000000000 sname  file
DHCP: offer received: 192.168.56.178
DHCP options: Offer : DNS servers(192.168.56.5), Routers(192.168.56.5), Broadcast(192.168.56.255), Subnet mask(255.255.255.0), Unknown(59[4]), Unknown(58[4]), Lease time(43200), Server identifer(192.168.56.5)
Sending DHCP broadcast (length 552)
DHCP response:
input ciaddr 0.0.0.0 yiaddr 192.168.56.178
siaddr 192.168.56.5 giaddr 0.0.0.0
chaddr c0ffeec0ffee00000000000000000000 sname  file
DHCP: offer received
                    IPv4: 192.168.56.178
                                        Netmask: 255.255.255.0
                                                              Gateways: [192.168.56.5]
 sg:true gso_tcpv4:true rx_copy:true rx_flip:false smart_poll:false
ARP: sending gratuitous from 192.168.56.178
DHCP offer received and bound to 192.168.56.178 nm 255.255.255.0 gw [192.168.56.5]
Manager: configuration done



The console output shows the IP address that was assigned to the unikernel ("DHCP offer received and bound..."). In the example above the IP is 192.168.56.178. From the host operating system you should now be able to open this IP in a web browser to see the "Hello Mirage World!" message.
If you login to dom0 in a new terminal xl list will show the running domains, which now includes "www":
$ sudo xl list
Name                                        ID   Mem VCPUsStateTime(s)
Domain-0                                     0  1355     1     r-----    6691.4
www                                         33   256     1     -b----       0.2



To stop the unikernel, run sudo xl destroy www.
Some alternatives
The environment described in this post is my current development environment and is based on a Xen server running in a Virtualbox VM with the latest versions of opam and Mirage. I use a host-only second network adapter to allow access to the Mirage applications from the host running Virtualbox. 
Mirage applications can also be compiled in Unix mode, which produces an executable that can be executed directly in a Unix-like operating system. Currently OS X seems to be particularly well supported. This mode may often be the easiest way to debug and develop a Mirage application, but not all of Mirage's features are available in this mode and some applications may require low level access to the system - for example to block storage or network interfaces - which may not be available in this mode. 
Another approach is to use a Cubieboard2 with a prebuilt Mirage/Xen image to set up a low cost, portable Xen server for development. If you want to have long running Mirage services in your local network or host your own web page this may be a good alternative. Note that compilation times can be slow on this platform compared to an x86 based VM.
An automated VM setup is being developed based on Debian, Vagrant and Packer here. This can be useful if you don't want to manually perform the setup steps outlined in this post. Currently this setup uses an older version of Debian which comes with Xen 4.1, but it should be possible to upgrade to Debian Jessie or later.Hide
        
      
                    by Magnus Skjegstad at Jan 19, 2015 
      
      
    
  


       
                  Towards a governance framework for OCaml.org
      (Amir Chaudhry)
    
    
                                The projects around the OCaml.org domain name are becoming more established
and it’s time to think about how they’re organised. 2014 saw a lot of
activity, which built on the successes from 2013.
Some of the main things that stand out to me are:

  More volunteers contributing to the public website with
translations, bug fixes and content updates, as well as many new visitors —
for example, the new page on teaching OCaml received over 5k
visits alone. The increasing contributions are a result of the earlier work on
re-engineering the site and there are many ways to get involved
so please do contribute!




  The relentless improvements and growth of OPAM, both in terms of the
repository — with over 1000 additional packages and several
new repo maintainers — and also improved workflows (e.g the new
pin functionality). 
The OPAM site and package list also moved to the ocaml.org domain, becoming
the substrate for the OCaml Platform efforts. This began with the work towards
OP…Read more...The projects around the OCaml.org domain name are becoming more established
and it’s time to think about how they’re organised. 2014 saw a lot of
activity, which built on the successes from 2013.
Some of the main things that stand out to me are:

  More volunteers contributing to the public website with
translations, bug fixes and content updates, as well as many new visitors —
for example, the new page on teaching OCaml received over 5k
visits alone. The increasing contributions are a result of the earlier work on
re-engineering the site and there are many ways to get involved
so please do contribute!




  The relentless improvements and growth of OPAM, both in terms of the
repository — with over 1000 additional packages and several
new repo maintainers — and also improved workflows (e.g the new
pin functionality). 
The OPAM site and package list also moved to the ocaml.org domain, becoming
the substrate for the OCaml Platform efforts. This began with the work towards
OPAM 1.2 and there is much more to come (including closer
integration in terms of styling). Join the Platform list to
keep up to date.


  Much more activity on the mailing lists in general and user groups
requesting to have lists made (e.g the teaching list). If anyone
has a need for a new list, just ask on the
infrastructure list!


There is other work besides those I’ve mentioned and I think by any measure,
all the projects have been quite successful. As the community continues to
develop, it’s important to clarify how things currently work to improve the
level of transparency and make it easier for newcomers to get involved.

Factors for a governance framework

For the last couple of months, I’ve been looking over how larger projects
manage themselves and the governance documents that are available. My aim has
been to put such a document together for the OCaml.org domain without
introducing burdensome processes.  There are number of things that stood out
to me during this process, which have guided the approach I’m taking.

My considerations for an OCaml.org governance document:

      A governance document is not necessary for success but it’s valuable to
demonstrate a commitment to a stable decision-making process.  There are
many projects that progress perfectly well without any documented processes
and indeed the work around OCaml.org so far is a good example of this (as well
as OCaml itself).  However, for projects to achieve a scale greater than the
initial teams, it’s a significant benefit to encode in writing how things work
(NB: please note that I didn’t define the type of decision-making process -
merely that it’s a stable one).
  
      It must clarify its scope so that there is no confusion about what the
document covers. In the case of OCaml.org, it needs to be clear that the
governance covers the domain itself, rather than referring to the website. 
  
      It should document the reality, rather than represent an aspirational
goal or what people believe a governance structure should look like.  It’s
very tempting to think of an idealised structure without recognising that
behaviours and norms have already been established. Sometimes this will be
vague and poorly defined but that might simply indicate areas that the
community hasn’t encountered yet (e.g it’s uncommon for any new project to
seriously think about dispute resolution processes until they have to).  In
this sense, the initial version of a governance document should simply be a
written description of how things currently stand, rather than a means to
adjust that behaviour.  
  
      It should be simple and self-contained, so that anyone can understand
the intent quickly without recourse to other documents.  It may be tempting to
consider every edge-case or try to resolve every likely ambiguity but this
just leads to large, legal documents.  This approach may well be necessary
once projects have reached a certain scale but to implement it sooner would be
a case of premature optimisation — not to mention that very few people would 
read and remember such a document.
  
      It’s a living document. If the community decides that it would prefer a
new arrangement, then the document conveniently provides a stable starting
point from which to iterate. Indeed, it should adapt along with the project
that it governs. 
  


With the above points in mind, I’ve been putting together a draft governance
framework to cover how the OCaml.org domain name is managed.  It’s been a
quiet work-in-progress for some time and I’ll be getting in touch with
maintainers of specific projects soon.  Once I’ve had a round of reviews, I’ll
be sharing it more widely and posting it here!


Hide
        
      
                    by Amir Chaudhry at Jan 08, 2015 
      
      
    
  


       
                  MirageOS 2014 review: IPv6, TLS, Irmin, Jitsu and community growth
      (Mirage OS)
    
    
                                       This work funded in part by the EU FP7 User-Centric Networking project, Grant
 No. 611001.

An action-packed year has flown by for MirageOS, and it's time for a little recap of what's been happening and the plans for the new year.
We announced MirageOS 1.0 just over a year ago, and 2014 also saw a major 2.0 summer release and the growth of a developer community that have been building support for IPv6, Transport Layer Security, on-demand spawning, profiling and much more.  There have been 205 individual library releases, 25 presentations, and lots of online chatter through the year, so here follows a summary of our major activities recently.
Clean-Slate Transport Layer Security


David Kaloper and Hannes Mehnert started 2014 with getting interested in writing a safer and cleaner TLS stack in OCaml, and ended the year with a complete demonstration and talk last week in 31C3, the premier hacker conference!  Their blog posts over the summer remain an excellent introduction to the…Read more...       This work funded in part by the EU FP7 User-Centric Networking project, Grant
 No. 611001.

An action-packed year has flown by for MirageOS, and it's time for a little recap of what's been happening and the plans for the new year.
We announced MirageOS 1.0 just over a year ago, and 2014 also saw a major 2.0 summer release and the growth of a developer community that have been building support for IPv6, Transport Layer Security, on-demand spawning, profiling and much more.  There have been 205 individual library releases, 25 presentations, and lots of online chatter through the year, so here follows a summary of our major activities recently.
Clean-Slate Transport Layer Security


David Kaloper and Hannes Mehnert started 2014 with getting interested in writing a safer and cleaner TLS stack in OCaml, and ended the year with a complete demonstration and talk last week in 31C3, the premier hacker conference!  Their blog posts over the summer remain an excellent introduction to the new stack:
"OCaml-TLS: Introducing transport layer security (TLS) in pure OCaml" presents the motivation and architecture behind our clean-slate implementation of the protocol."OCaml-TLS: building the nocrypto library core" talks about the cryptographic primitives that form the heart of TLS confidentiality guarantees, and how they expose safe interfaces to the rest of the stack."OCaml-TLS: adventures in X.509 certificate parsing and validation" explains how authentication and chain-of-trust verification is implemented in our stack."OCaml-TLS: ASN.1 and notation embedding" introduces the libraries needed for handling ASN.1 grammars, the wire representation of messages in TLS."OCaml-TLS: the protocol implementation and mitigations to known attacks" concludes with the implementation of the core TLS protocol logic itself.

By summer, the stack was complete enough to connect to the majority of TLS 1.0+ sites on the Internet, and work progressed to integration with the remainder of the MirageOS libraries.  By November, the Conduit network library had Unix support for both the OpenSSL/Lwt bindings and the pure OCaml stack, with the ability to dynamically select them.  You can now deploy and test the pure OCaml TLS stack on a webserver simply by:
opam install lwt tls cohttp
export CONDUIT_TLS=native
cohttp-server-lwt -c <certfile> -p <port> <directory>This will spin up an HTTPS server that serves the contents of <directory> to you over TLS.
At the same time, we were also working on integrating the TLS stack into the Xen unikernel backend, so we could run completely standalone.  This required some surgery:
The nocrypto crypto core is written in C, so we had to improve support for linking in external C libraries.  Since the Xen unikernel is a single address-space custom kernel, we also need to be careful to compile it with the correct compilation flags or else risk subtle bugs. Thomas Leonard completely rearranged the MirageOS compilation pipeline to support separation compilation of C stubs, and we had the opportunity to remove lots of duplicated code within mirage-platform as a result of this work.Meanwhile, the problem of gathering entropy in a virtual machine reared its head.  We created a mirage-entropy device driver, and an active discussion ensued about how best to gather reliable randomness from Xen.  Dave Scott built the best solution -- the xenentropyd that proxies entropy from dom0 to a unikernel VM.David Kaloper also ported the nocrypto library to use the OCaml-Ctypes library, which increases the safety of the C bindings significantly.  This is described in more detail in the "Modular foreign function bindings" blog post from the summer.  This forms the basis for allowing Xen unikernels to communicate with C code, and integration with the MirageOS toolchain will continue to improve next year.

You can see Hannes and David present OCaml-TLS at CCC online.  It's been a real pleasure watching their work develop in the last 12 months with such precision and attention to detail!
HTTP and JavaScript

Rudi Grinberg got sufficiently irked with the poor state of documentation for the CoHTTP library that he began gently contributing fixes towards the end of 2013, and rapidly became one of the maintainers.  He also began improving the ecosystem around the web stack by building a HTTP routing layer, described in his blog posts:
Type Safe Routing - Baby Steps: type-safe routing of URLs to avoid dangling linksIntroducing Opium: middleware for REST servicesMiddleware in Opium: a walkthrough the Opium HTTP middleware modelIntroducing Humane-Re: more friendly regular expression interfaces

Meanwhile, Andy Ray started developing HardCaml (a register transfer level hardware design system) in OCaml, and built the iocamljs interactive browser notebook.  This uses js_of_ocaml to port the entire OCaml compilation toolstack to JavaScript, including ocamlfind, Lwt threading and dynamic loading support.  The results are browsable online, and it is now easy to generate a JavaScript-driven interactive page for many MirageOS libraries.
An interesting side effect of Andy's patches were the addition of a JavaScript port to the CoHTTP library.  For those not familiar with the innards, CoHTTP uses the OCaml module system to build a very portable HTTP implementation that can make mapped to different I/O models (Lwt or Async cooperative threading or POSIX blocking I/O), and to different operating systems (e.g. Unix or MirageOS).  The JavaScript support mapped the high-level modules in CoHTTP to the XMLHTTPRequest native to JavaScript, allowing the same OCaml HTTP client code to run efficiently on Unix, Windows and now an IOCamlJS browser instance.
MirageOS uses a number of libraries developed by the Ocsigen team at IRILL in Paris, and so I was thrilled to deliver a talk there in December.  Romain Calascibetta started integrating Ocsigen and MirageOS over the summer, and the inevitable plotting over beer in Paris lead Gabriel Radanne to kick off an effort to integrate the complete Ocsigen web stack into MirageOS. Head to ocsigen/ocsigenserver#54 if you're interested in seeing this happen in 2015!
I also expect the JavaScript and MirageOS integration to continue to improve in 2015, thanks to large industrial users such as Facebook adopting js_of_ocaml in their open-source tools such as Hack and Flow.
IPv6

We've wanted IPv6 support in MirageOS since its inception, and several people contributed to making this possible.  At the start of the year, Hugo Heuzard and David Sheets got IPv6 parsing support into the ipaddr library (with me watching bemusedly at how insanely complex parsing is versus IPv4).
Meanwhile, Nicolas Ojeda Bar had been building OCaml networking libraries independently for some time, such as a IMAP client, Maildir handler, and a Bittorrent client.  He became interested in the networking layer of MirageOS, and performed a comprehensive cleanup  that resulted in a more modular stack that now supports both IPv4 and IPv6!
The addition of IPv6 support also forced us to consider how to simplify the configuration frontend to MirageOS unikernels that was originally written by Thomas Gazagnaire and described here by Mindy Preston.
Nicolas has proposed a declarative extension to the configuration that allows applications to extend the mirage command-line more easily, thus unifying the "built-in" MirageOS compilation modes (such as choosing between Xen or Unix) and protocol-specific choices (such as configuring IPv4 and IPv6).
The new approach opens up the possibility of writing more user-friendly configuration frontends that can render them as a text- or web-based selectors, which is really important as more real-world uses of MirageOS are being created.  It should be possible in 2015 to solve common problems such as web or DNS serving without having to write a single line of OCaml code.
Profiling


One of the benefits touted by our CACM article on unikernels at the start of the year was the improved tooling from the static linking of an entire application stack with an operating system layer.
Thomas Leonard joined the project this year after publishing a widely read blog series on his experiences from switching from Python to OCaml.
Aside from leading (and upstreaming to Xen) the port of MirageOS to ARM, he also explored how to add profiling throughout the unikernel stack.
The support is now comprehensive and integrated into the MirageOS trees: the Lwt cooperative threading engine has hooks for thread switching, most of the core libraries register named events, traces are dumped into shared memory buffers in the CTF file format used by the Linux trace toolkit, and there are JavaScript and GTK+ GUI frontends that can parse them.
You can find the latest instructions on Tracing and Profiling on this website, and here are Thomas' original blog posts on the subject:
Optimising the UnikernelVisualising an Asynchronous Monad

Irmin

Thomas Gazagnaire spent most of the year furiously hacking away at the storage layer in Irmin, which is a clean-slate storage stack that uses a Git-like branching model as the basis for distributed unikernel storage.  Irmin 0.9.0 was released in December with efficiency improvements and a sufficiently portable set of dependencies to make JavaScript compilation practical.
"Introducing Irmin: Git-like distributed, branchable storage"  describes the concepts and high-level architecture of the system."Using Irmin to add fault-tolerance to the Xenstore database" shows how Irmin is used in a real-world application: the security-critical Xen toolstack that manages hosts full of virtual machines (video).There have been several other early adopters of Irmin for their own projects (independent of MirageOS).  One of the most exciting is by Gregory Tsipenyuk, who has been developing a version-controlled Irmin-based IMAP server that offers a very different model for e-mail management.  Expect to see more of this in the new year!

We also had the pleasure of Benjamin Farinier and Matthieu Journault join us as summer interns.  Both of them did a great job improving the internals of Irmin, and Benjamin's work on Mergeable Persistent Datastructures will be presented at JFLA 2015.
Jitsu


Magnus Skjegstad returned to Cambridge and got interested in the rapid dynamic provisioning of unikernels.  He built Jitsu, a DNS server that spawns unikernels in response to DNS requests and boots them in real-time with no perceptible lag to the end user.  The longer term goal behind this is to enable a community cloud of ARM-based Cubieboard2 boards that serve user content without requiring centralised data centers, but with the ease-of-use of existing systems.
Building Jitsu and hitting our goal of extremely low latency management of unikernels required a huge amount of effort from across the MirageOS team.
Dave Scott and Jon Ludlam (two of the Xen maintainers at Citrix) improved the Xen xl toolstack to deserialise the VM startup chain to shave 100s of milliseconds off every operation.Thomas Leonard drove the removal of our forked Xen MiniOS with a library version that is being fed upstream (including ARM support).  This made the delta between Xen and MirageOS much smaller and therefore made reducing end-to-end latency tractable.David Sheets built a test harness to boot unikernel services and measure their latency under very different conditions, including contrasting boot timer versus Docker containers.  In many instances, we ended up booting faster than containers due to not touching disk at all with a standalone unikernel.  Ian Leslie built us some custom power measurement hardware that came in handy to figure out how to drive down the energy cost of unikernels running on ARM boards.Thomas Gazagnaire, Balraj Singh, Magnus Skjegstad built the synjitsu proxy server that intercepts and proxies TCP connections to mask the couple of 100 milliseconds during unikernel boot time, ensuring that no TCP connections ever require retransmission from the client.Dave Scott and I built out the vchan shared memory transport that supports low-latency communiction between unikernels and/or Unix processes.  This is rapidly heading into a Plan9-like model, with the additional twist of using Git instead of a flat filesystem hierarchy as its coordination basis.Amir Chaudhry and Richard Mortier documented the Git-based (and eventually Irmin-based) workflow behind managing the unikernels themselves, so that they can easily be deployed to distance ARM devices simply by running git pull.  You can read more about this in his From Jekyll to Unikernels post.

All of this work was hastily crammed into a USENIX NSDI 2015 paper that got submitted at 4am on a bright autumn morning.  We'll put the preprint available when it's ready in January, along with a blog post describing how you can deploy this infrastructure for yourself.
Community

All of the above work was only possible due to the vastly improved tooling and infrastructure around the project.  Our community manager Amir Chaudhry led the minuted calls every two weeks that tied the efforts together, and we established some pioneer projects for newcomers to tackle.

The OPAM package manager continued to be the frontend for all MirageOS tools, with releases of libraries happening regularly.  Because of the modular nature of MirageOS code, most of the libraries can also be used as normal Unix-based libraries, meaning that we aren't just limited to MirageOS users but can benefit from the entire OCaml community.  The graph to the right shows the growth of the total package database since the project started to give you a sense of how much activity there is.
The major OPAM 1.2 also added a number of new features that made MirageOS code easier to develop, including a Git-based library pinning workflow that works superbly with GitHub, and easier Travis integration for continuous integration.  Nik Sultana also improved the is-mirage-broken to give us a cron-driven prod if a library update caused an end-to-end failure in building the MirageOS website or other self-hosted infrastructure.
Our favourite random idiot, Mindy Preston, wrote up a superb blog series about her experiences in the spring of 2014 with moving her homepage to be hosted on MirageOS.  This was followed up by Thomas Leonard, Phil Tomson, Ian Wilkinson, Toby Moore, and many others that we've tried to record in our link log.  We really appreciate the hundreds of bug reports filed by users and folk trying out MirageOS; by taking the trouble to do this, you've  helped us refine and polish the frontend.  One challenge for 2015 that we could use help on is to pull together many of these distributed blogged instructions and merge them back into the main documentation (get in touch if interested!).
OCaml has come a long way in the last year in terms of tooling, and another task my research group OCaml Labs works on at Cambridge is the development of the OCaml Platform.  I'll be blogging separately about our OCaml-specific activities in a few days, but all of this work has a direct impact on MirageOS itself since it lets us establish a local feedback loop between MirageOS and OCaml developers to rapidly iterate on large-scale development.  The regular OCaml compiler hacking sessions organised by Jeremy Yallop and Leo White have been a great success this year, with a wide variety of people from academic (Cambridge, London universities and Microsoft Research) and industrial (Jane Street, Citrix and Facebook among others) and locally interested folk.
One very important project that has had a lot of work put into it in 2014 (but isn't quite ready for a public release yet) is Assemblage, which will remove much of the boilerplate currently needed to build and release an OCaml library to OPAM.
We also had a great time working with open-source summer programs. Thanks to the Xen Foundation and GNOME for their support here, and we hope to do this again next summer!  The roundup posts were:
OPW FIN by Mindy Preston: on of her FOSS Outreach Program work.Amazon Adventures by Jyotsna Prakash: on her Google Summer of Code 2014 efforts on EC2 bindings.

Upcoming features

So what's coming up for our unikernels in 2015?  Our focus heading into the new year is very much on improving the ease-of-use and deployability of MirageOS and fleshing out the feature set for the early adopters such as the XAPI project, Galois, and the Nymote personal data project.  Here are some of the highlights:
Dust Clouds: The work on Jitsu is leading to the construction of what we term "dust clouds": on-demand scaling of unikernel services within milliseconds of requests coming in, terminated right beside the user on local ARM devices.  The model supports existing clouds as well, and so we are improving support for cloud APIs such via Jyotsna Prakash's EC2 bindings, XenAPI, and (volunteers needed) OpenStack support.  If you're interested in tracking this work, head over to the Nymote site for updates.
Portability: Beyond Xen, there are several efforts afoot to port MirageOS to bare metal targets.  One promising effort is to use Rump Kernels as the boot infrastructure and MirageOS as the application stack.  We hope to have a Raspberry Pi and other ARM targets fairly soon.  Meanwhile at the end of the spectrum is mobile computing, which was part of the original multiscale vision for starting the project.  The JavaScript, iOS and Android ports are all progressing (mainly thanks to community contributions around OCaml support for this space, such as Jeff Psellos' hard work on OCaml-IOS).
Protocol Development: There are a huge number of protocols being developed independently, and more are always welcome.  Luke Dunstan is hacking on multicast DNS support, we have an IMAP client and server, Dominic Price has built a series of social network APIs for Facebook or Tumblr, and Masoud Koleini has been extending Haris Rotsos' work to achieve a line-rate and type-safe OpenFlow switch and controller based on the Frenetic project.  Hannes is also developing Jackline, which uses his MirageOS to assemble a trustworthy communication client.  Daniel Buenzli also continues to release a growing set of high-quality, modular libraries that we depend on throughout MirageOS.
Storage: All storage services from the unikernels will be Git-based (e.g. logging, command-and-control, key-value retrieval).  Expect to see Xen toolstack extensions that make this support seamless, so a single Linux VM will be able to control a large army of unikernels via persistent data structures.


Want to get involved?

This is a really fun time to get involved with unikernels and the MirageOS project. The year of 2014 has seen lots of discussion about the potential of unikernels and we'll see some of the first big deployments involving them in 2015.  For the ones among you who wish to learn more, then check out the pioneer projects, watch out for Amir's meeting notes and join the voice calls if you want a more interactive discussion, and engage on the mailing lists with any questions you might have.
For me personally, it's been a real privilege to spend the year working with and learning from the friendly, intelligent and diverse community that is springing up around the project.  The progression from experiment to reality has been a lot of work, but the unikernel dream is finally coming together rath[er nicely thanks to everyone's hard work and enthusiasm.  I'd also like to thank all of our funding bodies and the Linux Foundation and the Xen Project (especially Lars Kurth and Russell Pavlicek) for their support throughout the year that made all this work possible.  Happy new year, everyone!

   Hide
        
      
                    by Anil Madhavapeddy at Dec 31, 2014 
      
      
    
  


       
                  Visualising an asynchronous monad
      (Thomas Leonard)
    
    
                                Many asynchronous programs make use of promises (also known as using light-weight threads or an asynchronous monad) to manage concurrency.
I’ve been working on tools to collect trace data from such programs and visualise the results, to help with profiling and debugging.

The diagram below shows a trace from a Mirage unikernel reading data from disk in a loop.
You should be able to pan around by dragging in the diagram, and zoom by using your mouse’s scroll wheel.
If you’re on a mobile device then pinch-to-zoom should work if you follow the full-screen link, although it will probably be slow.
If nothing else works, the ugly zoom buttons at the bottom zoom around the last point clicked.



      WARNING: No HTML canvas support (this is just a static image)! Try a newer browser…
  

View full screen

The web viewer requires JavaScript and HTML canvas support.
If it doesn’t work, you can also build the trace viewer as a (much faster) native GTK application.

In this post I’…Read more...Many asynchronous programs make use of promises (also known as using light-weight threads or an asynchronous monad) to manage concurrency.
I’ve been working on tools to collect trace data from such programs and visualise the results, to help with profiling and debugging.

The diagram below shows a trace from a Mirage unikernel reading data from disk in a loop.
You should be able to pan around by dragging in the diagram, and zoom by using your mouse’s scroll wheel.
If you’re on a mobile device then pinch-to-zoom should work if you follow the full-screen link, although it will probably be slow.
If nothing else works, the ugly zoom buttons at the bottom zoom around the last point clicked.



      WARNING: No HTML canvas support (this is just a static image)! Try a newer browser…
  

View full screen

The web viewer requires JavaScript and HTML canvas support.
If it doesn’t work, you can also build the trace viewer as a (much faster) native GTK application.

In this post I’ll explain how to read these diagrams, and how to trace your own programs.

( this post also appeared on Hacker News and Reddit )

Table of Contents

  Introduction          Bind
      Join
      Choose
      Pick
      Exceptions
    
  
  Making your own traces
  Examples          Profiling the console
      UDP transmission
      TCP transmission
      Disk access
    
  
  Implementation notes
  Summary


Introduction

Many asynchronous programs make use of promises (also known as using light-weight threads or an asynchronous monad).
A promise/thread is a place-holder for a value that will arrive in the future.

Here’s a really simple example (an OCaml program using Lwt).
It creates a thread that resolves to unit (void) after one second:

1
2
3
4
5
6
7
8
9
open Lwt
open Lwt_unix
let example_1 () =
  sleep 1.0
let () =
  (* Run the main loop until example_1 resolves. *)
  Lwt_unix.run (example_1 ());




In the diagram, time runs from left to right.
Threads (promises) are shown as horizontal lines.
The diagram shows:

  Initially, only the main thread (“0”) is present.
  The main thread then creates a new thread, labelled “sleep”.
  The sleep thread is a “task” thread (this is just a helpful label added when the thread is created).
  The whole process then goes to sleep for one second, which is shown by the darker background.
  At the end of the second, the process wakes up and resolves the sleep thread to its value (unit), shown by the green arrow.
  The main thread, which was waiting for the sleep thread, reads the value (the blue arrow) and exits the program.


If you zoom in on the arrows (go down to a grid division of about 10 microseconds), you’ll also see a white segment on the main thread, which shows when it was running (only one thread runs at a time).

Because thread 0 is actually the main event loop (rather than a Lwt thread), things are a little more complicated than normal.
When the process has nothing to do, thread 0 puts the process to sleep until the next scheduled timer.
When the OS wakes the process, thread 0 resumes, determines that the “sleep” thread can be resolved, and does so.
This causes any callbacks registered on the sleep thread to be called, but in this case there aren’t any and control returns to thread 0.
Thread 0 then checks the sleep thread (because that determines when to finish), and ends the loop because it’s resolved.

Bind

Callbacks can be attached to a promise/thread to process the value when it arrives.
Attaching the callback immediately creates a new promise for the final result of the callback.

Here’s a program that sleeps twice in series.
The >>= (bind) operator attaches the callback function to the first thread.
I’ve made the sleeps very short so you can see the process waking up without having to zoom.

1
2
3
let example_2 () =
  sleep 0.00001 >>= fun () ->
  sleep 0.00001




In this case, the main thread creates two new threads at the start: one for the result of the first sleep and a second (“bind”) for the result of running the callback on the result.
It’s easier to see how the first thread is resolved here: the main thread handling the polling loop wakes up and resolves the sleep thread, which then causes the bind thread to resume.

You might wonder why the bind thread disappears when the second sleep starts.
It hasn’t finished, but when the bind’s callback function returns the second sleep thread as its result, the bind thread is merged with the sleep thread.
This is the asynchronous equivalent of a tail call optimisation, allowing us to create loops without needing an unbounded number of threads.

Actually, displaying binds in this way tends to clutter up the diagrams, so the viewer has a simplification rule that is enabled by default: if the first event on a bind thread is a read, the part of the bind up to that point isn’t drawn.
Therefore, the default display for this program is:



If you zoom in on the central green arrow, you can see the tiny remaining bind thread between the two sleeps.

Join

Lwt.join waits for a collection of threads to finish:

1
2
3
4
5
6
let example_3 () =
  join [
    sleep 0.003;
    sleep 0.001;
    sleep 0.002;
  ]




In the trace, you can see the join thread being notified each time one of the threads it’s waiting for completes.
When they’re all done, it resolves itself.

Choose

Lwt.choose is similar to join, but only waits until one of its threads finishes:

1
2
3
4
5
6
let example_4 () =
  choose [
    sleep 0.003;
    sleep 0.00001;
    sleep 0.002;
  ]




I cheated a bit here.
To avoid clutter, the viewer only draws each thread until its last recorded event (without this, threads that get garbage collected span the whole width of the trace), so I used Profile.label ~thread "(continues)" to create extra label events on the two remaining threads to make it clearer what’s happening here.

Pick

Lwt.pick is similar to choose, but additionally cancels the other threads:

1
2
3
4
5
6
let example_5 () =
  pick [
    sleep 0.003;
    sleep 0.00001;
    sleep 0.001;
  ]




Exceptions

Failed threads are shown with a red bar at the end and the exception message is displayed.
Also, any “reads” arrow coming from it is shown in red rather than blue.
Here, the bind thread fails but the try one doesn’t because it catches the exception and returns unit.

1
2
3
4
5
6
let example_6 () =
  try_lwt
    sleep 0.0001 >>= fun () ->
    failwith "oops"
  with _ ->
    return ()


Note: I’m using the Lwt syntax extension here for try_lwt, but you can use Lwt.try_bind if you prefer.



The same simplification done for “bind” threads also applies to “try” threads, so the try thread doesn’t appear at all until you zoom in on the red arrow.

Making your own traces

Update: These instructions were out-of-date so I’ve removed them. See the mirage-profile page for up-to-date instructions.

Examples

A few months ago, I made my first unikernel - a REST service for queuing files as a Xen guest OS.
Unlike a normal guest, which would include a Linux kernel, init system, libc, shell, Apache, etc, a Mirage unikernel is a single executable, and almost pure OCaml (apart from malloc, the garbage collector, etc).
Unikernels can be very small and simple, and have a much smaller attack surface than traditional systems.

For my first attempt at optimising the unikernel, I used OCaml’s built-in profiling support.
This recorded function calls and how much time was spent in each one.
But I quickly discovered that CPU time was rarely the important factor - how the various asynchronous threads were scheduled was more important, and the tracing made it difficult to see this.

So, let’s see how the new tracing does on my previous problems…

Profiling the console

In the previous profiling post, I generated this graph using libreoffice:



As a reminder, Xen guests output to the console by writing the text to a shared memory page, increasing a counter to indicate this, and signalling dom0. The console logger in dom0 reads the data, increments another counter to confirm it got it, and signals back to the guest that that part of the buffer is free again.

To use the new tracing system, I added a Profile.note_increase "sent" len to the main loop, which increments a “sent” count on each iteration (i.e. each time we write a line to the console).
The viewer adds a mark on the trace for each increment and overlays a graph (the red line) so you can see overall progress easily:


View full screen | Download console.sexp

As before, we can see that we send messages rapidly in bursts, followed by long periods without progress.
Zooming in to the places where the red line is increasing, we can see the messages being written to the buffer without any delays.
Looking at the edges of the sleeping regions, it’s clear that we’re simply waiting for Xen to notify us of space by signalling us on event channel 2.

Here’s the complete test code:

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
open Lwt
module Main (C: V1_LWT.CONSOLE) = struct
  let () = Profile.start ~size:10000
  let start c =
    let len = 6 in
    let msg = String.make (len - 1) 'X' in
    let iterations = 1800 in
    let bytes = len * iterations in
    let rec loop = function
      | 0 -> return ()
      | i ->
          Profile.note_increase "sent" len;
          C.log_s c msg >>= fun () -> loop (i - 1) in
    let t0 = Clock.time () in
    loop iterations >>= fun () ->
    let t1 = Clock.time () in
    let time = t1 -. t0 in
    C.log_s c (Printf.sprintf "Wrote %d bytes in %.3f seconds (%.2f KB/s)"
      bytes time
      (float_of_int bytes /. time /. 1024.)) >>= fun () ->
    let events = Profile.events () in
    for_lwt i = 0 to Array.length events - 1 do
      C.log_s c (Profile.to_string events.(i))
    done >>= fun () ->
    OS.Time.sleep 1.0
end


UDP transmission

Last time, we saw packet transmission being interrupted by periods of sleeping, garbage collection, and some brief but mysterious pauses.
I noted that the GC tended to run during Ring.ack_responses, suggesting that was getting called quite often, and with the new tracing we can see why.



This trace shows a unikernel booting (scroll left if you want to see that) and then sending 10 UDP packets.
I’ve left the trace running a little longer so you can see the acks (this is very obvious when sending more than 10 packets, but I wanted to keep this trace small):


View full screen | Download udp.sexp

Mirage creates two threads for each packet that we add to the ring buffer and they stick around until we get a notification back from dom0 that the packet has been read (actually, we create six threads for each packet, but the bind simplification hides four of them).

It looks like each packet is in two parts, as each one generates two acks, one much later than the other.
I think the two parts are the UDP header and the payload, which each have their own IO page.
Given the time needed to share and unshare pages, it would probably be more efficient to copy the payload into the same page as the header.
Interestingly, dom0 seems to ack all the headers first, but holds on to the payload pages for longer.

With 20 threads for just ten packets, you can imagine that the trace gets rather crowded when sending thousands!

TCP transmission

As before, the TCP picture is rather complicated:


View full screen | Download tcp.sexp

The above shows a unikernel running on my ARM Cubietruck connecting to netcat on my laptop and sending 100 TCP packets over the stream.
There are three counters here:

  main-to-tcp (purple) is incremented by the main thread just before sending a block of data to the TCP stream (just enough to fill one TCP segment).
  tcp-to-ip (red) shows when the TCP system sent a segment to the IP layer for transmission.
  tcp-ackd-segs (orange) shows when the TCP system got confirmation of receipt from the remote host (note: a TCP ask is not the same as a dom0 ring ack, which just says the network driver has accepted the segment for transmission).


There is clearly scope to improve the viewer here, but a few things can be seen already.
The general cycle is:

  The unikernel is sleeping, waiting for TCP acks.
  The remote end acks some packets (the orange line goes up).
  The TCP layer transmits some of the buffered packets (red line goes up).
  The TCP layer allows the main code to send more data (purple line goes up).
  The transmitted pages are freed (the dense vertical green lines) once Xen acks them.


I did wonder whether we unshared the pages as soon as dom0 had read the segment, or only when the remote end sent the TCP ack.
Having the graphs overlaid on the trace lets us answer this question - you can see that when the red line goes up (segments sent to dom0), the ring.write thread that is created then ends (and the page is unshared) in response to ring.poll ack_responses, before the TCP acks arrive.

TCP starts slowly, but as the window size gets bigger and more packets are transmitted at a time, the sleeping periods get shorter and then disappear as the process becomes CPU-bound.

There’s also a long garbage collection period near the end (shortly before we close the socket).
This might be partly the fault of the tracing system, which currently allocates lots of small values, rather than writing to a preallocated buffer.

Disk access

For our final example, let’s revisit the block device profiling from last time.
Back then, making a series of read requests, each for 32 pages of data, produced this chart:



With the new tracing, we can finally see what those mysterious wake-ups in the middle are:


View full screen | Download disk-direct.sexp

Each time the main test code’s read call returns, the orange trace (“read”) goes up.
You can see that we make three blocking calls to dom0 for each request.
I added another counter for the number of active grant refs (pages shared with dom0), shown as the red line (“gntref”).
You can see that for each call we share a bunch of pages, wait, and then unshare them all again.

In each group of three, we share 11 pages for the first two requests, but only 10 for the third.
This makes obvious what previously required a careful reading of the block code: requests for more than 11 pages have to be split up because that’s all you can fit in the request structure.
Our request for 32 pages is split into requests for 11 + 11 + 10 pages, which are sent in series.

In fact, Xen also supports “indirect” requests, where the request structure references full pages of requests.
I added support for this to mirage-block-xen, which improved the speed nicely.
Here’s a trace with indirect requests enabled:


View full screen | Download disk-indirect.sexp

If you zoom in where the red line starts to rise, you can see it has 32 steps, as we allocate all the pages in one go, followed by a final later increment for the indirect page.

Zooming out, you can see we paused for GC a little later.
We got lucky here, with the GC occurring just after we sent the request and just before we started waiting for the reply, so it hardly slowed us down.
If we’d been unlucky the GC might have run before we sent the request, leaving dom0 idle and wasting the time.
Keeping multiple requests in flight would eliminate this risk.

Implementation notes

I originally wrote the viewer as a native GTK application in OCaml.
The browser version was created by running the magical js_of_ocaml tool, which turned out to be incredibly easy.
I just had to add support for the HTML canvas API alongside the code for GTK’s Cairo canvas, but they’re almost the same anyway.
Now my embarrassing inability to learn JavaScript need not hold me back!

Finding a layout algorithm that produced sensible results was the hardest part.
I’m quite pleased with the result.
The basic algorithm is:

  Generate an interval tree of the thread lifetimes.
  Starting with the root thread, place each thread at the highest place on the screen where it doesn’t overlap any other threads, and no higher than its parent.
  Visit the threads recursively, depth first, visiting the child threads created in reverse order.
  If one thread merges with another, allow them to overlap.
  Don’t show bind-type threads as children of their actual creator, but instead delay their start time to when they get activated and make them children of the thread that activates them, unless their parent merges with them.


For the vertical layout I originally used scrolling, but it was hard to navigate.
It now transforms the vertical coordinates from the layout engine by passing them through the tanh function, allowing you to focus on a particular thread but still see all the others, just more bunched up.
The main difficulty here is focusing on one of the top or bottom threads without wasting half the display area, which complicated the code a bit.

Summary

Understanding concurrent programs can be much easier with a good visualisation.
By instrumenting Lwt, it was quite easy to collect useful information about what threads were doing.
Libraries that use Lwt only needed to be modified in order to label the threads.

My particular interest in making these tools is to explore the behaviour of Mirage unikernels - tiny virtual machines written in OCaml that run without the overhead of traditional operating systems.

The traces produced provide much more information than the graphs I made previously.
We can see now not just when the unikernel isn’t making progress, but why.
We saw that the networking code spends a lot of time handling ack messages from dom0 saying that it has read the data we shared with it, and that the disk code was splitting requests into small chunks because it didn’t support indirect requests.

There is plenty of scope for improvement in the tools - some things I’d like include:

  A way to group or hide threads if you want to focus on something else, as diagrams can become very cluttered with e.g. threads waiting for shared pages to be released.
  The ability to stitch together traces from multiple machines so you can e.g. follow the progress of an IP packet after it leaves the VM.
  A visual indication of when interrupts occur vs when Mirage gets around to servicing them.
  More stats collection and reporting (e.g. average response time to handle a TCP request, etc).
  A more compact log format and more efficient tracing.


But hopefully, these tools will already help people to learn more about how their unikernels behave.
If you’re interested in tracing or unikernels, the Mirage mailing list is a good place to discuss things.





Hide
        
      
                    by Thomas Leonard at Oct 27, 2014 
      
      
    
  


       
                  Eighth OCaml compiler hacking evening (at Mill Lane, by the river)
      (Compiler Hacking)
    
    
                                Update: This session will be a joint F#/OCaml hacking event, beginning with a talk from Don Syme about F# compiler and language development!

For the eighth Cambridge OCaml compiler hacking evening we'll be meeting in the University Postdoc centre at 16 Mill Lane (near the river, next door to Makespace) on 6.30pm Tuesday 30th September.

If you're planning to come along, it'd be helpful if you could indicate interest via Doodle and sign up to the mailing list to receive updates:

Where:
  Postdoc Centre  Basement, 16 Mill Lane  Cambridge, CB2 1SB  United Kingdom  

When: 6.30pm, Tuesday 30th September

Who: anyone interested in improving OCaml. Knowledge of OCaml programming will obviously be helpful, but prior experience of working on OCaml internals isn't necessary.

What: fixing bugs, implementing new features, learning about OCaml internals.  This time we'll be focusing on code quality: refactoring, adding test cases, reviewng existing proposals and updating packages after the r…Read more...Update: This session will be a joint F#/OCaml hacking event, beginning with a talk from Don Syme about F# compiler and language development!

For the eighth Cambridge OCaml compiler hacking evening we'll be meeting in the University Postdoc centre at 16 Mill Lane (near the river, next door to Makespace) on 6.30pm Tuesday 30th September.

If you're planning to come along, it'd be helpful if you could indicate interest via Doodle and sign up to the mailing list to receive updates:

Where:
  Postdoc Centre  Basement, 16 Mill Lane  Cambridge, CB2 1SB  United Kingdom  

When: 6.30pm, Tuesday 30th September

Who: anyone interested in improving OCaml. Knowledge of OCaml programming will obviously be helpful, but prior experience of working on OCaml internals isn't necessary.

What: fixing bugs, implementing new features, learning about OCaml internals.  This time we'll be focusing on code quality: refactoring, adding test cases, reviewng existing proposals and updating packages after the recent release of OCaml 4.02.

Wiki: https://github.com/ocamllabs/compiler-hacking/wiki

We're defining "compiler" pretty broadly, to include anything that's part of the standard distribution, which means at least the standard library, runtime, tools (ocamldep, ocamllex, ocamlyacc, etc.), ocamlbuild, the documentation, and the compiler itself. We'll have suggestions for mini-projects for various levels of experience (see also some things we've done on previous evenings), but feel free to come along and work on whatever you fancy.

We'll also be ordering pizza, so if you want to be counted for food you should aim to arrive by 6.45pm.
Hide
        
      
                    by Compiler Hacking at Sep 23, 2014 
      
      
    
  


       
                  Simplifying the solver with functors
      (Thomas Leonard)
    
    
                                After converting 0install to OCaml, I’ve been looking at using more of OCaml’s features to further clean up the APIs.
In this post, I describe how using OCaml functors has made 0install’s dependency solver easier to understand and more flexible.



( this post also appeared on Hacker News and Reddit )

Table of Contents

  Introduction          How dependency solvers work
      Optimising the result
    
  
  The current solver code
  Discovering the interface
  Comparison with Java
  Diagnostics
  Selections
  Summary


Introduction

To run a program you need to pick a version of it to use, as well as compatible versions of all its dependencies.
For example, if you wanted 0install to select a suitable set of components to run SAM, you could do it like this:

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
$ 0install select http://www.serscis.eu/0install/serscis-access-modeller
- URI: http://www.serscis.eu/0install/serscis-access-modeller
  Version: 0.16
  Path: (not cached)…Read more...After converting 0install to OCaml, I’ve been looking at using more of OCaml’s features to further clean up the APIs.
In this post, I describe how using OCaml functors has made 0install’s dependency solver easier to understand and more flexible.



( this post also appeared on Hacker News and Reddit )

Table of Contents

  Introduction          How dependency solvers work
      Optimising the result
    
  
  The current solver code
  Discovering the interface
  Comparison with Java
  Diagnostics
  Selections
  Summary


Introduction

To run a program you need to pick a version of it to use, as well as compatible versions of all its dependencies.
For example, if you wanted 0install to select a suitable set of components to run SAM, you could do it like this:

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
$ 0install select http://www.serscis.eu/0install/serscis-access-modeller
- URI: http://www.serscis.eu/0install/serscis-access-modeller
  Version: 0.16
  Path: (not cached)
  
  - URI: http://repo.roscidus.com/utils/graphviz
    Version: 2.38.0-2
    Path: (package:arch:graphviz:2.38.0-2:x86_64)
  
  - URI: http://repo.roscidus.com/java/swt
    Version: 3.6.1
    Path: (not cached)
  
  - URI: http://repo.roscidus.com/java/iris
    Version: 0.6.0
    Path: (not cached)
  
  - URI: http://repo.roscidus.com/java/openjdk-jre
    Version: 7.65-2.5.2-1
    Path: (package:arch:jre7-openjdk:7.65-2.5.2-1:x86_64)

Here, the solver selected SAM version 0.16, along with its dependencies. GraphViz 2.38.0 and OpenJDK-JRE 7.65 are already installed from my distribution repository, while SWT 3.6.1 and IRIS 0.6.0 need to be downloaded (e.g. using 0install download).

This post is about the code that decides which versions to use, and my attempts to make it easier to understand using OCaml functors and abstraction.
For a gentle introduction to functors, see Real World OCaml: Chapter 9. Functors.

How dependency solvers work

( This section isn’t about functors, but it’s quite interesting background. You can skip it if you prefer. )

Let’s say I want to run foo, a graphical Java application.
There are three versions available:

  foo1 (stable)
          requires Java 6..!7 (at least version 6, but before version 7)
    
  
  foo2 (stable)
          requires Java 6.. (at least version 6)
      requires SWT
    
  
  foo3 (testing)
          requires Java 7..
      requires SWT
    
  


Let’s imagine we have some candidates for Java and SWT too:

  Java: java6_32bit, java6_64bit, java7_64bit, java8_64bit
  SWT: swt35_32bit, swt35_64bit, swt36_32bit, swt36_64bit


My computer can run 32- and 64-bit binaries, so we need to consider both.

We start by generating a set of boolean constraints defining the necessary and sufficient conditions for a set of valid selections.
Each candidate becomes one variable, with true meaning it will be used and false that it won’t (following the approach in OPIUM).
For example, we don’t want to select more than one version of each component, so the following must all be true:

1
2
3
at_most_one(foo1, foo2, foo3)
at_most_one(java6_32bit, java6_64bit, java7_64bit, java8_64bit)
at_most_one(swt35_32bit, swt35_64bit, swt36_32bit, swt36_64bit)


We must select some version of foo itself, since that’s our goal:

1
foo1 or foo2 or foo3


If we select foo1, we must select one of the Java 6 candidates.
Another way to say this is that we must either not select foo1 or, if we do, we must select a compatible Java version:

1
2
3
4
5
6
not(foo1) or java6_32bit or java6_64bit
not(foo2) or java6_32bit or java6_64bit or java7_64bit or java8_64bit
not(foo3) or java7_64bit or java8_64bit
not(foo2) or swt35_32bit or swt35_64bit or swt36_32bit or swt36_64bit
not(foo3) or swt35_32bit or swt35_64bit or swt36_32bit or swt36_64bit


SWT doesn’t work with Java 8 (in this imaginary example):

1
2
3
4
not(swt35_32bit) or not(java8_64bit)
not(swt35_64bit) or not(java8_64bit)
not(swt36_32bit) or not(java8_64bit)
not(swt36_64bit) or not(java8_64bit)


Finally, although we can use 32 bit or 64 bit programs, we can’t mix different types within a single program:

1
2
3
4
5
6
7
8
9
not(java6_32bit) or not(x64)
not(java6_64bit) or x64
not(java7_64bit) or x64
not(java8_64bit) or x64
not(swt35_32bit) or not(x64)
not(swt35_64bit) or x64
not(swt36_32bit) or not(x64)
not(swt36_64bit) or x64


Once we have all the equations, we throw them at a standard SAT solver to get a set of valid versions.
0install’s SAT solver is based on the MiniSat algorithm.
The basic algorithm, DPLL, works like this:

  The SAT solver simplifies the problem by looking for clauses with only a single variable.
If it finds any, it knows the value of that variable and can simplify the other clauses.
  When no further simplification is possible, it asks its caller to pick a variable to try.
Here, we might try foo2=true, for example.
It then goes back to step 1 to simplify again and so on, until it has either a solution or a conflict.
If a variable assignment leads to a conflict, then we go back and try it the other way (e.g. foo2=false).


In the above example, the process would be:

  No initial simplification is possible.
  Try foo2=true.
  This immediately leads to foo1=false and foo3=false.
  Now we try java8_64bit=true.
  This immediately removes the other versions of Java from consideration.
  It also immediately leads to x64=true, which eliminates all the 32-bit binaries.
  It also eliminates all versions of SWT.
  foo2 depends on SWT, so eliminating all versions leads to foo2=false, which is a conflict because we already set it to true.


MiniSat, unlike basic DPLL, doesn’t just backtrack when it gets a conflict.
It also works backwards from the conflicting clause to find a small set of variables that are sufficient to cause the conflict.
In this case, we find that foo2 and java8_64bit implies conflict.
To avoid the conflict, we must make sure that at least one of these is false, and we can do this by adding a new clause:

1
not(foo2) or not(java8_64bit)


In this example, this has the same effect as simple backtracking, but if we’d chosen other variables between these two then learning the general rule could save us from exploring many other dead-ends.
We now try with java7_64bit=true and then swt36_64bit=true, which leads to a solution.

Optimising the result

The above process will always find some valid solution if one exists, but we generally want an “optimal” solution (e.g. preferring newer versions).
There are several ways to do this.

In his talk at OCaml 2014, Using Preferences to Tame your Package Manager, Roberto Di Cosmo explained how their tools allow you to specify a function to be optimised (e.g. -count(removed),-count(changed) to minimise the number of packages to be removed or changed).
When you have a global set of package versions (as in OPAM, Debian, etc) this is very useful, because the package manager needs to find solution that balances the needs of all installed programs.

0install has an interesting advantage here.
We can install multiple versions of libraries in parallel, and we don’t allow one program to influence another program’s choices.
If we run another SWT application, bar, we’ll probably pick the same SWT version (bar will probably also prefer the latest stable 64-bit version) and so foo and bar will share the copy.
But if we run a non-SWT Java application, we are free to pick a better version of Java (e.g. Java 8) just for that program.

This means that we don’t need to find a compromise solution for multiple programs.
When running foo, the version of foo itself is far more important than the versions of the libraries it uses.
We therefore define the “optimal” solution as the one optimising first the version of the main program, then the version of its first dependency and so on.
This means that:

  The behaviour is predictable and easy to explain.
  The behaviour is stable (small changes to libraries deep in the tree have little effect).
  Programs can rank their dependencies by importance, because earlier dependencies are optimised first.
  If foo2 is the best version for our policy (e.g. “prefer latest stable version”) then every solution with foo2=true is better than every solution without.
If we direct the solver to try foo2=true and get a solution, there’s no point considering the foo2=false cases.
This means that the first solution we find will always be optimal, which is very fast!


The current solver code

The existing code is made up of several components:

Arrows indicate “uses” relationship.

  SAT Solver
  Implements the SAT solver itself (if you want to use this code in your own projects, Simon Cruanes has
made a standalone version at https://github.com/c-cube/sat, which has some additional features and
optimisations).
  Solver
  Fetches the candidate versions for each interface URI (equivalent to the package name in other systems) and builds up the SAT problem, then uses the SAT solver to solve it.
It directs the SAT solver in the direction of the optimal solution when there is a choice, as explained above.
  Impl provider
  Takes an interface URI and provides the candidates (“implementations”) to the solver.
The candidates can come from multiple XML “feed” files: the one at the given URI itself, plus any extra feeds that one imports, plus any additional local or remote feeds the user has added (e.g. a version in a local Git checkout or which has been built from source locally).
The impl provider also filters out obviously impossible choices (e.g. Windows binaries if we’re on Linux, uncached versions in off-line mode, etc) and then ranks the remaining candidates according to the local policy (e.g. preferring stable versions, higher version numbers, etc).
  Feed provider
  Simply loads the feed XML from the disk cache or local file, if available.
It does not access the network.
  Driver
  Uses the solver to get an initial solution.
Then it asks the feed provider which feeds the solver tried to access and starts downloading them.
As new feeds arrive, it runs the solver again, possibly starting more downloads.
Managing these downloads is quite interesting; I used it as the example in Asynchronous Python vs OCaml.


This is reasonably well split up already, thanks to occasional refactoring efforts, but we can always do better.
The subject of today’s refactoring is the solver module itself.

Here’s the problem:

solver.mli 
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
class type result =
  object
    method get_selections : Selections.t
    (* The remaining methods are used to provide diagnostics *)
    method get_selected : source:bool -> General.iface_uri ->
    			  Impl.generic_implementation option
    method impl_provider : Impl_provider.impl_provider
    method implementations :
      ((General.iface_uri * bool) *
       (diagnostics * Impl.generic_implementation) option) list
    method requirements : requirements
  end
(** Find a set of implementations which satisfy these
  * requirements.
  * @param closest_match adds a lowest-ranked (but valid)
  *        implementation to every interface, so we can always
  *        select something. Useful for diagnostics. *)
val do_solve :
  Impl_provider.impl_provider -> requirements ->
  closest_match:bool -> result option


What does do_solve actually do?
It gets candidates (of type Impl.generic_implementation) from Impl_provider and produces a Selections.t.
Impl.generic_implementation is a complex type including, among other things, the raw XML <implementation> element from the feed XML.
A Selections.t is a set of <selection> XML elements.

In other words: do_solve takes some arbitrary XML and produces some other XML.
It’s very hard to tell from the solver.mli interface file what features of the input data it uses in the solve, and which it simply passes through.

Now imagine that instead of working on these messy concrete types, the solver instead used only a module with this type:

sigs.mli 
1
2
3
4
5
6
7
8
9
10
11
12
module type SOLVER_INPUT = sig
  type impl_provider
  type impl
  type dependency
  type restriction
  val implementations : impl_provider -> iface_uri -> impl list
  val dependencies : impl -> dependency list
  val required_interface : dependency -> iface_uri
  val restrictions : dependency -> restriction list
  val meets_restriction : impl -> restriction -> bool
end


We could then see exactly what information the solver needed to do its job.
For example, we could see just from the type signature that the solver doesn’t understand version numbers, but just uses meets_restriction to check whether an abstract implementation (candidate) meets an abstract restriction.

Using OCaml’s functors we can do just this, splitting out the core (non-XML) parts of the solver into a Solver_core module with a signature something like:

solver_core.mli 
1
2
3
4
5
module Make : functor (Model : Sigs.SOLVER_INPUT) -> sig
  val do_solve :
    Model.impl_provider -> requirements ->
    closest_match:bool -> Model.impl StringMap.t
end


This says that, given any concrete module that matches the SOLVER_INPUT type, the Make functor will return a module with a suitable do_solve function.
In particular, the compiler will check that the solver core makes no further assumptions about the types.
If it assumes that a Model.impl_provider is any particular concrete type then solver_core.ml will fail to compile, for example.

Discovering the interface

The above sounds nice in theory, but how easy is it to change the existing code to the new design?
I don’t even know what SOLVER_INPUT will actually look like - surely more complex than the example above!
Actually, it turned out to be quite easy.
You can start with just a few concrete types, e.g.

sigs.mli 
1
2
3
4
module type SOLVER_INPUT = sig
  type impl_provider = Impl_provider.impl_provider
  type impl = Impl.generic_implementation
end


This doesn’t constrain Solver_core at all, since it’s allowed to know the real types and use them as before.
This step just lets us make the Solver_core.Make functor and have things still compile and work.

Next, I made impl_provider abstract (removing the = Impl_provider.impl_provider), letting the compiler find all the places where the code assumed the concrete type.

First, it was getting the candidate implementations for an interface from it.
The impl_provider was actually returning several things: the valid candidates, the rejects, and an optional conflicting interface (used when one interface replaces another).
The solver doesn’t use the rejects, which are only needed for the diagnostics system, so we can simplify that interface here.

Secondly, not all dependencies need to be considered (e.g. a Windows-only dependency when we’re on Linux).
Since the impl_provider already knows the platform in order to filter out incompatible binaries, we also use it to filter the dependencies.
Our new module type is:

sigs.mli 
1
2
3
4
5
6
7
8
9
10
11
12
13
14
module type SOLVER_INPUT = sig
  type impl_provider
  type impl = Impl.generic_implementation
  type dependency = Impl.dependency
  type iface_info = {
    replacement : iface_uri option;
    impls : impl list;
  }
  val implementations : impl_provider -> iface_uri ->
                        source:bool -> iface_info
  val is_dep_needed : impl_provider -> dependency -> bool
end


At this point, I’m not trying to improve the interface, just to find out what it is.
Continuing to make the types abstract in this way is a fairly mechanical process, which led to:

sigs.mli 
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
module type SOLVER_INPUT = sig
  type t  (* new name for impl_provider *)
  type impl
  type command
  type dependency
  type restriction
  type iface_info = {
    replacement : iface_uri option;
    impls : impl list;
  }
  val implementations : t -> iface_uri -> source:bool -> iface_info
  val get_command : impl -> string -> command option
  val requires : impl -> dependency list
  val command_requires : command -> dependency list
  val machine_group : impl -> Arch.machine_group option
  val restrictions : dependency -> restriction list
  val meets_restriction : impl -> restriction -> bool
  val dep_iface : dependency -> iface_uri
  val dep_required_commands : dependency -> string list
  val dep_essential : dependency -> bool
  val restricts_only : dependency -> bool
  val is_dep_needed : t -> dependency -> bool
  val impl_self_commands : impl -> string list
  val command_self_commands : command -> string list
  val impl_to_string : impl -> string
  val command_to_string : command -> string
end


The main addition is the command type, which is essentially an optional entry point to an implementation.
For example, if a library can also be used as a program then it may provide a “run” command, perhaps adding a dependency on an option parser library.
Programs often also provide a “test” command for running the unit-tests, etc.
There are also two to_string functions at the end for debugging and diagnostics.

Having elicited this API between the solver and the rest of the system, it was clear that it had a few flaws:

      is_dep_needed is pointless. There are only two places where we pass a dependency to the solver (requires and command_requires), so we can just filter the unnecessary dependencies out there and not bother the solver core with them at all.
The abstraction ensures there’s no other way for the solver core to get a dependency.
  
      impl_self_commands and command_self_commands worried me.
These are used for the (rare) case that one command in an implementation depends on another command in the same implementation.
This might happen if, for example, the “test” command wants to test the “run” command.
Logically, these are just another kind of dependency; returning them separately means code that follows dependencies might forget them.

    Sure enough, there was just such a bug in the code.
When we build the SAT problem we do consider self commands (so we always find a valid result), but when we’re optimising the result we ignore them, possibly leading to non-optimal solutions.
I added a unit-test and made requires return both dependencies and self commands together to avoid the same mistake in future.
  
      For a similar reason, I replaced dep_iface, dep_required_commands, restricts_only and dep_essential with a single dep_info function returning a record type.
  
      I added type command_name = private string.
This means that the solver can’t confuse command names with other strings and makes the type signature more obvious.
I didn’t make it fully abstract, but was a bit lazy and used private, allowing the solver to cast to a string for debug logging and to let it use them as keys in a StringMap.
  
      There is a boolean source attribute in do_solve and implementations.
This is used if the user wants to select source code rather than a binary.
I wanted to support the case of a compiler that is compiled using an older version of itself (though I never completed this work).
In that case, we need to select different versions of the same interface, so the solver actually picks a unique implementation for each (interface, source) pair.

    I tried giving these pairs a new abstract type - “role” - and that simplified things nicely.
It turned out that every place where we passed only an interface (e.g. dep_iface), we eventually ended up doing (iface, false) to get back to a role, so I was able to replace these with roles too.

    This is quite significant.
Currently, the main interface can be source or binary but dependencies are always binary.
For example, source code may depend on a compiler, build tool, etc.
People have wondered in the past how easy it would be to support dependencies on source code too - it turns out this now requires no changes to the solver, just an extra attribute in the XML format!
  
      With the role type now abstract, I removed Model.t (the impl_provider) and moved it inside the role type.
This simplifies the API and allows us to use different providers for different roles (imagine solving for components to cross-compile a program; some dependencies like make should be for the build platform, while others are for the target).
  


Here’s the new API:

sigs.mli 
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
module type SOLVER_INPUT = sig
  module Role : sig
    type t
    val to_string : t -> string
    val compare : t -> t -> int
  end
  type impl
  type command
  type restriction
  type command_name = private string
  type dependency = {
    dep_role : Role.t;
    dep_restrictions : restriction list;
    dep_importance : [ `essential | `recommended | `restricts ];
    dep_required_commands : command_name list;
  }
  type role_information = {
    replacement : Role.t option;
    impls : impl list;
  }
  type requirements = {
    role : Role.t;
    command : command_name option;
  }
  val implementations : Role.t -> role_information
  val get_command : impl -> command_name -> command option
  val requires : Role.t -> impl -> dependency list * command_name list
  val command_requires : Role.t -> command -> dependency list * command_name list
  val machine_group : impl -> Arch.machine_group option
  val meets_restriction : impl -> restriction -> bool
  val impl_to_string : impl -> string
  val command_to_string : command -> string
end


Note: Role is a submodule to make it easy to use it as the key in a map.

Hopefully you find it much easier to understand what the solver does (and doesn’t) do from this type.
The Solver_core code no longer depends on the rest of 0install and can be understood on its own.

The remaining code in Solver defines the implementation of a SOLVER_INPUT module and applies the functor, like this:

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
module CoreModel = struct
  type impl = Impl.generic_implementation
  type command = Impl.command
  type restriction = Impl.restriction
  type command_name = string
  type dependency = Role.t * Impl.dependency
  ...
  let get_command impl name =
    StringMap.find name Impl.(impl.props.commands)
  let dep_info (role, dep) = {
    dep_role = {
      scope = role.scope;
      iface = dep.Impl.dep_iface;
      (* note: only dependencies on binaries supported for now. *)
      source = false;
    };
    dep_importance = dep.Impl.dep_importance;
    dep_required_commands = dep.Impl.dep_required_commands;
  }
  ...
end
module Core = Solver_core.Make(CoreModel)
...


The CoreModel implementation of SOLVER_INPUT simply maps the abstract types and functions to use the real types.
The limitation that dependencies are always binary is easier to see here, and it’s fairly obvious how to fix it.

Note that we don’t define the module as module CoreModel : SOLVER_INPUT = ....
The rest of the code in Solver still needs to see the concrete types; only Solver_core is restricted to see it just as a SOLVER_INPUT.

Comparison with Java

Using functors for this seemed pretty easy, and I started wondering how I’d solve this problem in other languages.
Python simply can’t do this kind of thing, of course - there you have to read all the code to understand what it does.
In Java, we might declare some abstract interfaces, though:

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
interface RoleInfo {
  List<Impl> get_implementations();
  Role get_replacement();
}
interface Impl {
  Command get_command(String name);
  String machine_group();
  List<Dependency> requires();
}
interface Restriction {
  boolean meets_restriction(Impl i);
}
interface Dependency {
  Role get_role();
  List<Restriction> get_restrictions();
  Importance get_importance();
  List<String> get_required_commands();
}
interface Role {
  RoleInfo get_implementations();
}
class Solver {
  public Map<Role,Impl> do_solve(Role r, String c) {
    ...
  }
}


There’s a problem, though.
We can create a ConcreteRole and pass that to Solver.do_solve, but we’ll get back a map from abstract roles to abstract impls.
We need to get concrete types out to do anything useful with the result.

A Java programmer would probably cast the results back to the concrete types, but there’s a problem with this (beyond the obvious fact that it’s not statically type checked):
If we accept dynamic casting as a legitimate technique (OCaml doesn’t support it), there’s nothing to stop the abstract solver core from doing it too.
We’re back to reading all the code to find out what information it really uses.

There are other places where dynamic casts are needed too, such as in meets_restriction (which needs a concrete implementation, not an abstract one).

I did try using generics, but I didn’t manage to get it to compile, and I stopped when I got to:

1
2
3
  public <I,R,D> Map<Role<I,R,D>,Impl<I,R,D>>
         do_solve(Role<I,R,D> role, String command) {
    ...


I think it’s fair to say that if this ever did compile, it certainly wouldn’t have made the code easier to read.

Diagnostics

The Diagnostics module takes a failed solver result (produced with do_solve ~closest_match:true) that is close to what we think the user wanted but with some components left blank, and tries to explain to the user why none of the available candidates was suitable
(see the Trouble-shooting guide for some examples and pictures).

I made a SOLVER_RESULT module type which extended SOLVER_INPUT with the final selections and diagnostic information:

sigs.mli 
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
module type SOLVER_RESULT = sig
  include SOLVER_INPUT
  type t
  val requirements : t -> requirements
  val user_restrictions : Role.t -> restriction option
  val get_selected : t -> Role.t -> impl option
  val selected_commands : t -> Role.t -> command_name list
  val explain : t -> Role.t -> string
  type rejection
  val rejects : Role.t -> (impl * rejection) list
  val describe_problem : impl -> rejection -> string
  type version
  val version : impl -> version
  val format_version : version -> string
  val id_of_impl : impl -> string
  val format_machine : impl -> string
  val string_of_restriction : restriction -> string
  val dummy_impl : impl	(** Placeholder for missing impls *)
end


Note: the explain function here is for the diagnostics-of-last-resort; the diagnostics system uses it on cases it can’t explain itself, which generally indicates a bug somewhere.

Then I made a Diagnostics.Make functor as with Solver_core.
This means that the diagnostics now sees the same information as the solver, with the above additions.
For example, it sees the same dependencies as the solver did (e.g. we can’t forget to filter them out with is_dep_needed).
Like the solver, the diagnostics assumed that a dependency was always a binary dependency and used (iface, false) to get the role.
Since the role is now abstract, it can’t do this and should cope with source dependencies automatically.

The new API prompted me to consider self-command dependencies again, so the diagnostics code is now able to explain correctly problems caused by missing self-commands (previously, I forgot to handle this case).

Selections

Sometimes we use the results of the solver directly.
In other cases, we save them to disk as an XML selections document first.
These XML documents are handled by the Selections module, which had its own API.

For consistency, I decided to share type names and methods as much as possible.
I split out the core of SOLVER_INPUT into another module type:

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
module type CORE_MODEL = sig
  module Role : sig ... end
  type impl
  type command
  type command_name = private string
  type dependency
  type dep_info = { ... }
  type requirements = { ... }
  val requires : Role.t -> impl -> dependency list * command_name list
  val command_requires : Role.t -> command -> dependency list * command_name list
  val dep_info : dependency -> dep_info
  val get_command : impl -> command_name -> command option
end
module type SOLVER_INPUT = sig
  include CORE_MODEL
  type role_information = { ... }
  type restriction
  val impl_to_string : impl -> string
  val command_to_string : command -> string
  val implementations : Role.t -> role_information
  val restrictions : dependency -> restriction list
  val meets_restriction : impl -> restriction -> bool
  val machine_group : impl -> Arch.machine_group option
end


Actually, there is some overlap with SOLVER_RESULT too, so I created a SELECTIONS type as well:



Now the relationship becomes clear.
SOLVER_INPUT extends the core model with ways to get the possible candidates and restrictions on their use.
SELECTIONS extends the core with ways to find out which implementations were selected.
SOLVER_RESULT combines the above two, providing extra information for diagnostics by relating the selections back to the candidates (information that isn’t available when loading saved selections).

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
module type SELECTIONS = sig
  type t
  type role
  type impl
  type command_name
  type requirements
  val get_selected : role -> t -> impl option
  val selected_commands : t -> role -> command_name list
  val requirements : t -> requirements
  module RoleMap : MAP with type key = role
end
module type SOLVER_RESULT = sig
  include SOLVER_INPUT
  include SELECTIONS with
    type impl := impl and
    type command_name := command_name and
    type requirements := requirements and
    type role = Role.t
  type rejection
  val rejects : Role.t -> (impl * rejection) list
  type version
  val version : impl -> version
  val format_version : version -> string
  val user_restrictions : Role.t -> restriction option
  val id_of_impl : impl -> string
  val format_machine : impl -> string
  val string_of_restriction : restriction -> string
  val describe_problem : impl -> rejection -> string
  val explain : t -> Role.t -> string
  val raw_selections : t -> impl RoleMap.t
end


I had a bit of trouble here.
I wanted to include CORE_MODEL in SELECTIONS, but doing that caused an error when I tried to bring them together in SOLVER_RESULT,
because of the duplicate Role submodule.
So instead I just define the types I need and let the user of the signature link them up.

Update: I’ve since discovered that you can just do with module Role := Role to solve this.

Correct use of the with keyword seems the key to a happy life with OCaml functors.
When defining the RoleMap submodule in SELECTIONS I use it to let users know the keys of a RoleMap are the same type as SELECTIONS.role (otherwise it will be abstract and you can’t assume anything about it).
In SOLVER_RESULT, I use it to link the types in SELECTIONS with the types in SOLVER_INPUT.

Notice the use of = vs :=.
= says that two types are the same.
:= additionally removes the type from the module signature.
We use := for impl because we already have a type with that name from SOLVER_INPUT and we can’t have two.
However, we use = for role because that doesn’t exist in CORE_MODEL and we’d like SOLVER_RESULT to include everything in SELECTIONS.

Finally, I included the new SELECTIONS signature in the interface file for the Selections module:

selections.mli 
1
2
3
4
5
6
7
8
9
10
11
12
13
14
type selection = [`selection] Element.t
type role = {
  iface : General.iface_uri;
  source : bool;
}
include Sigs.CORE_MODEL with
  type impl = selection and
  type command_name = string and
  type Role.t = role
include Sigs.SELECTIONS with
  type role := role and
  type command_name := command_name and
  type requirements := requirements and
  type impl := selection


With this change, anything that uses SELECTIONS can work with both loaded selections and with the solver output, even though they have different implementations.
For example, the Tree module generates a tree (for display) from a selections dependency graph (pruning loops and diamonds).
It’s now a functor, which can be used with either.
For example, 0install show selections.xml applies it to the Selections module:

1
2
3
module SelectionsTree = Tree.Make(Selections)
... SelectionsTree.as_tree sels


The same code is used in the GUI to render the tree view, but now with Make(Solver.Model).
As before, it’s important to preserve the types - the GUI needs to know that each node in the tree is a Solver.Model.Role.t so that it can show information about the available candidates, for example.

Summary

An important function of a package manager is finding a set of package versions that are compatible.
An efficient way to do this is to express the necessary and sufficient constraints as a set of boolean equations and then use a SAT solver to find a solution.
While finding a valid solution is easy, finding the optimal one can be harder.
Because 0install is able to install libraries in parallel and can choose to use different versions for different applications, it only needs to consider one application at a time.
As well as being faster, this makes it possible to use a simple definition of optimality that is easy to compute.

0install’s existing solver code has already been broken down into modular components: downloading metadata, collecting candidates, rejecting invalid candidates and ranking the rest, building the SAT problem and solving it.
However, the code that builds the SAT problem and optimises the solution was tightly coupled to the concrete representation, making it harder to see what it was doing and harder to extend it with new features.
Its type signature essentially just said that it takes XML as input and returns XML as output.

OCaml functors are functions over modules.
They allow a module to declare the interface it expects from its dependencies in an abstract way, providing just the information the module requires and nothing else.
The module can then be compiled against this abstract interface, ensuring that it makes no assumptions about the actual types.
Later, the functor can be applied to the concrete representation to get a module that uses the concrete types.

Turning the existing solver code into a functor turned out to be a simple iterative process that discovered the existing implicit API between the solver and the rest of the code.
Once this abstract API had been found, many possible improvements became obvious.
The new solver core is both simpler than the original and can be understood on its own without looking at the rest of the code.
It is also more flexible: we could now add support for source dependencies, cross-compilation, etc, without changing the core of the solver.
The challenge now is only how to express these things in the XML format.

In a language without functors, such as Java, we could still define the solver to work over abstract interfaces, but the results returned would also be abstract, which is not useful.
Trying to achieve the same effect as functors using generics appears very difficult and the resulting code would likely be hard to read.

Splitting up the abstract interface into multiple module types allowed parts of the interface to be shared with the separate selections-handling module.
This in turn allowed another module - for turning selections into trees - to become a functor that could also work directly on the solver results.
Finally, it made the relationship between the solver results and the selections type clear - solver results are selections plus diagnostics information.

The code discussed in this post can be found at https://github.com/0install/0install.

Hide
        
      
                    by Thomas Leonard at Sep 17, 2014 
      
      
    
  


       
                  Optimising the unikernel
      (Thomas Leonard)
    
    
                                After creating my REST queuing service as a Mirage unikernel, I reported that it could serve the data at 2.46 MB/s from my ARM CubieTruck dev board.
That’s fast enough for my use (it’s faster than my Internet connection), but I was curious why it was slower than the Linux guest, which serves files with nc at 20 MB/s.



( this post also appeared on Hacker News and Reddit )

Table of Contents

  The TCP test-case
  Compiler optimisations
  Profiling support
  Profiling the console
  Profiling UDP
  Profiling TCP
  Profiling disk access          Update: Linux is slow too!
    
  
  Profiling the queuing service
  Conclusions


The TCP test-case

To avoid confusing things by testing the disk and the network at the same time, I made a simpler test case that waits for a TCP connection
and transmits a pre-allocated buffer multiple times:

unikernel.ml 
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
module Main (C: V1_LWT.CONSOLE) (S: V1_LWT.STACKV4) = struct
  let buffe…Read more...After creating my REST queuing service as a Mirage unikernel, I reported that it could serve the data at 2.46 MB/s from my ARM CubieTruck dev board.
That’s fast enough for my use (it’s faster than my Internet connection), but I was curious why it was slower than the Linux guest, which serves files with nc at 20 MB/s.



( this post also appeared on Hacker News and Reddit )

Table of Contents

  The TCP test-case
  Compiler optimisations
  Profiling support
  Profiling the console
  Profiling UDP
  Profiling TCP
  Profiling disk access          Update: Linux is slow too!
    
  
  Profiling the queuing service
  Conclusions


The TCP test-case

To avoid confusing things by testing the disk and the network at the same time, I made a simpler test case that waits for a TCP connection
and transmits a pre-allocated buffer multiple times:

unikernel.ml 
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
module Main (C: V1_LWT.CONSOLE) (S: V1_LWT.STACKV4) = struct
  let buffer = Io_page.pages 16 |> List.map Io_page.to_cstruct
  let start c s =
    S.listen_tcpv4 s ~port:80 (fun flow ->
      let warmups = 10 in
      let iterations = 100 in
      let bytes = Cstruct.lenv buffer * iterations in
      let rec loop = function
        | 0 -> return ()
        | i ->
            lwt () = S.TCPV4.writev flow buffer in
            loop (i - 1) in
      loop warmups >>= fun () ->
      lwt time = Profile.time (fun () -> loop iterations) in
      S.TCPV4.close flow >>= fun () ->
      C.log_s c (Printf.sprintf "Wrote %d bytes in %.3f seconds (%.2f KB/s)"
        bytes time
        (float_of_int bytes /. time /. 1024.))
    );
    S.listen s
end


Profile.time just runs the function and returns how long it took in seconds.
I do a few warm-up iterations at the start because TCP starts slowly and we don’t want to benchmark that.

Compiler optimisations

While looking at the assembler output during some earlier debugging, I’d noticed that gcc was generating very poor code. For example:

0041a550 <_xmalloc>:
  41a550:	e92d4800 	push	{fp, lr}
  41a554:	e28db004 	add	fp, sp, #4
  41a558:	e24dd028 	sub	sp, sp, #40	; 0x28
  41a55c:	e50b0028 	str	r0, [fp, #-40]	; 0x28
  41a560:	e50b102c 	str	r1, [fp, #-44]	; 0x2c
  41a564:	e3a03000 	mov	r3, #0
  41a568:	e50b300c 	str	r3, [fp, #-12]
  41a56c:	e3a03010 	mov	r3, #16
  41a570:	e50b3014 	str	r3, [fp, #-20]
  41a574:	e51b002c 	ldr	r0, [fp, #-44]	; 0x2c
  41a578:	e3a01004 	mov	r1, #4
  41a57c:	ebffff5d 	bl	41a2f8 <align_up>
  41a580:	e50b002c 	str	r0, [fp, #-44]	; 0x2c
  41a584:	e51b002c 	ldr	r0, [fp, #-44]	; 0x2c
  ...


gcc is using registers very inefficiently here.
For example, it stores r1 to [fp, #-44] and then a few lines later loads from there into r0, when it could just have moved it directly.
The last two lines show it saving r0 to the stack and then immediately loading it back again into the same register!

The fix here turned out to be simple.
Mini-OS by default compiles in debug mode with no optimisations.
Compiling with debug=n fixes this, and I updated mirage-xen-minios to do this.

            Optimisations
      TCP download speed
    
  
            none
      6.92 MB/s
    
          -O3
      11.93 MB/s
    
  


Even though Mirage is almost all OCaml, it does use Mini-OS’s C functions for various low-level operations and these optimisations make a big difference!

Profiling support

The OCaml compiler provides a profiling option, which works the same way as gcc’s -pg option for C code.
To enable it, you add true: profile to your _tags file and rebuild.

I decided to see what would happen if I enabled this for my Xen unikernel:

_build/main.native.o: In function `caml_program':
:(.text+0x2): undefined reference to `__gnu_mcount_nc'


Profiling works by inserting a call to __gnu_mcount_nc at the start of every function.
It looks like this:

00000000 <caml_program>:
   0:       b500            push    {lr}
   2:       f7ff fffe       bl      0 <__gnu_mcount_nc>
   ...


The __gnu_mcount_nc function gets the address of the callee function (caml_program in this example) in the link register (lr/r14) and the address of its caller on the stack (pushed by the code fragment above).
Normally, the profiler would use this information to build up a static call graph (saying which functions call which other functions).
Using a regular timer interrupt to sample the program counter it can estimate how much time was spent in each function,
and using the call graph it can show cumulative totals (time spent in each function plus time spent in its children).

I decided to start with something a bit simpler.
I wrote some ARM code for __gnu_mcount_nc that simply writes the caller, callee and current time to a trace buffer (when the buffer is full, it stops tracing).
Ideally, I’d like to get notified each time we leave a function too.
gcc can do that for C code with its -finstrument-functions option, but I didn’t see an option for that in OCaml.
Instead, I assume that every function runs until I see a call whose caller is not its parent.
This works surprisingly well, though it does mean that if a function seems to take a long time you need to check its parents too,
and it might get confused for recursive calls.
Also, for tail calls, we see the parent as the function we will return to rather than the function that actually called us.

At the end, I dump out the trace buffer to the console with some OCaml code.
Back on my laptop, I wrote some code to parse this output and look up each address in the ELF image to get the function name for each address.
(This code isn’t public yet as it needs a lot of cleaning up.)

One thing I quickly discovered: compiling just the unikernel with profiling isn’t sufficient.
As soon as you call a non-profiled function it can no longer construct the call graph and the results are useless.
I manually recompiled every C and OCaml library I was using with profiling, which was quite tedious.

Update: Thomas Gazagnaire has added an OPAM profiling switch which should make this much easier in future.

Initially, the trace buffer filled up almost instantly with calls to stub_evtchn_test_and_clear.
It seems that we call this once for each of the 4096 channels every time we look for work.
To avoid clutter, I reduced the number of event channels to 10 (this had no noticeable effect on performance).
I also tried removing the memset which zeroes out newly allocated IO pages.
This also made no difference.

I measured the overhead added by the tracing, both when compiled in but inactive and when actively writing to the trace buffer:



So, not too bad.

Profiling the console

The TCP code’s trace was quite complicated, so I decided to start by profiling the much simpler console device,
which I’d noticed was surprisingly slow at dumping the trace results.

A Xen virtual console is a pair of ring buffers (one for input from the keyboard, one for output to the screen) in a shared memory page, defined like this:

console.h 
1
2
3
4
5
6
struct xencons_interface {
    char in[1024];
    char out[2048];
    XENCONS_RING_IDX in_cons, in_prod;
    XENCONS_RING_IDX out_cons, out_prod;
};


We’re only interested in the “out” side here.
The producer (i.e. our unikernel) writes the data to the buffer and advances the out_prod counter.
The consumer (xenconsoled, running in Dom0) reads the data and advances out_cons.
If the consumer catches up with the producer it sleeps until the producer notifies it there is more data.
If the producer catches up with the consumer (the buffer is full) it sleeps until the consumer notifies it there is space available again.

Here’s my console test-case - writing a string to the console in a loop:

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
module Main (C: V1_LWT.CONSOLE) = struct
  let start c =
    let len = 6 in
    let msg = String.make (len - 1) 'X' in
    let iterations = 10000 in
    let bytes = len * iterations in
    let rec loop = function
      | 0 -> return ()
      | i ->
          C.log_s c msg >>= fun () -> loop (i - 1) in
    loop 1000 >>= fun () ->  (* Warm up *)
    let t0 = Clock.time () in
    lwt () = loop iterations in
    let time = Clock.time () -. t0 in
    C.log_s c (Printf.sprintf "Wrote %d bytes in %.3f seconds (%.2f KB/s)"
      bytes time
      (float_of_int bytes /. time /. 1024.)) >>= fun () ->
    OS.Time.sleep 2.0
end


I got around 45 KB/s. The trace output looked like this:

            Start
      Function
      Duration
    
  
            118684
      - - camlUnikernel__loop_1270
      219
    
          118730
      - - > camlConsole__write_all_low_1123
      158
    
          118737
      - - > - camlRing__repeat_1279
      89
    
          118739
      - - > - - camlRing__write_1270
      87
    
          118830
      - - > - camlEventchn__fun_1138
      58
    
          118831
      - - > - - stub_evtchn_notify
      57
    
          118907
      - - camlUnikernel__loop_1270
      211
    
          118944
      - - > camlConsole__write_all_low_1123
      168
    
          118951
      - - > - camlRing__repeat_1279
      100
    
          118953
      - - > - - camlRing__write_1270
      98
    
          119054
      - - > - camlEventchn__fun_1138
      58
    
          119055
      - - > - - stub_evtchn_notify
      57
    
  


The start time and duration are measured in counter clock ticks, and the counter is running at 24 MHz.
The -->-->--> indicates the level of nesting (I vary the character to make it easier to scan vertically with the eye).
The output shows two iterations of the loop taken from the middle of the sample.
To make the output more readable, my analysis script prunes the tree at calls that took less than 50 ticks, and removes calls to the Lwt library (while still showing the functions they called as a result).
The durations include the times for their children (including pruned children).

You can see that on each iteration we call Console.write_all_low, which writes the string to the shared memory ring and notifies the console daemon in Dom0.
Each iteration is taking roughly 200 ticks, which is about 8 us per iteration.
So we’d expect the speed to be around 6 bytes / 8 us, which is about 700 KB/s.

Looking at the cumulative time spent in each function, the top entries are:

            Function
      Ticks (at 24 MHz)
    
  
            caml_c_call
      9002738
    
          caml_block_domain
      9001576
    
          block_domain
      9001462
    
          camlUnikernel__loop_1270
      374418
    
          camlConsole__write_all_low_1123
      298735
    
  


Note: the trace only includes calls until the trace buffer was full, so these aren’t the total times for the whole run.
But we can immediately see that we spent most of the time in block_domain, which is what Mirage calls when it has nothing to do and is waiting for an external event.
Here’s a graph showing how many iterations of the test loop we had started over time:



So, we wrote 679 messages very quickly, then waited a long time, then wrote 1027 more, then waited again, etc.
I thought there might be a bug in block_domain causing it to miss a wake-up event, so I limited the time it would spend blocking.
It didn’t make any difference; it would keep waking up, seeing that it had nothing to do, and going back to sleep again.

In case the problem was with Mirage’s implementation of the shared rings or console device,
I tried writing the same test directly in C in Mini-OS’s test.c and got the same result (I had to modify it slightly because by default Mini-OS’s console_print discards data when the buffer is full instead of waiting).
Finally, I tried it from a Linux guest and got 25 KB/s (interestingly, Linux uses 100% CPU while doing this).
The times were highly variable (each point on this plot is from writing the message 10,000 times and calculating the average):



After some investigation, it turned out that Xen was deliberately limiting the rate:

xen/tools/console/daemon/io.c 
1
2
3
4
/* How many events are allowed in each time period */
#define RATE_LIMIT_ALLOWANCE 30
/* Duration of each time period in ms */
#define RATE_LIMIT_PERIOD 200


Mystery solved, although I don’t know why the rates are so variable.
Mirage wasn’t doing anything except running the test case and Linux was booted with init=/bin/bash, so there was nothing else running there either.

Lessons:

  Just scrolling through the raw trace can be misleading. It appears that it keeps calling the loop function, but in fact almost all the time was spent in the two very rare block_domain calls. Graphing iterations over time can show these problems effectively.
  Compare with the speed on Linux. Sometimes, Xen really is that slow and it’s not our fault.
  Compile everything for profiling or the results aren’t much use.


Profiling UDP

TCP involves ack packets, expanding windows and other complications, so I next looked at the simpler UDP protocol.
Here, we can throw packets out continuously without worrying about the other end.

With a payload size of 1476 bytes (the maximum possible for UDP), I got 17 MB/s.
All packets were successfully received on my laptop.
My Linux guest got 13.4 MB/s with nc -u < /dev/zero, so we’re actually faster!

Here’s a sample iteration from the trace:

            Start
      Function
      Duration
    
  
            243418
      - - > - camlUnikernel__loop_1287
      938
    
          243436
      - - > - - camlUdpv4__fun_1430
      266
    
          243439
      - - > - - > camlIpv4__allocate_frame_1369
      190
    
          243440
      - - > - - > - camlIo_page__get_1122
      72
    
          243632
      - - > - - > camlIpv4__fun_1509
      69
    
          243818
      - - > - - camlNetif__fun_2893
      185
    
          243852
      - - > - - > camlNetif__fun_2618
      103
    
          243895
      - - > - - > - camlLwt_ring__fun_1223
      58
    
          244007
      - - > - - camlNetif__fun_2931
      132
    
          244009
      - - > - - > camlNetif__xmit_1509
      123
    
          244028
      - - > - - > - camlNetif__fun_2618
      76
    
          244141
      - - > - - camlNetif__fun_2958
      177
    
          244144
      - - > - - > camlRing__push_requests_and_check_notify_1112
      174
    
          244150
      - - > - - > - camlRing__sring_push_requests_1070
      158
    
          244151
      - - > - - > - - caml_memory_barrier
      77
    
          244231
      - - > - - > - - caml_memory_barrier
      77
    
  


Here’s a graph of loop iterations (packets sent) over time (each blue dot is one packet sent):



The gaps indicate places where we were not sending packets.
The garbage collector shows up twice in the trace (both times in Ring.ack_responses oddly).
However, we spend more time in block_domain than doing GC, indicating that we’re often waiting for Xen.
Looking at the trace just before it blocks, I see calls to Netif.wait_for_free_tx, which seems reasonable.

Profiling TCP

The TCP header is larger than the UDP one, making it less efficient even in the best case,
and TCP needs to process acks, keep track of window sizes, and handle retransmissions.
Strange, then, that the Linux guest manages 39 MB/s over TCP compared with just 13.4 MB/s for UDP!
(even stranger is that I got 47.2 MB/s for Linux when I tried it for last
month’s post; however I am using a different version of Linux in dom0 now)

I capture some packets sent by the Linux guest using tshark running in Dom0.
Loading it into Wireshark on my laptop, I see that all the TCP checksums are wrong, so it
looks like Linux is using TCP checksum offloading.

According to Question about TCP checksum offload in Xen:

  A domain has no way of knowing how any given packet is going to leave the host (or even if it is) so it can’t know ahead of time whether to calculate any checksums: the skb’s [socket buffers] are just marked with “checksum needed” as usual and either the egress NIC will do the job or dom0 will do it.


Getting this working on Mirage was a bit tricky.
The TCP layer can avoid adding the checksum only if the network device says it’s capable of doing it itself, and packets have to be flagged as needing the checksum.
You can’t just flag all packets because the Linux dom0 silently drops non-TCP/UDP packets with it set (e.g. ARP packets).
I hacked something together and got a modest speed improvement.

Here’s a graph for the TCP test, where each iteration of the loop is sending one TCP packet (segment):



Note: we send many warm up packets before starting the trace as TCP starts slowly (which looks pretty but isn’t relevant here).

Zooming in, the picture is quite interesting (where it had gaps, I searched for a typical function that occurred in the gap and added a symbol for it):



It looks like we start by transmitting packets steadily, until the current window is full.
Then we start buffering the packets instead of sending them, which is very fast.
At some point the TCP system stops accepting more data, which causes the main loop to block, allowing us to process other events.
rx_poll response indicates one iteration of the Netif.rx_poll loop, which seems to be dealing with acks from Xen saying that our packets have been transmitted (and the memory can therefore be recycled).
After a while, the TCP ack packets arrive and we process them, which opens up the transmit window again.
Then we send out the buffered packets, before returning to the main loop.

So, in each cycle we spend about 60% of the time transmitting packets, a quarter dealing with acks from Xen and the rest handling TCP acks from the remote host.
It might be possible to optimise things a bit here by reusing grant references, but I didn’t investigate further.

Profiling disk access

My next test case reads a series of sectors sequentially from the disk and then writes them. Reading or writing one sector (4096 bytes) at a time was very slow (2.7 MB/s read, 0.7 MB/s write).
Using larger buffers, so that we transfer more in each operation, helped but even at 64 sectors per op I only got 12.3 MB/s read / 5.12 MB/s write (the device is capable of 20 MB/s read and 10 MB/s write).
Here’s a trace where we read using 32-sector buffers (10.9 MB/s):



We spend a lot of time waiting for each block to arrive, although there are some curious ack messages, which we deal with quickly.
What if we have two requests in flight at once?
This gets us 18.27 MB/s:



Strangely, the two blocks arrive close together.
Although it takes us longer to get the first one (I don’t know why), we get them more quickly after that.
Having three requests in flight doesn’t help though (18.25 MB/s):



Looking at the block driver code, it batches requests into groups of 11. This probably explains why 32 sectors-per-read did well - it’s very close to 33.

For writing, the number of requests in flight makes little difference, but writing 8 sectors in each request is by far the best (7 MB/s).

I don’t understand why we’re not getting the full speed of the card here, since we’re spending most of the time blocking.
However, we are pretty close (18r/7w out of a possible 20r/10w), which is good enough for today.

Update: Linux is slow too!

I originally tested with hdparm, which reports about 20 MB/s as expected:

$ hdparm -t /dev/mmcblk0
 Timing buffered disk reads:  62 MB in  3.07 seconds =  20.21 MB/sec


But testing with dd, I don’t get this speed.
dd’s speed seems to depend a lot on the block size. Using 4096 * 11 bytes (which I assume is what dom0 would do in response to a single guest request), I get just 16.9 MB/s:

$ dd iflag=direct if=/dev/vg0/bench of=/dev/null bs=45056 count=1000
1000+0 records in
1000+0 records out
45056000 bytes (45 MB) copied, 2.65911 s, 16.9 MB/s


            Block size (pages)
      Linux dom0
      Linux domU
    
  
            11
      17.0 MB/s
      14.5 MB/s
    
          16
      18.8 MB/s
      16.3 MB/s
    
          32
      20.8 MB/s
      18.6 MB/s
    
  


So perhaps Mirage is doing pretty well already - it’s about as fast as the Linux guest.
Xen seems to be the limiting factor here, because it doesn’t allow us to make large enough requests.

Profiling the queuing service

Finally, I looked at applying all this new information to my queuing service.
As a baseline, wget reports that I can currently download from it at 4.6 MB/s, with profiling compiled in but disabled:



There’s some complicated copying going on because we’re using the HTTP Chunked encoding, which writes the size of each chunk of data followed by the data itself, then the next chunk, etc.
Since we know the length at the start, we can use the simpler Fixed encoding.
This increases the speed to 5.2 MB/s.
It’s a shame the HTTP API uses strings everywhere: we have to copy the data from the disk buffer to a string on the heap to give it to the HTTP API, which then copies it back into a new buffer to send it to the network card.
If it took a stream of buffers, we could just pass them straight through.

Finally, I added the read-ahead support from the block profiling above, which increased the speed to 6.8 MB/s.
Here’s the new graph, showing that we’re sending packets much faster (note the change in the Y-scale):



I used a queue length of 5, with 33 sectors per request. I tried increasing it to 10, but that caused more GC work.

Conclusions

Even the unoptimised service is faster than my current (ADSL) Internet connection, so optimising it isn’t currently necessary, but it’s interesting to look at performance and get a feel for where the bottlenecks are.

Mirage doesn’t have any specific profiling support, but the fact that the whole OS is a single executable makes profiling it quite easy.
OCaml’s profile option isn’t a perfect fit for tracing because it doesn’t record when a function finishes, but you can still get useful results from it.
Graphing some metric (e.g. packets sent) over time seemed the most useful way to look at the data.
I’m currently just using libreoffice’s chart tool, but I should probably find something more suitable.
It would be great to be able to zoom in easily, show durations (not just events), filter the trace display easily, etc.
I’d also like support for following Lwt threads even when they block.
Recommendations for good visualisation tools welcome!

Writing to the Xen console from Mirage is slow because xenconsoled rate limits us. Mirage still gets better performance than Linux though, and uses far less CPU (looks like Linux is just spinning). My UDP test kernel sent data faster than Linux’s nc utility (probably because nc made a poor choice of payload size). Linux does very well on TCP. I don’t know why it’s so fast. Using Xen’s TCP checksum offloading does help a bit though. SD card performance on Mirage is close to what the hardware supports when I choose the right request size and keep two requests in flight at once. It’s surprising we don’t manage the full speed, though. For networking and disk access, managing Xen’s grant refs for the shared memory pages seems to take up a lot of time - maybe there are ways to optimise that.

With a few modifications (TCP checksum offload, HTTP fixed encoding, keeping multiple disk reads in flight and using optimal buffer sizes), I increased the download speed of my test service running on my ARM dev board from 2.46 MB/s to 7.24 MB/s (when compiled without profiling).
I’m sure people more familiar with Mirage will have more suggestions.
Hide
        
      
                    by Thomas Leonard at Aug 15, 2014 
      
      
    
  


       
                  My first unikernel
      (Thomas Leonard)
    
    
                                I wanted to make a simple REST service for queuing file uploads, deployable as a virtual machine. The traditional way to do this is to download a Linux cloud image, install the software inside it, and deploy that. Instead I decided to try a unikernel.

Unikernels promise some interesting benefits. The Ubuntu 14.04 amd64-disk1.img cloud image is 243 MB unconfigured, while the unikernel ended up at just 5.2 MB (running the queue service). Ubuntu runs a large amount of C code in security-critical places, while the unikernel is almost entirely type-safe OCaml. And besides, trying new things is fun.



( this post also appeared on Reddit and Hacker News )

Table of Contents

  Introduction          A hello world kernel
      Using Mirage libraries
      The mirage-unix libraries
      The mirage tool
    
  
  Test case          Storage
      Implementation
      Unit-testing the storage system
      The HTTP server
      Buffered reads
      Streaming uploads
      Buffered writes
     …Read more...I wanted to make a simple REST service for queuing file uploads, deployable as a virtual machine. The traditional way to do this is to download a Linux cloud image, install the software inside it, and deploy that. Instead I decided to try a unikernel.

Unikernels promise some interesting benefits. The Ubuntu 14.04 amd64-disk1.img cloud image is 243 MB unconfigured, while the unikernel ended up at just 5.2 MB (running the queue service). Ubuntu runs a large amount of C code in security-critical places, while the unikernel is almost entirely type-safe OCaml. And besides, trying new things is fun.



( this post also appeared on Reddit and Hacker News )

Table of Contents

  Introduction          A hello world kernel
      Using Mirage libraries
      The mirage-unix libraries
      The mirage tool
    
  
  Test case          Storage
      Implementation
      Unit-testing the storage system
      The HTTP server
      Buffered reads
      Streaming uploads
      Buffered writes
      Upload speed on Xen
      TCP retransmissions
      Adding a block cache
      Replacing FAT
    
  
  Conclusions


Regular readers will know that a few months ago I began a new job at Cambridge University.
Working for an author of Real World OCaml and leader of OCaml Labs, on a project building pure-OCaml distributed systems, who found me through my blog posts about learning OCaml, I thought they might want me to write some OCaml.

But no.
They’ve actually had me porting the tiny Mini-OS kernel to ARM, using a mixture of C and assembler, to let the Mirage unikernel run on ARM devices.
Of course, I got curious and wanted to write a Mirage application for myself…

Introduction

Linux, like many popular operating systems, is a multi-user system.
This design dates back to the early days of computing, when a single expensive computer, running a single OS, would be shared between many users.
The goal of the kernel is to protect itself from its users, and to protect the users from each other.

Today, computers are cheap and many people own several.
Even when a physical computer is shared (e.g. in cloud computing), this is typically done by running multiple virtual machines, each serving a single user.
Here, protecting the OS from its (usually single) application is pointless.

Removing the security barrier between the kernel and the application greatly simplifies things;
we can run the whole system (kernel + application) as a single, privileged, executable - a unikernel.

And while we’re rewriting everything anyway, we might as well replace C with a modern memory safe language, eliminating whole classes of bugs and security vulnerabilities, allowing decent error reporting, and providing structured data types throughout.

In the past, two things have made writing a completely new OS impractical:

  Legacy applications won’t run on it.
  It probably won’t support your hardware.


Virtualisation removes both obstacles:
legacy applications can run in their own legacy VMs, and
drivers are only needed for the virtual devices - e.g.
a single network driver and a single block driver will cover all real network cards and hard drives.

A hello world kernel

The mirage tutorial starts by showing the easy, fully-automated way to build a unikernel.
If you want to get started quickly you may prefer to read that and skip this section, but since one of the advantages of unikernels is their relative simplicity, let’s do things the “hard” way first to understand how it works behind the scenes.

Here’s the normal “hello world” program in OCaml:

hw.ml 
1
2
let () =
  print_endline "Hello, world!"


To compile and run as a normal application, we’d do:

$ ocamlopt hw.ml -o hw
$ ./hw 
Hello, world!


How can we make a unikernel that does the equivalent?
As it turns out, the above code works unmodified (though the Mirage people might frown at you for doing it this way).
We compile hw.ml to a hw.native.o file and then link with the unikernel libraries instead of the standard C library:

$ export OPAM_DIR=$(opam config var prefix)
$ export PKG_CONFIG_PATH=$OPAM_DIR/lib/pkgconfig
$ ocamlopt -output-obj -o hw.native.o hw.ml
$ ld -d -static -nostdlib --start-group \
    $(pkg-config --static --libs openlibm libminios-xen) \
    hw.native.o \
    $OPAM_DIR/lib/mirage-xen/libocaml.a \
    $OPAM_DIR/lib/mirage-xen/libxencaml.a \
    --end-group \
    $(gcc -print-libgcc-file-name) \
    -o hw.xen


We now have a kernel image, hw.xen, which can be booted as a VM under the Xen hypervisor (as used by Amazon, Rackspace, etc to host VMs). But first, let’s look at the libraries we added:

  openlibm
  This is a standard maths library. It provides functions such as sin, cos, etc.
  libminios-xen
  This provides the architecture-specific boot code, a printk function for debugging, malloc for allocating memory and some low-level functions for talking to Xen.
  libocaml.a
  The OCaml runtime (the garbage collector, etc).
  libxencaml.a
  OCaml bindings for libminios and some boot code.
  libgcc.a
  Support functions for code that gcc generates (actually, not needed on x86).


To deploy the new unikernel, we create a Xen configuration file for it (here, I’m giving it 16 MB of RAM):

hw.xl 
1
2
3
4
5
name = 'hw'
kernel = 'hw.xen'
memory = 16
on_crash = 'preserve'
on_poweroff = 'preserve'


Setting on_crash and on_poweroff to preserve lets us see any output or errors, which would otherwise be missed if the VM exits too quickly.

We can now boot our new VM:

$ xl create -c hw.xl
Xen Minimal OS!
  start_info: 000000000009b000(VA)
    nr_pages: 0x800
  shared_inf: 0x6ee97000(MA)
     pt_base: 000000000009e000(VA)
nr_pt_frames: 0x5
    mfn_list: 0000000000097000(VA)
   mod_start: 0x0(VA)
     mod_len: 0
       flags: 0x0
    cmd_line: 
       stack: 0000000000055e00-0000000000075e00
Mirage: start_kernel
MM: Init
      _text: 0000000000000000(VA)
     _etext: 000000000003452d(VA)
   _erodata: 000000000003c000(VA)
     _edata: 000000000003e4d0(VA)
stack start: 0000000000055e00(VA)
       _end: 0000000000096d64(VA)
  start_pfn: a6
    max_pfn: 800
Mapping memory range 0x400000 - 0x800000
setting 0000000000000000-000000000003c000 readonly
skipped 0000000000001000
MM: Initialise page allocator for a8000(a8000)-800000(800000)
MM: done
Demand map pfns at 801000-2000801000.
Initialising timer interface
Initialising console ... done.
gnttab_table mapped at 0000000000801000.
xencaml: app_main_thread
getenv(OCAMLRUNPARAM) -> null
getenv(CAMLRUNPARAM) -> null
Unsupported function lseek called in Mini-OS kernel
Unsupported function lseek called in Mini-OS kernel
Unsupported function lseek called in Mini-OS kernel
Hello, world!
main returned 0


( Note: I’m testing locally by running Xen under VirtualBox. Not all of Xen’s features can be used in this mode, but it works for testing unikernels. I’m also using my Git version of mirage-xen; the official one will display an error after printing the greeting because it expects you to provide a mainloop too. The warnings about lseek are just OCaml trying to find the current file offsets for stdin, stdout and stderr.)

As you can see, the boot process is quite short.
Execution begins at _start.
Using objdump -d hw.xen, you can see that this just sets up the stack pointer register and calls the C function arch_init:

0000000000000000 <_start>:
       0:   fc                      cld    
       1:   48 8b 25 0f 00 00 00    mov    0xf(%rip),%rsp        # 17 <stack_start>
       8:   48 81 e4 00 00 ff ff    and    $0xffffffffffff0000,%rsp
       f:   48 89 f7                mov    %rsi,%rdi
      12:   e8 e2 bb 00 00          callq  bbf9 <arch_init>


arch_init (in libminios) initialises the traps and FPU and then prints Xen Minimal OS! and information about various addresses.
It then calls start_kernel.

start_kernel (in libxencaml) sets up a few more features (events, interrupts, malloc, time-keeping and grant tables), then calls caml_startup.

caml_startup (in libocaml) initialises the garbage collector and calls caml_program, which is our hw.native.o.

We call print_endline, which libxencaml, as a convenience for debugging, forwards to libminios’s console_print.

Using Mirage libraries

The above was a bit of a hack, which ended up just using the C console driver in libminios (one of the few things it provides, as it’s needed for printk).
We can instead use the mirage-console-xen OCaml library, like this:

hw.ml 
1
2
3
4
5
6
7
8
9
10
open Lwt
let main =
  Console.connect "0" >>= function
  | `Error _ -> failwith "Failed to connect to console"
  | `Ok default_console ->
      Console.log_s default_console "Hello, world!"
let () =
  OS.Main.run main


Mirage uses the usual Lwt library for cooperative threading, which I wrote about at last year in Asynchronous Python vs OCaml - >>= means to wait for the result, allowing other code to run. Everything in Mirage is non-blocking, even looking up the console. OS.Main.run runs the main event loop.

Since we’re using libraries, let’s switch to ocamlbuild and give the dependencies in the _tags file, as usual for OCaml projects:

_tags 
1
true: warn(A), strict_sequence, package(mirage-console-xen)

The only unusual thing we have to do here is tell ocamlbuild not to link in the Unix module when we build hw.native.o:

$ ocamlbuild -lflags -linkpkg,-dontlink,unix -use-ocamlfind hw.native.o


In the same way, we can use other libraries to access raw block devices (mirage-block-xen), timers (mirage-clock-xen) and network interfaces (mirage-net-xen).
Other (non-Xen-specific) OCaml libraries can then be used on top of these low-level drivers.
For example, fat-filesystem can provide a filesystem on a block device, while tcpip provides an OCaml TCP/IP stack on a network interface.

The mirage-unix libraries

You may have noticed that the Xen driver libraries we used above ended in -xen.
In fact, each of these is just an implementation of some generic interface provided by Mirage.
For example, mirage/types defines the abstract CONSOLE interface as:

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
module type CONSOLE = sig
  (** Text console input/output operations. *)
  type error = [
    | `Invalid_console of string
  ]
  (** The type representing possible errors when attaching a console. *)
  include DEVICE with
    type error := error
  val write : t -> string -> int -> int -> int
  (** [write t buf off len] writes up to [len] chars of [String.sub buf
      off len] to the console [t] and returns the number of bytes
      written. Raises {!Invalid_argument} if [len > buf - off]. *)
  val write_all : t -> string -> int -> int -> unit io
  (** [write_all t buf off len] is a thread that writes [String.sub buf
      off len] to the console [t] and returns when done. Raises
      {!Invalid_argument} if [len > buf - off]. *)
  val log : t -> string -> unit
  (** [log str] writes as much characters of [str] that can be written
      in one write operation to the console [t], then writes
      "\r\n" to it. *)
  val log_s : t -> string -> unit io
  (** [log_s str] is a thread that writes [str ^ "\r\n"] in the
      console [t]. *)
end


By linking against the -unix versions of libraries rather than the -xen ones, we can compile our code as an ordinary Unix program and run it directly.
This makes testing and debugging very easy.

To make sure our code is generic enough to do this, we can wrap it in a functor that takes any console module as an input:

unikernel.ml 
1
2
3
4
module Main (C : V1_LWT.CONSOLE) = struct
  let start c =
    C.log_s c "Hello, world!"
end


The code that provides a Xen or Unix console and calls this goes in main.ml:

main.ml 
1
2
3
4
5
6
7
8
9
10
11
open Lwt
let console =
  Console.connect "0" >>= function
  | `Error _ -> failwith "Failed to connect to console"
  | `Ok c -> return c
module U = Unikernel.Main(Console)
let () =
  OS.Main.run (console >>= U.start)


The mirage tool

With the platform-specific code isolated in main.ml, we can now use the mirage command-line tool to generate it automatically for the target platform.
mirage takes a config.ml configuration file and generates Makefile and main.ml based on the current platform and the arguments passed.

config.ml 
1
2
3
4
5
6
7
8
open Mirage
let main = foreign "Unikernel.Main" (console @-> job)
let () =
  register "hw" [
    main $ default_console
  ]


$ mirage configure --unix
$ make
$ ./mir-hw 
Hello, world!


I won’t describe this in detail because at this point we’ve reached the start of the official tutorial, and you can read that instead.

Test case

Because 0install is decentralised, it doesn’t need a single centrally-managed repository (or several incompatible repositories, each trying to package every program, as is common with Linux distributions).
In 0install, it’s possible for every developer to run their own repository, containing just their software, with cross-repository dependencies handled automatically.
But just because it’s possible doesn’t mean we have to go to that extreme: having medium sized repositories each managed by a team of people can be very convenient, especially where package maintainers come and go.

The general pattern for a group repository is to have a public server that accepts new package uploads from developers, and a private (firewalled) server with the repository’s GPG key, which downloads from it:



Debian uses an anonymous FTP server for its incoming queue, polling it with a cron job.
This turns out to be surprisingly complicated.
You need to handle incomplete uploads (not processing them until they’re done, or deleting them eventually if they never complete), allow contributors to overwrite or delete their own partial uploads (Debian allows you to upload a GPG-signed command file, which provides some control), etc, as well as keep the service fully patched.
Also, the cron system can be annoying: if the package contains a mistake then it will be several minutes before it discovers this and emails the packager.

Perhaps there are some decent systems out there to handle all this, but it seemed like a good opportunity to try making a unikernel.

A particularly nice feature of this test-case is that it doesn’t matter too much if it fails:
the repository itself will check the developer’s signature on the files, so an attacker can’t compromise the repository by breaking into the queue; everything in the queue is intended to become public, so we need not worry much about confidentiality; lost uploads can be easily resubmitted; and if it goes down for a bit, it just means that new software can’t be added to the repository.
So, there’s nothing critical about this service, which is reassuring.

Storage

The merge-queues library builds a queue abstraction on top of
Irmin, a Git-inspired storage system for Mirage.
But my needs are simple, and I wanted to test the more primitive libraries first, so I decided to build my queue directly on a plain filesystem.
This was the first interface I came up with:

upload_queue.mli 
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
module type FS = V1_LWT.FS with
  type page_aligned_buffer = Cstruct.t and
  type block_device_error = Fat.Fs.block_error
(** An upload.
 * To avoid loading complete uploads into RAM, we stream them
 * between the network and the disk. *)
type item = {
  size : int64;
  data : string Lwt_stream.t;
}
type add_error = [`Wrong_size of int64 | `Unknown of exn]
module Make : functor (F : FS) -> sig
  (** An upload queue. *)
  type t
  (** Create a new queue, backed by a filesystem. *)
  val create : F.t -> t Lwt.t
  module Upload : sig
    (** Add an upload to the queue.
     * The upload is added only once the end of the stream is
     * reached, and only if the total size matches the size
     * in the record.
     * To cancel an add, just terminate the stream. *)
    val add :
      t -> item -> [ `Ok of unit | `Error of add_error ] Lwt.t
  end
  module Download : sig
    (** Interface for the repository software to fetch items
     * from the queue. Only one client may use this interface
     * at a time, or things will go wrong. *)
    (** Return a fresh stream for the item at the head of the
     * queue, without removing it. After downloading it
     * successfully, the client should call [delete]. If the
     * queue is empty, this blocks until an item is available. *)
    val peek : t -> item Lwt.t
    (** Delete the item previously retrieved by [peek].
     * If the previous item has already been deleted, this does
     * nothing, even if there are more items in the queue. *)
    val delete : t -> unit Lwt.t
  end
end


Our unikernel.ml will use this to make a queue, backed by a filesystem.
Uploaders’ HTTP POSTs will be routed to Upload.add, while the repository’s GET and DELETE invocations go to the Download submodule.
delete is a separate operation because we want the repository to confirm that it got the item successfully before we delete it, in case of network errors.

Ideally, we might require that the DELETE comes over the same HTTP connection as the GET just in case we accidentally run two instances of the repository software, but that’s unlikely and it’s convenient to test using separate curl invocations.

We’re using another functor here, Upload_queue.Make, so that our queue will work over any filesystem.
In theory, we can configure our unikernel with a FAT filesystem on a block device when running under Xen,
while using a regular directory when running under Linux (e.g. for testing).

But it doesn’t work.
You can see at the top that I had to restrict Mirage’s abstract FS type in two ways:

      The read and write functions in FS pass the data using the abstract page_aligned_buffer type.
Since we need to do something with the data, this isn’t good enough.
I therefore declare that this must be a Cstruct.t (basically, an array of bytes).
This is actually OK; mirage-fs-unix also uses this type.
  
      One of the possible error codes from FS is the abstract type FS.block_device_error, and I can’t
see any way to turn one of these into a string using the FS interface.
I therefore require a filesystem implementation that defines it to be Fat.Fs.block_error.
Obviously, this means we now only support the FAT filesystem.
  


This doesn’t prevent us from running as a normal process, because we can ask for a Unix “block” device (actually, just a plain disk.img file) and pass that to the Fat module, but it would be nice to have the option of using a real directory.

I asked about this on the mailing list - Mirage questions from writing a REST service - and it looks like the FS type will change soon.

Implementation

For the curious, this initial implementation is in upload_queue.ml.

Internally, the module creates an in-memory queue to keep track of successful uploads.
Uploads are streamed to the disk and
when an upload completes with the declared size, the filename is added to the queue.
If the upload ends with the wrong size (probably because the connection was lost), the file is deleted.

But what if our VM gets rebooted?
We need to scan the file system at start up and work out which uploads are complete and which should be deleted.
My first thought was to name the files NUMBER.part during the upload and rename on success.
However, the FS interface currently lacks a rename method.
Instead, I write an N byte to the start of each file and set it to Y on success.
That works, but renaming would be nicer!

For downloading, the peek function returns the item at the head of the queue.
If the queue is empty, it waits until something arrives.
The repository just makes a GET request - if something is available then it returns immediately,
otherwise the connection stays open until some data is ready, allowing the repository to respond immediately to new uploads.

Unit-testing the storage system

Because our unikernel can run as a process, testing is easy even if you don’t have a local Xen deployment.
A set of unit-tests test the upload queue module just as for any other program, and the service can be run as a normal process, listening on a normal TCP socket.
A slight annoyance here is that the generated Makefile doesn’t include any rules to build the tests so you have to add them manually, and
if you regenerate the Makefile then it loses the new rule.

As you might expect from such a new system, testing uncovered several problems. The first (minor) problem is that when the disk becomes full, the unhelpful error reported by the filesystem is Failure("Unknown error: Failure(\"fault\")").

( I asked about this on the mailing list - Error handling in Mirage - and there seems to be agreement that error handling should change. )

A more serious problem was that deleting files corrupted the FAT directory index.
I downloaded the FAT library and added a unit-test for delete, which made it easy to track the problem down (despite my lack of knowledge of FAT).
Here’s the code for marking a directory entry as deleted in the FAT library:

1
2
3
4
5
6
7
8
9
10
11
12
    let b = Cstruct.sub block offset sizeof in
    let delta = Cstruct.create sizeof in
    begin match unmarshal b with
      | Lfn lfn ->
	let lfn' = { lfn with lfn_deleted = true } in
	marshal delta (Lfn lfn')
      | Dos dos ->
	let dos' = { dos with deleted = true } in
	marshal b (Dos dos')
      | End -> assert false
    end;
    Update.from_cstruct (Int64.of_int offset) delta :: acc


It’s supposed to take an entry, unmarshal it into an OCaml structure, set the deleted flag, and marshal the result into a new delta structure.
These deltas are returned and applied to the device.
The bug is a simple typo: Lfn (long filename) entries update correctly, but for old Dos ones it writes the new block to the input, not to delta.
The fix was simple enough (I also refactored it slightly to encourage the correct behaviour in future):

1
2
3
4
5
6
7
8
    let b = Cstruct.sub block offset sizeof in
    let delta = Cstruct.create sizeof in
    marshal delta begin match unmarshal b with
      | Lfn lfn -> Lfn { lfn with lfn_deleted = true }
      | Dos dos -> Dos { dos with deleted = true }
      | End -> assert false
    end;
    Update.from_cstruct (Int64.of_int offset) delta :: acc


This demonstrates both the good and the bad of Mirage: the bug was easy to find and fix, using regular debugging tools.
I’m sure fixing a filesystem corruption bug in the Linux kernel would have been vastly more difficult.
On the other hard, Linux is rather well tested, whereas I appear to be the first person ever to try deleting a file in Mirage!

The HTTP server

This turned out to be quite simple. Here’s the unikernel’s start function:

unikernel.ml 
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
module Main (C : V1_LWT.CONSOLE)
            (F : Upload_queue.FS)
            (H : Cohttp_lwt.Server) = struct
  module Q = Upload_queue.Make(F)
  [...]
  let start c fs http =
    Log.write := C.log_s c;
    Log.info "starting queue service" >>= fun () ->
    Q.create fs >>= fun q ->
    let callback _conn_id request body =
      match Uri.path request.H.Request.uri with
      | "/uploader" -> handle_uploader q request body
      | "/downloader" -> handle_downloader q request
      | path ->
          H.respond_error
	    ~status:`Bad_request
      	    ~body:(Printf.sprintf "Bad path '%s'\n" path)
	    () in
    let conn_closed _conn_id () =
      Log.info "connection closed" |> ignore in
    http { H.
      callback;
      conn_closed
    }
end


Here, our functor is extended to take a filesystem (using the restricted type required by our Upload_queue, as noted above) and an HTTP server module as arguments.

The HTTP server calls our callback each time it receives a request, and this dispatches /uploader requests to handle_uploader and /downloader ones to handle_downloader. These are also very simple, e.g.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
  let get q =
    Q.Download.peek q >>= fun {Upload_queue.size; data} ->
    let body = Cohttp_lwt_body.of_stream data in
    let headers = Cohttp.Header.init_with
      "Content-Length" (Int64.to_string size) in
    (* Adding a content-length loses the transfer-encoding
     * for some reason, so add it back: *)
    let headers = Cohttp.Header.add headers
      "transfer-encoding" "chunked" in
    H.respond ~headers ~status:`OK ~body ()
  let handle_downloader q request =
    match H.Request.meth request with
    | `GET -> get q
    | `DELETE -> delete q
    | `HEAD | `PUT | `POST
    | `OPTIONS | `PATCH -> unsupported_method


The other methods (put and delete) are similar.

Buffered reads

Running as a --unix process, I initially got a download speed of
17.2 KB/s, which was rather disappointing.
Especially as Apache on the same machine gets 615 MB/s!

Increasing the size of the chunks I was reading from the Fat
filesystem (a disk.img file) from 512 bytes to 1MB, I was able to
increase this to 2.83 MB/s, and removing the O_DIRECT flag from
mirage-block-unix, download speed increased to 15 MB/s (so this is
with Linux caching the data in RAM).

To check the filesystem was the problem, I removed the F.read call
(so it would return uninitialised data instead of the actual file contents).
It then managed a very respectable 514 MB/s.
Nothing wrong with the HTTP code then.

Streaming uploads

It all worked nicely running as a Unix process, so the next step was to deploy on Xen.
I was hoping that most of the bugs would already have been found during the Unix testing,
but in fact there were more lurking.

It worked for very small files, but when uploading larger files it quickly ran
out of memory on my 64-bit x86 test system. I also tried it on my 32-bit CubieTruck
ARM board, but that failed even sooner, with Invalid_argument("String.create") (on 32-bit
platforms, OCaml strings are limited to 16 MB).

In both cases, the problem was that the cohttp library tried to read the entire upload in one go.
I found the read function in Transfer_io:

1
2
3
4
5
6
7
8
let read ~len ic =
  (* TODO functorise string to a bigbuffer *)
  match len with
  |0 -> return Done
  |len ->
    read_exactly ic len >>= function
    |None -> return Done
    |Some buf -> return (Final_chunk buf)


I changed it to use read rather than read_exactly (read returns whatever data is available, waiting only if there isn’t any at all):

1
2
3
4
5
6
7
8
9
let read ~remaining ic =
  (* TODO functorise string to a bigbuffer *)
  match !remaining with
  |0 -> return Done
  |len ->
    read ic len >>= fun buf ->
    remaining := !remaining - String.length buf;
    if !remaining = 0 then return (Final_chunk buf)
    else return (Chunk buf)


I also had to change the signature to take a mutable reference (remaining) for the remaining data, otherwise it has no way to know when it’s done (patch).

Buffered writes

With the uploads now split into chunks, upload speed with --unix was 178 KB/s.
Batching up the chunks (which were generally 4 KB each) into a 64 KB buffer increased the speed to 2083 KB/s.
With a 1 MB buffer, I got 6386 KB/s.

Here’s the code I used:

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
let page_buffer = Io_page.get 256 |> Io_page.to_cstruct in
(* Set the first byte to N to indicate that we're not done yet.
 * If we reboot while this flag is set, the partial upload will
 * be deleted. *)
let page_buffer_used = ref 1 in
Cstruct.set_char page_buffer 0 'N';
let file_offset = ref 1 in
let flush_page_buffer () =
  Log.info "Flushing %d bytes to disk" !page_buffer_used >>= fun () ->
  let buffered_data = Cstruct.sub page_buffer 0 !page_buffer_used in
  F.write q.fs name !file_offset buffered_data >>|= fun () ->
  file_offset := !file_offset + !page_buffer_used;
  page_buffer_used := 0;
  return () in
let rec add_data src i =
  let src_remaining = String.length src - i in
  if src_remaining = 0 then return ()
  else (
    let page_buffer_free = Cstruct.len page_buffer - !page_buffer_used in
    let chunk_size = min page_buffer_free src_remaining in
    Cstruct.blit_from_string src i page_buffer !page_buffer_used chunk_size;
    page_buffer_used := !page_buffer_used + chunk_size;
    lwt () =
      if page_buffer_free = chunk_size then flush_page_buffer ()
      else return () in
    add_data src (i + chunk_size)
  ) in
data |> Lwt_stream.iter_s (fun data -> add_data data 0) >>=
flush_page_buffer


Asking on the mailing list confirmed that
Fat is not well optimised.
This isn’t actually a problem for my service, since it’s still faster than
my Internet connection, but there’s clearly more work needed here.

Upload speed on Xen

Testing on my little CubieTruck board, I then got:

            Upload speed
      74 KB/s
    
          Download speed
      1.6 KB/s
    
  


Hmm. To get a feel for what the board is capable of, I ran nc -l -p 8080 < /dev/zero on the board
and nc cubietruck 8080 | pv > /dev/null on my laptop, getting 29 MB/s.

Still, my unikernel is running as a guest, meaning it has the overhead of using the virtual network
interface (it has to pass the data to dom0, which then sends it over the real interface). So I installed
a Linux guest and tried from there. 47.2 MB/s. Interesting. I have no idea why it’s faster than dom0!

I loaded up Wireshark to see what was happening with the unikernel transfers.
The upload transfer mostly went fast, but stalled in the middle for 15 seconds and then for 12 seconds at the end.
Wireshark showed that the unikernel was ack’ing the packets but reducing the TCP window size, indicating that the packets weren’t being processed by the application code.
The delays corresponded to the times when we were flushing the data to the SD card, which makes sense.
So, this looks like another filesystem problem (we should be able to write to the SD card much faster than this).

TCP retransmissions

For the download, Wireshark showed that many of the packets had incorrect TCP checksums and were having to be retransmitted.
I was already familiar with this bug from a previous mailing list discussion: wireshark capture of failed download from mirage-www on ARM.
That turned out be a Linux bug - the privileged dom0 code responsible for sending our virtual network packets to the real network becomes confused if two packets occupy the same physical page in memory.

Here’s what happens:

  We read 1 MB of data from the disk and send it to the HTTP layer as the next chunk.
  Chunked.write does the HTTP chunking and sends it to the TCP/IP channel.
  Channel.write_string writes the HTTP output into pages (aligned 4K blocks of memory).
  Pcb.writefn then determines that each page is too big for a TCP packet and splits each one into smaller chunks, sharing the single underlying page:


1
2
3
4
5
6
7
8
9
10
11
12
13
  let rec writefn pcb wfn data =
    let len = Cstruct.len data in
    match write_available pcb with
    | 0 ->
      write_wait_for pcb 1 >>
      writefn pcb wfn data
    | av_len when av_len < len ->
      let first_bit = Cstruct.sub data 0 av_len in
      let remaing_bit = Cstruct.sub data av_len (len - av_len) in
      writefn pcb wfn first_bit  >>
      writefn pcb wfn remaing_bit
    | av_len ->
      wfn [data]


My original fix changed mirage-net-xen to wait until the first buffer had been read before sending the second one.
That fixed the retransmissions, but all the waiting meant I still only got 56 KB/s.
Instead, I changed writefn to copy remaining_bit into a new IO page, and with that I got 495 KB/s.

Replacing the filesystem read with a simple String.create of the same length, I got 3.9 MB/s, showing that once again the
FAT filesystem was now the limiting factor.

Adding a block cache

I tried adding a block cache layer between mirage-block-xen and fat-filesystem, like this:

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
module Main (C : V1_LWT.CONSOLE)
            (B : V1_LWT.BLOCK)
	    (H : Cohttp_lwt.Server) = struct
  module BC = Block_cache.Make(B)
  module F = Fat.Fs.Make(BC)(Io_page)
  module Q = Upload_queue.Make(F)
  let mem_cache_size = 1024 * 1024	(* 1 MB *)
  let start c b http =
    Log.write := C.log_s c;
    Log.info "start in queue service" >>= fun () ->
    BC.connect (b, mem_cache_size) >>= function
    | `Error _ -> failwith "BC.connect"
    | `Ok bc ->
    F.connect bc >>= function
    | `Error _ -> failwith "F.connect"
    | `Ok fs ->
    Q.create fs >>= fun q ->
...


With this in place, upload speed remains at 76 KB/s, but the download speed increases to 1 MB/s (for a 20 MB file, which therefore doesn’t fit in the cache).
This suggests that the FAT filesystem is reading the same disk sectors many times.
Enlarging the memory cache to cover the whole file, the download speed only increases to 1.3 MB/s,
so the FAT code must be doing some inefficient calculations too.

Replacing FAT

Since most of my problems seemed to be coming from using FAT, I decided to try a new approach.
I removed all the FAT code and the block cache and changed upload_queue.ml to write directly to the block
device.
With that (no caching), I get:

            Upload speed
      2.27 MB/s
    
          Download speed
      2.46 MB/s
    
  


That’s not too bad. It’s faster than my Internet connection, which means that the unikernel is no longer the limiting factor.

Here’s the new version: upload_queue.ml.
The big simplification comes from knowing that the queue will spend most of its time empty (another good reason to use a small VM for it).
The code has a next_free_sector which it advances every time an upload starts.
When the queue becomes empty and there are no uploads in progress this variable is reset back to sector 1 (sector 0 holds the index).
This does mean that we may report disk full errors to uploaders even when there is free space on the disk, but this won’t happen in typical usage because the repository downloads things as soon as they’re uploaded (if it does happen, it just means uploaders have to wait a couple of minutes until the repository empties the queue).

Managing the block device manually brought a few more advantages over FAT:

  No need to generate random file names for the uploads.
  No need to delete incomplete uploads (we only write the file’s index entry to disk on success).
  The system should recover automatically from filesystem corruption because invalid entries can be detected reliably at boot time and discarded.
  Disk full errors are reported correctly.
  The queue ordering isn’t lost on reboot.


Conclusions

Modern operating systems are often extremely complex, but much of this is historical baggage which isn’t needed on a modern system where you’re running a single application as a VM under a hypervisor. Mirage allows you to create very small VMs which contain almost no C code. These VMs should be easier to write, more reliable and more secure.

Creating a bootable OCaml kernel is surprisingly easy, and from there adding support for extra devices is just a matter of pulling in the appropriate libraries. By programming against generic interfaces, you can create code that runs under Linux/Unix/OS X or as a virtual machine under Xen, and switch between configurations using the mirage tool.

Mirage is still very young, and I found many rough edges while writing my queuing service for 0install:

  While Linux provides fast, reliable filesystems as standard, Mirage currently only provides a basic FAT implementation.
  Linux provides caching as standard, while you have to implement this yourself on Mirage.
  Error reporting should be a big improvement over C’s error codes, but getting friendly error messages from Mirage is currently difficult.
  The system has clearly been designed for high performance (the APIs generally write to user-provided buffers to avoid copying, much like C libraries do), but many areas have not yet been optimised.
  Buffers often have extra requirements (e.g. must be page-aligned, a single page, immutable, etc) which are not currently captured in the type system, and this can lead to run-time errors which would ideally have been detected at compile time.


However, there is a huge amount of work happening on Mirage right now and it looks like all of these problems are being worked on.
If you’re interested in low-level OS programming and don’t want to mess about with C, Mirage is a lot of fun, and it can be useful for practical tasks already with a bit of effort.

There are still many areas I need to find out more about.
In particular, using the new pure-OCaml TLS stack to secure the system and trying the Irmin Git-like distributed, branchable storage to provide the queue instead of writing it myself.
I hope to try those soon…
Hide
        
      
                    by Thomas Leonard at Jul 28, 2014 
      
      
    
  


       
                  Release of  “ARC: Analysis of Raft Consensus”
      (Heidi Howard)
    
    
                                 “ARC: Analysis of Raft Consensus” is now available online as a UCAM technical report. http://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-857.pdf
Abstract
The Paxos algorithm, despite being synonymous with distributed consensus for a decade, is famously difficult to reason about and implement due to its non-intuitive approach and underspecification. In response, this project implemented and evaluated a framework for constructing fault-tolerant applications, utilising the recently proposed Raft algorithm for distributed consensus. Constructing a simulation framework for our implementation enabled us to evaluate the protocol on everything from understandability and efficiency to correctness and performance in diverse network environments. We propose a range of optimisations to the protocol and released to the community a testbed for developing further optimisations and investigating optimal protocol parameters for real-world deployments.
Thank you everyone for your feedback.

        
      
                    by Heidi Howard at Jul 25, 2014 
      
      
    
  


       
                  Seventh OCaml compiler hacking session (at Citrix)
      (Compiler Hacking)
    
    
                                For the seventh Cambridge OCaml compiler-hacking session we'll be meeting at the Citrix office in the Cambridge Science Park on 6.30pm Friday 1st August.  Thanks to Citrix for supporting and hosting the session!

We'll kick off with a demo from Frédéric Bour of modular implicits, an OCaml extension that adds support for overloading.

If you're planning to come along, it'd be helpful if you could indicate interest via Doodle and sign up to the mailing list to receive updates:

Where:
  Citrix Systems Research & Development Ltd.  Building 101  Cambridge Science Park  Milton Road  Cambridge, CB4 0FY  United Kingdom  

When: 6.30pm, Friday 1st August

Who: anyone interested in improving OCaml. Knowledge of OCaml programming will obviously be helpful, but prior experience of working on OCaml internals isn't necessary.

What: fixing bugs, implementing new features, learning about OCaml internals

Wiki: https://github.com/ocamllabs/compiler-hacking/wiki

We're defining "compiler" prett…Read more...For the seventh Cambridge OCaml compiler-hacking session we'll be meeting at the Citrix office in the Cambridge Science Park on 6.30pm Friday 1st August.  Thanks to Citrix for supporting and hosting the session!

We'll kick off with a demo from Frédéric Bour of modular implicits, an OCaml extension that adds support for overloading.

If you're planning to come along, it'd be helpful if you could indicate interest via Doodle and sign up to the mailing list to receive updates:

Where:
  Citrix Systems Research & Development Ltd.  Building 101  Cambridge Science Park  Milton Road  Cambridge, CB4 0FY  United Kingdom  

When: 6.30pm, Friday 1st August

Who: anyone interested in improving OCaml. Knowledge of OCaml programming will obviously be helpful, but prior experience of working on OCaml internals isn't necessary.

What: fixing bugs, implementing new features, learning about OCaml internals

Wiki: https://github.com/ocamllabs/compiler-hacking/wiki

We're defining "compiler" pretty broadly, to include anything that's part of the standard distribution, which means at least the standard library, runtime, tools (ocamldep, ocamllex, ocamlyacc, etc.), ocamlbuild, the documentation, and the compiler itself. We'll have suggestions for mini-projects for various levels of experience (see also some things we've worked on in previous sessions), but feel free to come along and work on whatever you fancy.

We'll also be ordering pizza, so if you want to be counted for food you should aim to arrive by 6.45pm.
Hide
        
      
                    by Compiler Hacking at Jul 24, 2014 
      
      
    
  


       
                  MirageOS v2.0: a recap of the new features
      (Mirage OS)
    
    
                                       This work funded in part by the EU FP7 User-Centric Networking project, Grant
 No. 611001.

The first release of MirageOS back in December 2013 introduced the prototype
of the unikernel concept, which realised the promise of a safe,
flexible mechanism to build highly optimized software stacks purpose-built for deployment in the public cloud (more background on this).
Since then, we've been hard at work using and extending MirageOS for real projects and the community has been
steadily growing.
We're thrilled to announce the release of MirageOS v2.0 today!  Over the past
few weeks the team has been hard at work blogging about all
the new features in this latest release, coordinated by the tireless Amir Chaudhry:

ARM device support: While the first version of MirageOS was specialised towards conventional x86 clouds, the code generation and boot libraries have now been made portable enough to operate on low-power embedded ARM devices such as the Cubieboard 2.  This is a key part …Read more...       This work funded in part by the EU FP7 User-Centric Networking project, Grant
 No. 611001.

The first release of MirageOS back in December 2013 introduced the prototype
of the unikernel concept, which realised the promise of a safe,
flexible mechanism to build highly optimized software stacks purpose-built for deployment in the public cloud (more background on this).
Since then, we've been hard at work using and extending MirageOS for real projects and the community has been
steadily growing.
We're thrilled to announce the release of MirageOS v2.0 today!  Over the past
few weeks the team has been hard at work blogging about all
the new features in this latest release, coordinated by the tireless Amir Chaudhry:

ARM device support: While the first version of MirageOS was specialised towards conventional x86 clouds, the code generation and boot libraries have now been made portable enough to operate on low-power embedded ARM devices such as the Cubieboard 2.  This is a key part of our efforts to build a safe, unified mutiscale programming model for both cloud and mobile workloads as part of the Nymote project.  We also upstreamed the changes required to the Xen Project so that other unikernel efforts such as HalVM or ClickOS can benefit."Introducing an ARMy of unikernels" by Thomas Leonard talks about the changes required and instructions for trying this out for yourself on your own cheap Cubieboard.Irmin distributed, branchable storage: Unikernels usually execute in a distributed, disconnection-prone environment (particularly with the new mobile ARM support).  We therefore built the Irmin library to explicitly make synchronization easier via a Git-like persistence model that can be used to build and easily trace the operation of distributed applications across all of these diverse environments."Introducing Irmin: Git-like distributed, branchable storage" by Thomas Gazagnaire describes the concepts and high-level architecture of the system."Using Irmin to add fault-tolerance to the Xenstore database" by Dave Scott shows how Irmin is used in a real-world application: the security-critical Xen toolstack that manages hosts full of virtual machines (video).OCaml TLS: The philosophy of MirageOS is to construct the entire operating system in a safe programming style, from the device drivers up.  This continues in this release with a comprehensive OCaml implementation of Transport Level Security, the most widely deployed end-to-end encryption protocol on the Internet (and one that is very prone to bad security holes).  The blog series is written by Hannes Mehnert and David Kaloper."OCaml-TLS: Introducing transport layer security (TLS) in pure OCaml" presents the motivation and architecture behind our clean-slate implementation of the protocol."OCaml-TLS: building the nocrypto library core" talks about the cryptographic primitives that form the heart of TLS confidentiality guarantees, and how they expose safe interfaces to the rest of the stack."OCaml-TLS: adventures in X.509 certificate parsing and validation" explains how authentication and chain-of-trust verification is implemented in our stack."OCaml-TLS: ASN.1 and notation embedding" introduces the libraries needed for handling ASN.1 grammars, the wire representation of messages in TLS."OCaml-TLS: the protocol implementation and mitigations to known attacks" concludes with the implementation of the core TLS protocol logic itself.Modularity and communication: MirageOS is built on the concept of a library operating system, and this release provides many new libraries to flexibly extend applications with new functionality."Fitting the modular MirageOS TCP/IP stack together" by Mindy Preston explains the rather unique modular architecture of our TCP/IP stack that lets you swap between the conventional Unix sockets API, or a complete implementation of TCP/IP in pure OCaml."Vchan: low-latency inter-VM communication channels" by Jon Ludlam shows how unikernels can communicate efficiently with each other to form distributed clusters on a multicore Xen host, by establishing shared memory rings with each other."Modular foreign function bindings" by Jeremy Yallop continues the march towards abstraction by expaining how to interface safely with code written in C, without having to write any unsafe C bindings!  This forms the basis for allowing Xen unikernels to communicate with existing libraries that they may want to keep at arm's length for security reasons.

All the libraries required for these new features are regularly
released into the OPAM package manager, so
just follow the installation instructions to give them a spin.
A release this size probably introduces minor hiccups that may cause build
failures, so we very much encourage bug
reports on our issue tracker or
questions to our mailing lists.  Don't be shy: no question is too
basic, and we'd love to hear of any weird and wacky uses you put this new
release to!  And finally, the lifeblood of MirageOS is about sharing and
publishing libraries that add new functionality to the framework, so do get
involved and open-source your own efforts.
Breaking news: Richard Mortier and I will be speaking at OSCON this week on Thursday morning about the new features in F150 in the Cloud Track. Come along if you are in rainy Portland at the moment!

   Hide
        
      
                    by Anil Madhavapeddy at Jul 22, 2014 
      
      
    
  


       
                  Building an ARMy of Xen unikernels
      (Mirage OS)
    
    
                                      Mirage has just gained the ability to compile unikernels for the Xen/arm32
platform, allowing Mirage guests to run under the Xen hypervisor on ARM
devices such as the Cubieboard 2 and CubieTruck.
Introduction

The ARMv7 architecture introduced the (optional) Virtualization Extensions,
providing hardware support for running virtual machines on ARM devices, and
Xen's ARM Hypervisor uses this to support hardware accelerated
ARM guests.
Mini-OS is a tiny OS kernel designed specifically for running under Xen.
It provides code to initialise the CPU, display messages on the console,
allocate memory (malloc), and not much else. It is used as the low-level
core of Mirage's Xen implementation.
Mirage v1 was built on an old version of Mini-OS which didn't support ARM.
For Mirage v2, we have added ARM support to the current Mini-OS (completing
Karim Allah Ahmed's initial ARM port) and made Mirage depend
on it as an external library.
This means that Mirage will automatically gain support for…Read more...      Mirage has just gained the ability to compile unikernels for the Xen/arm32
platform, allowing Mirage guests to run under the Xen hypervisor on ARM
devices such as the Cubieboard 2 and CubieTruck.
Introduction

The ARMv7 architecture introduced the (optional) Virtualization Extensions,
providing hardware support for running virtual machines on ARM devices, and
Xen's ARM Hypervisor uses this to support hardware accelerated
ARM guests.
Mini-OS is a tiny OS kernel designed specifically for running under Xen.
It provides code to initialise the CPU, display messages on the console,
allocate memory (malloc), and not much else. It is used as the low-level
core of Mirage's Xen implementation.
Mirage v1 was built on an old version of Mini-OS which didn't support ARM.
For Mirage v2, we have added ARM support to the current Mini-OS (completing
Karim Allah Ahmed's initial ARM port) and made Mirage depend
on it as an external library.
This means that Mirage will automatically gain support for other
architectures that get added later.
We are currently working with the Xen developers to get
our Mini-OS fork upstreamed.
In a similar way, we have replaced Mirage v1's bundled maths library with a
dependency on the external
OpenLibm, which we also extended
with ARM support (this was just a case of fixing the build system; the code
is from FreeBSD's libm, which already supported ARM).
Mirage v1 also bundled dietlibc to provide its standard C library.
A nice side-effect of this work came when we were trying to separate out the
dietlibc headers from the old Mini-OS headers in Mirage.
These had rather grown together over time and the work was proving
difficult, until we discovered that we no longer needed a libc at all, as
almost everything that used it had been replaced with pure OCaml versions!
The only exception was the printf code for formatting floating point
numbers, which OCaml uses in its printf implementation.
We replaced that by taking the small fmt_fp function from
musl libc.
Here's the final diffstat of the changes to mirage-platform
adding ARM support:
778 files changed, 1949 insertions(+), 59689 deletions(-)

Trying it out

You'll need an ARM device with the Virtualization Extensions.
I've been testing using the Cubieboard 2 (and CubieTruck):

The first step is to install Xen.
Running Xen on the Cubieboard2
documents the manual installation process, but you can now also use
mirage/xen-arm-builder to build
an SDcard image automatically.
Copy the image to the SDcard, connect the network cable and power, and the
board will boot Xen.
Once booted you can ssh to Dom0, the privileged Linux domain used to manage
the system, install Mirage, and build your unikernel just
as on x86.
Currently, you need to select the Git versions of some components.
The following commands will install the necessary versions if you're using
the xen-arm-builder image:
$ opam init
$ opam install mirage-xen-minios
$ opam remote add mirage-dev git://github.com/mirage/mirage-dev
$ opam install mirage

Technical details

One of the pleasures of unikernels is that you can comprehend the whole
system with relatively little effort, and
those wishing to understand, debug or contribute to the ARM support may find
the following technical sections interesting.
However, you don't need to know the details of the ARM port to use it,
as Mirage abstracts away the details of the underlying platform.
The boot process

An ARM Mirage unikernel uses the Linux zImage format, though it is
not actually compressed. Xen will allocate some RAM for the image and load
the kernel at the offset 0x8000 (32 KB).
Execution begins in arm32.S, with the r2 register pointing to a
Flattened Device Tree (FDT) describing details of the virtual system.
This assembler code performs a few basic boot tasks:
Configuring the MMU, which maps virtual addresses to physical addresses (see next section).Turning on caching and branch prediction.Setting up the exception vector table (this says how to handle interrupts and deal with various faults, such as reading from an invalid address).Setting up the stack pointer and calling the C function arch_init.

arch_init makes some calls to the hypervisor to set up support for the console and interrupt controller, and then calls start_kernel.
start_kernel (in libxencaml) sets up a few more features (events, malloc, time-keeping and grant tables), then calls caml_startup.
caml_startup (in libocaml) initialises the garbage collector and calls caml_program, which is your application's main.ml.
The address space

With the Virtualization Extensions, there are two stages to converting a
virtual memory address (used by application code) to a physical address in
RAM.
The first stage is under the control of the guest VM, mapping the virtual
address to what the guest believes is the physical address (this address is
referred to as the Intermediate Physical Address or IPA).
The second stage, under the control of Xen, maps the IPA to the real
physical address.
The tables holding these mappings are called translation tables.
Mirage's memory needs are simple: most of the RAM should be used for the
garbage-collected OCaml heap, with a few pages used for interacting with Xen
(these don't go on the OCaml heap because they must be page aligned and must
not move around).
Xen does not commit to using a fixed address as the IPA of the RAM, but the
C code needs to run from a known location. To solve this problem the
assembler code in arm32.S detects where it is running from and sets up a
virtual-to-physical mapping that will make it appear at the expected
location, by adding a fixed offset to each virtual address.
For example, on Xen/unstable, we configure the beginning of the virtual
address space to look like this (on Xen 4.4, the physical addresses would
start at 80000000 instead):
  Virtual addressPhysical address (IPA)Purpose
  40000040000000Stack (16 KB)
  40400040004000Translation tables (16 KB)
  40800040008000Kernel image


The physical address is always at a fixed offset from the virtual address and
the addresses wrap around, so virtual address c0400000 maps back to physical
address 0 (in this example).
The stack, which grows downwards, is placed at the start of RAM so that a
stack overflow will trigger a fault rather than overwriting other data.
The 16 KB translation table is an array of 4-byte entries each mapping 1 MB
of the virtual address space, so the 16 KB table is able to map the entire
32-bit address space (4 GB). Each entry can either give the physical section
address directly (which is what we do) or point to a second-level table
mapping individual 4 KB pages. By using only the top-level table we reduce
possible delays due to TLB misses.
After the kernel code comes the data (constants and global variables), then
the bss section (data that is initially
zero, and therefore doesn't need to be stored in the kernel image),
and finally the rest of the RAM, which is handed over to the malloc system.
Contact

The current version seems to be working well on Xen 4.4 (stable) and the 4.5
development version, but has only been lightly tested.
If you have any problems or questions, or get it working on other devices,
please let us know!

   Hide
        
      
                    by Thomas Leonard at Jul 22, 2014 
      
      
    
  


       
          Using Irmin to add fault-tolerance to the Xenstore database
      (Mirage OS)
    
                                      This is the second in a series of posts that introduces the Irmin distributed storage engine.
You might like to begin with the introductory post.
Xenstore is a critical service found on all hosts
running Xen. Xenstore is necessary to
configure all VM I/O devices such as disk controllers and network interface cards;share performance statistics and OS version information; andsignal VMs during shutdown, suspend, resume, migrate etc.

Xenstore must be reliable: if it fails then the host is unmanageable and must be rebooted.
Xenstore must be secure: if it is compromised by a VM then that VM can access data belonging
to other VMs.
The current version of Xenstore is already written in OCaml
and documented in the paper
OXenstored: an efficient hierarchical and transactional database using functional programming with reference cell comparisons presented at ICFP 2009.
The existing code works very reliably, but there is always room for improvement
for debuggability of such a complex system…Read more...      This is the second in a series of posts that introduces the Irmin distributed storage engine.
You might like to begin with the introductory post.
Xenstore is a critical service found on all hosts
running Xen. Xenstore is necessary to
configure all VM I/O devices such as disk controllers and network interface cards;share performance statistics and OS version information; andsignal VMs during shutdown, suspend, resume, migrate etc.

Xenstore must be reliable: if it fails then the host is unmanageable and must be rebooted.
Xenstore must be secure: if it is compromised by a VM then that VM can access data belonging
to other VMs.
The current version of Xenstore is already written in OCaml
and documented in the paper
OXenstored: an efficient hierarchical and transactional database using functional programming with reference cell comparisons presented at ICFP 2009.
The existing code works very reliably, but there is always room for improvement
for debuggability of such a complex system component. This is where Irmin, the
storage layer of Mirage 2.0, can help.
But first, a quick Xenstore primer:
Xen and Xenstore in 30 seconds

The Xen hypervisor focuses on isolating VMs from each-other; the hypervisor provides a virtual CPU scheduler
and a memory allocator but does not perform I/O on behalf of guest VMs.
On a Xen host, privileged server VMs perform I/O on behalf of client VMs.
The configuration for calculating which server VM services requests for which client VMs is stored in Xenstore, as
key/value pairs.
The following diagram shows a Xen host with a single client and server VM, with
a single virtual device in operation.  Disk blocks and network packets flow via
shared memory between Xen-aware drivers in the VMs, shown in the lower-half.
The control-plane, shown in the upper-half, contains the metadata about the
datapath: how the device should appear in the client VM; where the I/O should
go in the server VM; where the shared memory control structures are etc.

The Xenstore device attach protocol insists that all device keys are added
through atomic transactions, i.e. partial updates are never visible to clients and transactions
cannot interfere with each other.
A Xenstore server must abort transactions whose operations were not successfully
isolated from other transactions. After an abort, the client is expected to retry.
Each key=value write is communicated to the server as a single request/response, so transactions
comprising multiple writes are open for multiple round-trip times.
This protocol is baked into guest VM kernels (including Linux, FreeBSD, Mirage, ...)
and won't change anytime soon.
Xenstore is used heavily when lots of VMs are starting in parallel. Each VM typically
has several devices, each of these devices is added in a parallel transaction and therefore
many transactions are open at once. If the server aborts too many of these transactions,
causing the clients to retry, the system will make little progress and may appear to live-lock.
The challenge for a Xenstore implementation is to minimise the number of aborted
transactions and retries, without compromising on the isolation guarantee.
Irmin Xenstore design goals

The design goals of the Irmin-based Mirage Xenstore server are:
safely restart after a crash;make system debugging easy; andgo really fast!

How does Irmin help achieve these goals?
Restarting after crashes

The Xenstore service is a reliable component and very rarely crashes. However,
if a crash does occur, the impact is severe on currently running virtual
machines. There is no protocol for a running VM to close its connection to a
Xenstore and open a new one, so if Xenstore crashes then running VMs are simply
left orphaned. VMs in this state are impossible to manage properly: there is no
way to shut them down cleanly, to suspend/resume or migrate, or to configure
any disk or network interfaces. If Xenstore crashes, the host must be rebooted
shortly after.
Irmin helps make Xenstore recoverable after a crash, by providing a library
that applications can use to persist and synchronise distributed data
structures on disk and in memory. By using Irmin to persist all our state
somewhere sensible and taking care to manage our I/O carefully, then the server
process becomes stateless and can be restarted at will.
To make Xenstore use Irmin,
the first task is to enumerate all the different kinds of state in the running process.
This includes the obvious key-value pairs used for VM configuration
as well as data currently hidden away in the OCaml heap:
the addresses in memory of established communication rings,
per-domain quotas, pending watch events and watch registrations etc etc. 
Once the state has been enumerated it must be mapped onto key-value pairs which can
be stored in Irmin. Rather than using ad-hoc mappings everywhere, the Mirage Irmin
server has
persistent Maps,
persistent Sets,
persistent Queues
and
persistent reference cells.
Irmin applications are naturally written as functors, with the details of the persistence kept
abstract.
The following Irmin-inspired signature represents what Xenstore needs
from Irmin:
module type VIEW = sig
  type t

  val create: unit -> t Lwt.t
  (** Create a fresh VIEW from the current state of the store.
      A VIEW tracks state queries and updates and acts like a branch
      which has an explicit [merge]. *)

  val read: t -> Protocol.Path.t -> 
    [ `Ok of Node.contents | `Enoent of Protocol.Path.t ] Lwt.t
  (** Read a single key *)

  val list: t -> Protocol.Path.t -> 
    [ `Ok of string list | `Enoent of Protocol.Path.t ] Lwt.t
  (** List all the children of a key *)

  val write: t -> Protocol.Path.t -> Node.contents -> 
    [ `Ok of unit ] Lwt.t
  (** Update a single key *)

  val mem: t -> Protocol.Path.t -> bool Lwt.t
  (** Check whether a key exists *)

  val rm: t -> Protocol.Path.t -> [ `Ok of unit ] Lwt.t
  (** Remove a key *)

  val merge: t -> string -> bool Lwt.t
  (** Merge this VIEW into the current state of the store *)
endThe main 'business logic' of Xenstore can then be functorised over this signature relatively easily.
All we need is to instantiate the functor using Irmin to persist the data somewhere sensible.
Eventually we will need two instantiations: one which runs as a userspace application and which
writes to the filesystem; and a second which will run as a
native Xen kernel (known as a xenstore stub domain)
and which will write to a fixed memory region (like a ramdisk).
The choice of which to use is left to the system administrator. Currently most (if not all)
distribution packagers choose to run Xenstore in userspace. Administrators who wish to
further secure their hosts are encouraged to run the kernelspace version to isolate Xenstore
from other processes (where a VM offers more isolation than a container, which offers more
isolation than a chroot). Note this choice is invisible to the guest VMs.
So far in the Irmin Xenstore integration only the userspace instantiation has been implemented.
One of the most significant user-visible features is that all of the operations done through
Irmin can be inspected using the standard git command line tool.
The runes to configure Irmin to write
git format data to the filesystem are as follows:
    let open Irmin_unix in
    let module Git = IrminGit.FS(struct
      let root = Some filename
      let bare = true
    end) in
    let module DB = Git.Make(IrminKey.SHA1)(IrminContents.String)(IrminTag.String) in
    DB.create () >>= fun db ->where keys and values will be mapped into OCaml strings, and our
VIEW.t is simply an Irmin DB.View.t. All that remains is to implement
read, list, write, rm by
mapping Xenstore Protocol.Path.t values onto Irmin keys; andmapping Xenstore Node.contents records onto Irmin values.

As it happens Xenstore and Irmin have similar notions of "paths" so the first mapping is
easy. We currently use sexplib to map Node.contents
values onto strings for Irmin.
The resulting Irmin glue module looks like:
    let module V = struct
      type t = DB.View.t
      let create = DB.View.create
      let write t path contents =
        DB.View.update t (value_of_filename path) (Sexp.to_string (Node.sexp_of_contents contents))
      (* omit read,list,write,rm for brevity *)
      let merge t origin =
        let origin = IrminOrigin.create "%s" origin in
        DB.View.merge_path ~origin db [] t >>= function
        | `Ok () -> return true
        | `Conflict msg ->
          info "Conflict while merging database view: %s" msg;
          return false
    end inThe write function simply calls through to Irmin's update function, while the merge function
calls Irmin's merge_path. If Irmin cannot merge the transaction then our merge function will
return false and this will be signalled to the client, which is expected to retry the high-level
operation (e.g. hotplugging or unplugging a device).
Now all that remains is to carefully adjust the I/O code so that effects (reading and writing packets
along the persistent connections) are interleaved properly with persisted state changes and
voilà, we now have a xenstore which can recover after a restart.
Easy system debugging with Git

When something goes wrong on a Xen system it's standard procedure to
take a snapshot of the current state of Xenstore; andexamine the log files for signs of trouble.

Unfortunately by the
time this is done, interesting Xenstore state has usually been deleted. Unfortunately the first task
of the human operator is to evaluate by-hand the logged actions in reverse to figure out what the state
actually was when the problem happened. Obviously this is tedious, error-prone and not always
possible since the log statements are ad-hoc and don't always include the data you need to know.
In the new Irmin-powered Xenstore the history is preserved in a git-format repository, and can
be explored using your favourite git viewing tool. Each store
update has a compact one-line summary, a more verbose multi-line explanation and (of course)
the full state change is available on demand.
For example you can view the history in a highly-summarised form with:
$ git log --pretty=oneline --abbrev-commit --graph
* 2578013 Closing connection -1 to domain 0
* d4728ba Domain 0: rm /bench/local/domain/0/backend/vbd/10 = ()
* 4b55c99 Domain 0: directory /bench/local/domain/0/backend = [ vbd ]
* a71a903 Domain 0: rm /bench/local/domain/10 = ()
* f267b31 Domain 0: rm /bench/vss/uuid-10 = ()
* 94df8ce Domain 0: rm /bench/vm/uuid-10 = ()
* 0abe6b0 Domain 0: directory /bench/vm/uuid-10/domains = [  ]
* 06ddd3b Domain 0: rm /bench/vm/uuid-10/domains/10 = ()
* 1be2633 Domain 0: read /bench/local/domain/10/vss = /bench/vss/uuid-10
* 237a8e4 Domain 0: read /bench/local/domain/10/vm = /bench/vm/uuid-10
* 49d70f6 Domain 0: directory /bench/local/domain/10/device = [  ]
*   ebf4935 Merge view to /
|\
| * e9afd9f Domain 0: read /bench/local/domain/10 =
* | c4e0fa6 Domain 0: merging transaction 375
|/The summarised form shows both individual operations as well as isolated transactions which
are represented as git branches.
You can then 'zoom in' and show the exact state change with commands like:
$ git show bd44e03
commit bd44e0388696380cafd048eac49474f68d41bd3a
Author: 448 <irminsule@openmirage.org>
Date:   Thu Jan 1 00:09:26 1970 +0000

    Domain 0: merging transaction 363

diff --git a/*0/bench.dir/local.dir/domain.dir/7.dir/control.dir/shutdown.value b/*0/bench.dir/local.dir/domain.dir/7.dir/control.dir/shutdown.value
new file mode 100644
index 0000000..aa38106
--- /dev/null
+++ b/*0/bench.dir/local.dir/domain.dir/7.dir/control.dir/shutdown.value
@@ -0,0 +1 @@
+((creator 0)(perms((owner 7)(other NONE)(acl())))(value halt))Last but not least, you can git checkout to the exact time the problem occurred and examine
the state of the store.
Going really fast

Xenstore is part of the control-plane of a Xen system and is most heavily stressed when lots
of VMs are being started in parallel. Each VM has multiple devices and each device is added in a
separate transaction. These transactions remain open for multiple client-server round-trips, as
each individual operation is sent to Xenstore as a separate RPC. 
To provide isolation, each Xenstore transaction is represented by an Irmin VIEW.t which
is persisted on disk as a git branch.
When starting lots of VMs in
parallel, lots of branches are created and must be merged back together. If a branch cannot
be merged then an abort signal is sent to the client and it must retry.
Earlier versions of Xenstore had naive transaction merging algorithms
which aborted many of these transactions, causing the clients to re-issue them.This led to a live-lock
where clients were constantly reissuing the same transactions again and again.
Happily Irmin's default merging strategy is much better: by default Irmin
records the results of every operation and replays the operations on merge
(similar to git rebase). Irmin will only generate a Conflict and signal an
abort if the client would now see different results to those it has already
received (imagine reading a key twice within an isolated transaction and seeing
two different values). In the case of parallel VM starts, the keys are disjoint
by construction so all transactions are merged trivially; clients never receive
abort signals; and therefore the system makes steady, predictable progress
starting the VMs.
Trying it out

The Irmin Xenstore is under active development
but you can try it by:
Install basic development tools along with the xen headers and xenstore tools (NB you don't
actually have to run Xen):
  sudo apt-get install libxen-dev xenstore-utils opam build-essential m4Initialise opam (if you haven't already). Make sure you have OCaml 4.01:
  opam init
  opam update
  opam switch 4.01.0Install the OCaml build dependencies:
  opam install lwt irmin git sexplib cstruct uri sexplib cmdliner xen-evtchn shared-memory-ring io-page ounitClone the code and build it:
  git clone git://github.com/mirage/ocaml-xenstore-server
  cd ocaml-xenstore-server
  makeRun a server (as a regular user):
  ./main.native --database /tmp/db --enable-unix --path /tmp/xenstoredIn a separate terminal, perform some operations:
  export XENSTORED_PATH=/tmp/xenstored
  xenstore-write -s /one/two/three 4 /five/six/seven 8
  xenstore-ls -s /Next check out the git repo generated by Irmin:
  cd /tmp/db
  git log

Comments and/or contributions are welcome: join the Mirage email list and say hi!

   Hide
        
      
                    by Dave Scott at Jul 21, 2014 
      
      
    
  


       
          Introducing Irmin: Git-like distributed, branchable storage
      (Mirage OS)
    
                                      This is the first post in a series which will describe Irmin,
 the new Git-like storage layer for Mirage OS 2.0. This post gives a
 high-level description on Irmin and its overall architecture, and
 later posts will detail how to use Irmin in real systems.


Irmin is a library to persist and synchronize distributed
data structures both on-disk and in-memory. It enables a style of
programming very similar to the Git workflow, where
distributed nodes fork, fetch, merge and push data between
each other. The general idea is that you want every active node to
get a local (partial) copy of a global database and always be very
explicit about how and when data is shared and migrated.
Irmin is not, strictly speaking, a full database engine. It
is, as are all other components of Mirage OS, a collection of
libraries designed to solve different flavours of the challenges raised
by the CAP theorem. Each application can select the right
combination of libraries to solve its particular distrib…Read more...      This is the first post in a series which will describe Irmin,
 the new Git-like storage layer for Mirage OS 2.0. This post gives a
 high-level description on Irmin and its overall architecture, and
 later posts will detail how to use Irmin in real systems.


Irmin is a library to persist and synchronize distributed
data structures both on-disk and in-memory. It enables a style of
programming very similar to the Git workflow, where
distributed nodes fork, fetch, merge and push data between
each other. The general idea is that you want every active node to
get a local (partial) copy of a global database and always be very
explicit about how and when data is shared and migrated.
Irmin is not, strictly speaking, a full database engine. It
is, as are all other components of Mirage OS, a collection of
libraries designed to solve different flavours of the challenges raised
by the CAP theorem. Each application can select the right
combination of libraries to solve its particular distributed problem. More
precisely, Irmin consists of a core of well-defined low-level
data structures that specify how data should be persisted
and be shared across nodes. It defines algorithms for efficient
synchronization of those distributed low-level constructs. It also
builds a collection of higher-level data structures, like persistent
mergeable queues, that can be used by developers without
having to know precisely how Irmin works underneath.
Since it's a part of Mirage OS, Irmin does not make strong assumptions about the
OS environment that it runs in. This makes the system very portable, and the
details below hold for in-memory databases as well as for slower persistent
serialization such as SSDs, hard drives, web browser local storage, or even
the Git file format.
Persistent Data Structures

Persistent data structures are well known and used pervasively in many
different areas. The programming language community has
investigated the concepts widely (and this is not
limited to functional programming), and in the meantime,
the systems community experimented with various persistent
strategies such as copy-on-write filesystems. In most of these
systems, the main concern is how to optimize the space complexity by
maximizing the sharing of immutable sub-structures.
The Irmin design ideas share roots with previous works on persistent data
structures, as it provides an efficient way to fork data structures,
but it also explores new strategies and mechanisms to be able to
efficiently merge back these forked structures. This offers
programming constructs very similar to the Git workflow.
Irmin focuses on two main aspects:
Semantics: what properties the resulting merged objects should
verify.
Complexity: how to design efficient merge and synchronization
primitives, taking advantage of the immutable nature of the underlying
objects.


Although it is pervasively used, data persistence has a very broad and
fuzzy meaning. In this blog post, I will refer to data persistence as
a way for:
a single process to lazily populate a process memory on startup.
 You need this when you want the process to be able to resume while
 holding part of its previous state if it crashes
concurrent processes to share references between objects living in
 a global pool of data. Sharing references, as opposed to sharing
 values, reduces memory copies and allow different processes to
 concurrently update a shared store.


In both cases, you need a global pool of data (the Irmin block store)
and a way to name values in that pool (the Irmin tag store).
The Block Store: a Virtual Heap

Even high-level data structures need to be allocated in memory, and it
is the purpose of the runtime to map such high-level constructs into
low-level memory graph blocks. One of the strengths of OCaml
is the very simple and deterministic mapping from high-level data
structures to low-level block representations (the heap): see for
instance, the excellent series of blog posts on OCaml
internals by Richard W. Jones, or
Chapter 20: Memory Representation of Values in
Real World OCaml.
An Irmin block store can be seen as a virtual OCaml heap that uses a more
abstract way of connecting heap blocks. Instead of using the concrete physical
memory addresses of blocks, Irmin uses the hash of the block contents as an
address. As for any content-addressable storage, this gives Irmin
block stores a lot of nice properties and greatly simplifies the way distributed
stores can be synchronized.
Persistent data structures are immutable, and once a block is created in
the block store, its contents will never change again.
Updating an immutable data structure means returning a completely new
structure, while trying to share common sub-parts to avoid the cost of
making new allocations as much as possible. For instance, modifying a
value in a persistent tree means creating a chain of new blocks, from
the root of the tree to the modified leaf.
For convenience, Irmin only considers acyclic block graphs --
it is difficult in a non-lazy pure language to generate complex cyclic
values with reasonable space usage.
Conceptually, an Irmin block store has the following signature:
type t
(** The type for Irmin block store. *)

type key
(** The type for Irmin pointers *)

type value = ...
(** The type for Irmin blocks *)

val read: t -> key -> value option
(** [read t k] is the block stored at the location [k] of the
store. It is [None] if no block is available at that location. *)

val add: t -> key -> value -> t
(** [add t k v] is the *new* store storing the block [v] at the
location [k]. *)

Persistent data structures are very efficient to store in memory and on
disk as you do not need write barriers, and updates
can be written sequentially instead of requiring random
access into the data structure.
The Tag Store: Controlled Mutability and Concurrency

So far, we have only discussed purely functional data structures,
where updating a structure means returning a pointer to a new
structure in the heap that shares most of its contents with the previous
one. This style of programming is appealing when implementing
complex protocols as it leads to better compositional properties.

However, this makes sharing information between processes much more
difficult, as you need a way to "inject" the state of one structure into another process's memory. In order to do so, Irmin borrows the concept of
branches from Git by relating every operation to a branch name, and
modifying the tip of the branch if it has side-effects.
The Irmin tag store is the only mutable part of the whole system and
is responsible for mapping some global (branch) names to blocks in the
block store. These tag names can then be used to pass block references between
different processes.
A block store and a tag store can be combined to build
a higher-level store (the Irmin store) with fine concurrency control
and atomicity guarantees. As mutation happens only in the tag store,
we can ensure that as long a given tag is not updated, no change made
in the block store will be visible by anyone. This also gives a nice
story for concurrency: as in Git, creating a concurrent view of the
store is the straightforward operation of creating a new tag that
denotes a new branch. All concurrent operations can then happen on
different branches:
type t
(** The type for Irmin store. *)

type tag
(** Mutable tags *)

type key = ...
(** The type for user-defined keys (for instance a list of strings) *)

type value = ...
(** The type for user-defined values *)

val read: t -> ?branch:tag -> key -> value option
(** [read t ?branch k] reads the contents of the key [k] in the branch
[branch] of the store [t]. If no branch is specified, then use the
["HEAD"] one. *)

val update: t -> ?branch:tag -> key -> value -> unit
(** [update t ?branch k v] *updates* the branch [branch] of the store
[t] the association of the key [key] to the value [value]. *)

Interactions between concurrent processes are completely explicit and
need to happen via synchronization points and merge events (more on
this below). It is also possible to emulate the behaviour of
transactions by recording the sequence of operations (read and
update) on a given branch -- that sequence is used before a merge
to check that all the operations are valid (i.e. that all reads in the
transaction still return the same result on the current tip of the
store) and it can be discarded after the merge takes place.
Merging Data Structures

To merge two data structures in a consistent way, one has to compute
the sequence of operations which leads, from an initial common state, to two
diverging states (the ones that you want to merge). Once these two
sequences of operations have been found, they must be combined (if
possible) in a sensible way and then applied again back on the initial
state, in order to get the new merged state. This mechanism sounds
nice, but in practice it has two major drawbacks:
It does not specify how we find the initial state from two diverging
 states -- this is generally not possible (think of diverging
 counters); andIt means we need to compute the sequence of update operations
 that leads from one state to an other.  This is easier than finding
 the common initial state between two branches, but is still generally
 not very efficient.

In Irmin, we solve these problems using two mechanisms.
First of all, an interesting observation is that that we can model the
sequence of store tips as a purely functional data-structure. We model
the partial order of tips as a directed acyclic graph where nodes are
the tips, and there is an edge between two tips if either (i) one is
the result of applying a sequence of updates to the other, or (ii)
one is the result of a merge operation between the other and some
other tips. Practically speaking, that means that every tip should
contains the list of its predecessors as well as the actual data it
associated to. As it is purely functional, we can (and we do) store
that graph in an Irmin block store.

Having a persistent and immutable history is good for various obvious
reasons, such as access to a forensics if an error occurs or
snapshot and rollback features for free. But another less obvious
useful property is that we can now find the greatest common
ancestors of two data structures without an expensive global search.
The second mechanism is that we require the data structures used in
Irmin to be equipped with a well-defined 3-way merge operation, which
takes two diverging states, the corresponding initial state (computed
using the previous mechanism) and that return either a new state or a
conflict (similar to the EAGAIN exception that you get when you try
to commit a conflicting transaction in more traditional transactional
databases). Having access to the common ancestors makes a great
difference when designing new merge functions, as usually no
modification is required to the data-structure itself. In contrast,
the conventional approach is more invasive as it requires the data
structure to carry more information about the operation history
(for instance conflict-free replicated
datatypes, which relies on unbounded vector clocks).
We have thus been designing interesting data structure equipped with a 3-way
merge, such as counters, queues and ropes.
This is what the implementation of distributed and mergeable counters
looks like:
type t = int
(** distributed counters are just normal integers! *)

let merge ~old t1 t2 = old + (t1-old) + (t2-old)
(** Merging counters means:
   - computing the increments of the two states [t1] and [t2]
     relatively to the initial state [old]; and
   - and add these two increments to [old]. *)

Next steps, how to git at your data

From a design perspective, having access to the history makes it easier to
design complex data structures with good compositional properties to use in
unikernels. Moreover, as we made few assumptions on how the substrate of the
low-level constructs need to be implemented, the Irmin engine can be be ported
to many exotic backends such as JavaScript or anywhere else that Mirage OS
runs: this is just a matter of implementing a rather trivial
signature.
From a developer perspective, this means that the full history of operations is
available to inspect, and that the history model is very similar to the Git
workflow that is increasingly familiar. So similar, in fact, that we've
developed a bidirectional mapping between Irmin data structures and the Git
format to permit the git command-line to interact with.
The next post in our series explains what Dave Scott has been doing
with the new version of the Xenstore database that powers every Xen host,
where the entire database is stored in a prefix-tree Irmin data-structure and exposed
as a Git repository which is live-updated!  Here's a sneak preview...
     



   Hide
        
      
                    by Thomas Gazagnaire at Jul 18, 2014 
      
      
    
  


       
          Fitting the modular MirageOS TCP/IP stack together
      (Mirage OS)
    
                                      A critical part of any unikernel is its network stack -- it's difficult to
think of a project that needs a cloud platform or runs on a set-top box with no
network communications.
Mirage provides a number of module
types that abstract
interfaces at different layers of the network stack, allowing unikernels to
customise their own stack based on their deployment needs. Depending on the
abstractions your unikernel uses, you can fulfill these abstract interfaces
with implementations ranging from the venerable and much-imitated Unix sockets
API to a clean-slate Mirage TCP/IP
stack written from the ground up in
pure OCaml!
A Mirage unikernel will not use all these interfaces, but will pick those that
are appropriate for the particular application at hand. If your unikernel just
needs a standard TCP/IP stack, the STACKV4 abstraction will be sufficient.
However, if you want more control over the implementation of the different
layers in the stack or you don't need TCP support, you might …Read more...      A critical part of any unikernel is its network stack -- it's difficult to
think of a project that needs a cloud platform or runs on a set-top box with no
network communications.
Mirage provides a number of module
types that abstract
interfaces at different layers of the network stack, allowing unikernels to
customise their own stack based on their deployment needs. Depending on the
abstractions your unikernel uses, you can fulfill these abstract interfaces
with implementations ranging from the venerable and much-imitated Unix sockets
API to a clean-slate Mirage TCP/IP
stack written from the ground up in
pure OCaml!
A Mirage unikernel will not use all these interfaces, but will pick those that
are appropriate for the particular application at hand. If your unikernel just
needs a standard TCP/IP stack, the STACKV4 abstraction will be sufficient.
However, if you want more control over the implementation of the different
layers in the stack or you don't need TCP support, you might construct your
stack by hand using just the NETWORK, ETHIF, IPV4 and UDPV4 interfaces.
How a Stack Looks to a Mirage Application

Mirage provides a high-level interface to a TCP/IP network stack through the module type
STACKV4.
(Currently this can be included with open V1_LWT, but soon open
V2_LWT will also bring this module type into scope as well when Mirage 2.0 is released.)
(** Single network stack *)                                                     
module type STACKV4 = STACKV4                                                   
  with type 'a io = 'a Lwt.t                                                    
   and type ('a,'b,'c) config = ('a,'b,'c) stackv4_config                       
   and type ipv4addr = Ipaddr.V4.t                                              
   and type buffer = Cstruct.t 

STACKV4 has useful high-level functions, a subset of which are reproduced below:
    val listen_udpv4 : t -> port:int -> UDPV4.callback -> unit
    val listen_tcpv4 : t -> port:int -> TCPV4.callback -> unit
    val listen : t -> unit io

as well as submodules that include functions for data transmission:
    module UDPV4 :
      sig
        type callback =
            src:ipv4addr -> dst:ipv4addr -> src_port:int -> buffer -> unit io
        val input :
          listeners:(dst_port:int -> callback option) -> t -> ipv4input
        val write :
          ?source_port:int ->
          dest_ip:ipv4addr -> dest_port:int -> t -> buffer -> unit io

    module TCPV4 :
      sig
        type flow
        type callback = flow -> unit io
        val read : flow -> [ `Eof | `Error of error | `Ok of buffer ] io
        val write : flow -> buffer -> unit io
        val close : flow -> unit io
        val create_connection :
          t -> ipv4addr * int -> [ `Error of error | `Ok of flow ] io
        val input : t -> listeners:(int -> callback option) -> ipv4input

These should look rather familiar if you've used the Unix sockets
API before, with one notable difference: the stack accepts functional
callbacks to react to events such as a new connection request.  This
permits callers of the library to define the precise datastructures that
are used to store intermediate state (such as active connections).
This becomes important when building very scalable systems that have
to deal with lots of concurrent connections
efficiently.
Configuring a Stack

The STACKV4 signature shown so far is just a module signature, and you
need to find a concrete module that satisfies that signature.  The known
implementations of a module can be found in the mirage CLI frontend,
which provids the configuration API for unikernels.There are currently two implementations for STACKV4: direct and socket.
module STACKV4_direct: CONFIGURABLE with                                        
  type t = console impl * network impl * [`DHCP | `IPV4 of ipv4_config]         
                                                                                
module STACKV4_socket: CONFIGURABLE with                                        
  type t = console impl * Ipaddr.V4.t list  

The socket implementations rely on an underlying OS kernel to provide the
transport, network, and data link layers, and therefore can't be used for a Xen
guest VM deployment.  Currently, the only way to use socket is by configuring
your Mirage project for Unix with mirage configure --unix.  This is the mode
you will most often use when developing high-level application logic that doesn't
need to delve into the innards of the network stack (e.g. a REST website).
The direct implementations use the mirage-tcpip implementations of the
transport, network, and data link layers.  When you use this stack, all the network
traffic from the Ethernet level up will be handled in pure OCaml.  This means that the
direct stack will work with either a Xen
guest VM (provided there's a valid network configuration for the unikernel's
running environment of course), or a Unix program if there's a valid tuntap interface.
direct this works with both mirage configure --xen and mirage configure --unix
as long as there is a corresponding available device when the unikernel is run.
There are a few Mirage functions that provide IPv4 (and UDP/TCP) stack
implementations (of type stackv4 impl), usable from your application code.
The stackv4 impl is generated in config.ml by some logic set when the
program is mirage configure'd - often by matching an environment variable.
This means it's easy to flip between different stack implementations when
developing an application just be recompiling the application.  The config.ml
below allows the developer to build socket code with NET=socket make and
direct code with NET=direct make.
let main = foreign "Services.Main" (console @-> stackv4 @-> job)

let net =
  try match Sys.getenv "NET" with
    | "direct" -> `Direct
    | "socket" -> `Socket
    | _        -> `Direct
  with Not_found -> `Direct

let dhcp =
  try match Sys.getenv "ADDR" with
    | "dhcp"   -> `Dhcp
    | "static" -> `Static
    | _ -> `Dhcp
  with Not_found -> `Dhcp

let stack console =
  match net, dhcp with
  | `Direct, `Dhcp   -> direct_stackv4_with_dhcp console tap0
  | `Direct, `Static -> direct_stackv4_with_default_ipv4 console tap0
  | `Socket, _       -> socket_stackv4 console [Ipaddr.V4.any]

let () =
  register "services" [
    main $ default_console $ stack default_console
  ]

Moreover, it's possible to configure multiple stacks individually for use in
the same program, and to register multiple modules from the same config.ml.
This means functions can be written such that they're aware of the network
stack they ought to be using, and no other - a far cry from developing network
code over most socket interfaces, where it can be quite difficult to separate
concerns nicely.
let client = foreign "Unikernel.Client" (console @-> stackv4 @-> job)
let server = foreign "Unikernel.Server" (console @-> stackv4 @-> job) 

let client_netif = (netif "0")
let server_netif = (netif "1") 

let client_stack = direct_stackv4_with_dhcp default_console client_netif
let server_stack = direct_stackv4_with_dhcp default_console server_netif

let () = 
  register "unikernel" [
    main $ default_console $ client_stack;
    server $ default_console $ server_stack 
  ]



Acting on Stacks

Most network applications will either want to listen for incoming connections
and respond to that traffic with information, or to connect to some remote
host, execute a query, and receive information.  STACKV4 offers simple ways
to define functions implementing either of these patterns.
Establishing and Communicating Across Connections

STACKV4 offers listen_tcpv4 and listen_udpv4 functions for establishing
listeners on specific ports.  Both take a stack impl, a named port, and a
callback function.
For UDP listeners, which are datagram-based rather than connection-based,
callback is a function of the source IP, destination IP, source port, and the
Cstruct.t that contains the payload data.  Applications that wish to respond
to incoming UDP packets with their own UDP responses (e.g., DNS servers) can
use this information to construct reply packets and send them with
UDPV4.write from within the callback function.
For TCP listeners, callback is a function of TCPV4.flow -> unit Lwt.t.  STACKV4.TCPV4 offers read, write, and close on flows for application writers to build higher-level protocols on top of.
TCPV4 also offers create_connection, which allows client application code to establish TCP connections with remote servers.  In success cases, create_connection returns a TCPV4.flow, which can be acted on just as the data in a callback above.  There's also a polymorphic variant for error conditions, such as an unreachable remote server.
A Simple Example

Some very simple examples of user-level TCP code are included in mirage-tcpip/examples.  config.ml is identical to the first configuration example above, and will build a direct stack by default.
Imagine a very simple application - one which simply repeats any data back to the sender, until the sender gets bored and wanders off (RFC 862, for the curious).
open Lwt
open V1_LWT

module Main (C: V1_LWT.CONSOLE) (S: V1_LWT.STACKV4) = struct
  let report_and_close c flow message =
    C.log c message;
    S.TCPV4.close flow

  let rec echo c flow =
    S.TCPV4.read flow >>= fun result -> (
      match result with  
        | `Eof -> report_and_close c flow "Echo connection closure initiated."
        | `Error e -> 
          let message = 
          match e with 
            | `Timeout -> "Echo connection timed out; closing.\n"
            | `Refused -> "Echo connection refused; closing.\n"
            | `Unknown s -> (Printf.sprintf "Echo connection error: %s\n" s)
             in
          report_and_close c flow message
        | `Ok buf ->
            S.TCPV4.write flow buf >>= fun () -> echo c flow
        ) 

  let start c s = 
    S.listen_tcpv4 s ~port:7 (echo c);
    S.listen s

end

All the application programmer needs to do is define functionality in relation to flow for sending and receiving data, establish this function as a callback with listen_tcpv4, and start a listening thread with listen.
More Complex Uses

An OCaml HTTP server, Cohttp, is currently powering this very blog.  A simple static webserver using Cohttp is included in mirage-skeleton.
The OCaml-TLS demonstration server announced here just a few days ago is also running atop Cohttp - source is available on Github.
The future

Mirage's TCP/IP stack is under active development!  Some low-level details are still stubbed out, and we're working on implementing some of the trickier corners of TCP, as well as doing automated testing on the stack.  We welcome testing tools, bug reports, bug fixes, and new protocol implementations!

   Hide
        
      
                    by Mindy Preston at Jul 17, 2014 
      
      
    
  


       
          Vchan: Low-latency inter-VM communication channels
      (Mirage OS)
    
                                      Today's post is an update to Vincent Bernardoff's
introducing vchan blog
post, updated to use the modern build scheme for Mirage.
Unless you are familiar with Xen's source code, there is little chance
that you've ever heard of the vchan library or
protocol. Documentation about it is very scarce: a description can be
found on vchan's
public header file,
that I quote here for convenience:
Originally borrowed from the
Qubes OS Project, this code (i.e. libvchan)
has been substantially rewritten [...]
This is a library for inter-domain communication.  A standard Xen ring
buffer is used, with a datagram-based interface built on top.  The
grant reference and event channels are shared in XenStore under a
user-specified path.


This protocol uses shared memory for inter-domain communication,
i.e. between two VMs residing in the same Xen host, and uses Xen's
mechanisms -- more specifically,
ring buffers
and
event channels
-- in order to achieve its aims. The term datagram-based interface …Read more...      Today's post is an update to Vincent Bernardoff's
introducing vchan blog
post, updated to use the modern build scheme for Mirage.
Unless you are familiar with Xen's source code, there is little chance
that you've ever heard of the vchan library or
protocol. Documentation about it is very scarce: a description can be
found on vchan's
public header file,
that I quote here for convenience:
Originally borrowed from the
Qubes OS Project, this code (i.e. libvchan)
has been substantially rewritten [...]
This is a library for inter-domain communication.  A standard Xen ring
buffer is used, with a datagram-based interface built on top.  The
grant reference and event channels are shared in XenStore under a
user-specified path.


This protocol uses shared memory for inter-domain communication,
i.e. between two VMs residing in the same Xen host, and uses Xen's
mechanisms -- more specifically,
ring buffers
and
event channels
-- in order to achieve its aims. The term datagram-based interface simply
means that the
interface
resembles UDP, although there is support for stream based communication (like
TCP) as well.
The vchan protocol is an important feature in MirageOS 2.0 since it
forms the foundational communication mechanism for building distributed
clusters of unikernels that cooperate to solve problems that are beyond
the power of a single node.  Instead of forcing communication between
nodes via a conventional wire protocol like TCP, it permits highly efficient
low-overhead communication to nodes that are colocated on the same Xen
host machine.
Before diving into vchan, I thought I'd also take the opportunity to describe the
Ubuntu-Trusty environment for developing
and running Xen unikernels.
Installing Xen on Ubuntu

Ubuntu 14.04 has good support for running Xen 4.4, the most recent release (at time of writing).
For running VMs it's a good idea to install Ubuntu on an LVM volume rather than directly on a
partition, which allows the use of LVs as the virtual disks for your VMs. On my system I have
a 40 Gig partition for '/', an 8 Gig swap partition and the rest is free for my VMs:
$ sudo lvs
   LV     VG      Attr      LSize  Pool Origin Data%  Move Log Copy%  Convert
   root   st28-vg -wi-ao--- 37.25g
   swap_1 st28-vg -wi-ao---  7.99g

In this particular walkthough I won't be using disks, but later posts will.
Install Xen via the meta-package. This brings in all you will need to run VMs:
$ sudo apt-get install xen-system-amd64

It used to be necessary to reorder the grub entries to make sure Xen was started
by default, but this is no longer necessary. Once the machine has rebooted, you
should be able to verify you're running virtualized by invoking 'xl':
$ sudo xl list
Name                                        ID   Mem VCPUs      State   Time(s)
Domain-0                                     0  7958     6     r-----       9.7

My machine has 8 Gigs of memory, and this list shows that it's all being used by
my dom0, so I'll need to either balloon down dom0 or reboot with a lower maximum
memory. Ballooning is the most straightfoward:
$ sudo xenstore-write /local/domain/0/memory/target 4096000
$ sudo xl list
Name                                        ID   Mem VCPUs      State   Time(s)
Domain-0                                     0  4000     6     r-----      12.2

This is handy for quick testing, but is discouraged by the Xen folks. So alternatively, change the xen command line by
editing /etc/default/grub and add the line:
GRUB_CMDLINE_XEN_DEFAULT="dom0_mem=4096M,max:4096M"

Once again, update-grub and reboot.
Mirage

Now lets get Mirage up and running. Install ocaml, opam and set up the opam environment:
$ sudo apt-get install ocaml opam ocaml-native-compilers camlp4-extra
...
$ opam init
...
$ eval `opam config env`

Don't forget the ocaml-native-compilers, as without this we can't
compile the unikernels. Now we are almost ready to install Mirage; we
need two more dependencies, and then we're good to go.
$ sudo apt-get install m4 libxen-dev
$ opam install mirage mirage-xen mirage-unix vchan

Where m4 is for ocamlfind, and libxen-dev is required to compile the
unix variants of the xen-evtchn and xen-gnt libraries. Without these
installing vchan will complain that there is no xen-evtchn.lwt
library installed.
This second line installs the various Mirage and vchan libraries, but
doesn't build the demo unikernel and Unix CLI.  To get them, clone
the ocaml-vchan repository:
$ git clone https://github.com/mirage/ocaml-vchan

The demo unikernel is a very straightforward capitalizing echo server.
The main function simply consists of
let (>>=) = Lwt.bind

let (>>|=) m f = m >>= function
| `Ok x -> f x
| `Eof -> Lwt.fail (Failure "End of file")
| `Error (`Not_connected state) ->
    Lwt.fail (Failure (Printf.sprintf "Not in a connected state: %s"
      (Sexplib.Sexp.to_string (Node.V.sexp_of_state state))))

let rec echo vch =
  Node.V.read vch >>|= fun input_line ->
  let line = String.uppercase (Cstruct.to_string input_line) in
  let buf = Cstruct.create (String.length line) in
  Cstruct.blit_from_string line 0 buf 0 (String.length line);
  Node.V.write vch buf >>|= fun () ->
  echo vch

where we've defined an error-handling monadic bind (>>|=) which
is then used to sequence the read and write operations.
Building the CLI is done simply via make.
$ make
...
$ ls -l node_cli.native
lrwxrwxrwx 1 jludlam jludlam 52 Jul 14 14:56 node_cli.native -> /home/jludlam/ocaml-vchan/_build/cli/node_cli.native

Building the unikernel is done via the mirage tool:
$ cd test
$ mirage configure --xen
...
$ make depend
...
$ make
...
$ ls -l mir-echo.xen echo.xl
-rw-rw-r-- 1 jludlam jludlam     596 Jul 14 14:58 echo.xl
-rwxrwxr-x 1 jludlam jludlam 3803982 Jul 14 14:59 mir-echo.xen

This make both the unikernel binary (the mir-echo.xen file) and a convenient
xl script to run it. To run, we use the xl tool, passing '-c' to connect
directly to the console so we can see what's going on:
$ sudo xl create -c echo.xl
Parsing config from echo.xl
kernel.c: Mirage OS!
kernel.c:   start_info: 0x11cd000(VA)
kernel.c:     nr_pages: 0x10000
kernel.c:   shared_inf: 0xdf2f6000(MA)
kernel.c:      pt_base: 0x11d0000(VA)
kernel.c: nr_pt_frames: 0xd
kernel.c:     mfn_list: 0x114d000(VA)
kernel.c:    mod_start: 0x0(VA)
kernel.c:      mod_len: 0
kernel.c:        flags: 0x0
kernel.c:     cmd_line:
x86_setup.c:   stack:      0x144f40-0x944f40
mm.c: MM: Init
x86_mm.c:       _text: 0x0(VA)
x86_mm.c:      _etext: 0xb8eec(VA)
x86_mm.c:    _erodata: 0xde000(VA)
x86_mm.c:      _edata: 0x1336f0(VA)
x86_mm.c: stack start: 0x144f40(VA)
x86_mm.c:        _end: 0x114d000(VA)
x86_mm.c:   start_pfn: 11e0
x86_mm.c:     max_pfn: 10000
x86_mm.c: Mapping memory range 0x1400000 - 0x10000000
x86_mm.c: setting 0x0-0xde000 readonly
x86_mm.c: skipped 0x1000
mm.c: MM: Initialise page allocator for 0x1256000 -> 0x10000000
mm.c: MM: done
x86_mm.c: Pages to allocate for p2m map: 2
x86_mm.c: Used 2 pages for map
x86_mm.c: Demand map pfns at 10001000-2010001000.
Initialising timer interface
Initializing Server domid=0 xs_path=data/vchan
gnttab_stubs.c: gnttab_table mapped at 0x10001000
Server: right_order = 13, left_order = 13
allocate_buffer_locations: gntref = 9
allocate_buffer_locations: gntref = 10
allocate_buffer_locations: gntref = 11
allocate_buffer_locations: gntref = 12
Writing config into the XenStore
Shared page is:

00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
0d 00 0d 00 02 01 01 00 09 00 00 00 0a 00 00 00
0b 00 00 00 0c 00 00 00
Initialization done!

Vchan is domain-to-domain communication, and relies on Xen's grant
tables to share the memory. The entries in the grant tables have
domain-level access control, so we need to know the domain ID of the
client and server in order to set up the communications. The test
unikernel server is hard-coded to talk to domain 0, so we only need to
know the domain ID of our echo server. In another terminal,
$ sudo xl list
Name                                        ID   Mem VCPUs      State   Time(s)
Domain-0                                     0  4095     6     r-----    1602.9
echo                                         2   256     1     -b----       0.0

In this case, the domain ID is 2, so we invoke the CLI as follows:
$ sudo ./node_cli.native 2
Client initializing: Received gntref = 8, evtchn = 4
Mapped the ring shared page:

00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
0d 00 0d 00 02 01 01 00 09 00 00 00 0a 00 00 00
0b 00 00 00 0c 00 00 00
Correctly bound evtchn number 71

We're now connected via vchan to the Mirage domain. The test server
is simply a capitalisation service:
hello from dom0
HELLO FROM DOM0

Ctrl-C to get out of the CLI, and destroy the domain with an xl destroy:
$ sudo xl destroy test

vchan is a very low-level communication mechanism, and so our next post on
this topic will address how to use it in combination with a name resolver
to intelligently map connection requests to use vchan if available, and
otherwise fall back to normal TCP or TCP+TLS.

   Hide
        
      
                    by Jon Ludlam at Jul 16, 2014 
      
      
    
  


       
          Modular foreign function bindings
      (Mirage OS)
    
                                      One of the most frequent questions about MirageOS from developers is
"do I really need to write all my code in OCaml"?  There are, of
course, very good reasons to build the core system in pure OCaml: the
module system permits reusing algorithmic abstractions at scale, and
OCaml's static type checking makes it possible to enforce lightweight
invariants across interfaces.  However, it's ultimately necessary to
support interfacing to existing code, and this blog post will describe
what we're doing to make this possible this without sacrificing the
security benefits afforded by unikernels.
A MirageOS application works by abstracting the logic of the
application from the details of platform that it is compiled for.
The mirage CLI tool parses a configuration file that represents the
desired hardware target, which can be a Unix binary or a specialized
Xen guest OS.  Our foreign function interface design elaborates on
these design principles by separating the description of the C
foreig…Read more...      One of the most frequent questions about MirageOS from developers is
"do I really need to write all my code in OCaml"?  There are, of
course, very good reasons to build the core system in pure OCaml: the
module system permits reusing algorithmic abstractions at scale, and
OCaml's static type checking makes it possible to enforce lightweight
invariants across interfaces.  However, it's ultimately necessary to
support interfacing to existing code, and this blog post will describe
what we're doing to make this possible this without sacrificing the
security benefits afforded by unikernels.
A MirageOS application works by abstracting the logic of the
application from the details of platform that it is compiled for.
The mirage CLI tool parses a configuration file that represents the
desired hardware target, which can be a Unix binary or a specialized
Xen guest OS.  Our foreign function interface design elaborates on
these design principles by separating the description of the C
foreign functions from how we link to that code.  For instance, a
Unix unikernel could use the normal ld.so to connect to a shared
library, while in Xen we would need to interface to that C library
through some other mechanism (for instance, a separate VM could be
spawned to run the untrusted OpenSSL code).  If you're curious about
how this works, this blog post is for you!
Introducing ctypes

ocaml-ctypes ("ctypes" for short) is a library for
gluing together OCaml code and C code without writing any C.  This
post introduces the ctypes library with a couple of simple examples,
and outlines how OCaml's module system makes it possible to write
high-level bindings to C that are independent of any particular
linking mechanism.
Hello, C

Binding a C function using ctypes involves two steps.
First, construct an OCaml value that represents the type of the functionSecond, use the type representation and the function name to resolve and bind the function

For example, here's a binding to C's puts function, which prints a string to
standard output and returns the number of characters written:
let puts = foreign "puts" (string @-> returning int)

After the call to foreign the bound function is available to OCaml
immediately.  Here's a call to puts from the interactive top level:
# puts "Hello, world";;
Hello, world
- : int = 13

<Hello-C/>

Now that we've had a taste of ctypes, let's look at a more realistic
example: a program that defines bindings to the expat XML
parsing library, then uses them to display the structure of an XML
document.
We'll start by describing the types used by expat.  Since ctypes
represents C types as OCaml values, each of the types we need becomes
a value binding in our OCaml program.  The parser object involves an
incomplete (abstract) struct definition and a typedef for a pointer to
a struct:
struct xml_ParserStruct;
typedef xml_ParserStruct *xml_Parser;

In ctypes these become calls to the structure and ptr functions:
let parser_struct : [`XML_ParserStruct] structure typ = structure "xml_ParserStruct"
let xml_Parser = ptr parser_struct

Next, we'll use the type representations to bind some functions.  The
XML_ParserCreate
and
XML_ParserFree
functions construct and destroy parser objects.  As with puts, each
function binding involves a simple call to foreign:
let parser_create = foreign "XML_ParserCreate"
  (ptr void @-> returning xml_Parser)
let parser_free = foreign "XML_ParserFree"
  (xml_Parser @-> returning void)

Expat operates primarily through callbacks: when start and end elements are
encountered the parser invokes user-registered functions, passing the tag names
and attributes (along with a piece of user data):
typedef void (*start_handler)(void *, char *, char **);
typedef void (*end_handler)(void *, char *);

In ctypes function pointer types are built using the funptr function:
let start_handler =
  funptr (ptr void @-> string @-> ptr string @-> returning void)
let end_handler =
  funptr (ptr void @-> string @-> returning void)

We can use the start_handler and end_handler type representations to bind
XML_SetElementHandler, the callback-registration function:
let set_element_handler = foreign "XML_SetElementHandler"
  (xml_Parser @-> start_handler @-> end_handler @-> returning void)

The type that OCaml infers for set_element_handler reveals that the function
accepts regular OCaml functions as arguments, since the argument types are
normal OCaml function types:
val set_element_handler :
  [ `XML_ParserStruct ] structure ptr ->
  (unit ptr -> string -> string ptr -> unit) ->
  (unit ptr -> string -> unit) -> unit

There's one remaining function to bind, then we're ready to use the
library.  The
XML_Parse
function performs the actual parsing, invoking the callbacks when tags
are encountered:
let parse = foreign "XML_Parse"
  (xml_Parser @-> string @-> int @-> int @-> returning int)

As before, all the functions that we've bound are available for use
immediately.  We'll start by using them to define a more idiomatic OCaml entry
point to the library.  The parse_string function accepts the start and end
callbacks as labelled arguments, along with a string to parse:
let parse_string ~start_handler ~end_handler s =
  let p = parser_create null in
  let () = set_element_handler p start_handler end_handler in
  let _ = parse p s (String.length s) 1 in
  parser_free p

Using parse_string we can write a program that prints out the names of each
element in an XML document, indented according to nesting depth:
let depth = ref 0

let start_handler _ name _ =
  Printf.printf "%*s%s\n" (!depth * 3) "" name;
  incr depth

let end_handler _ _ =
  decr depth

let () =
  parse_string ~start_handler ~end_handler (In_channel.input_all stdin)

The full source of the program is available on github.
Here's the program in action:
$ ocamlfind opt -thread -package core,ctypes.foreign expat_example.ml \
   -linkpkg -cclib -lexpat -o expat_example
$ wget -q http://openmirage.org/blog/atom.xml -O /dev/stdout \
  | ./expat_example
feed
   id
   title
   subtitle
   rights
   updated
   link
   link
   contributor
      email
      uri
      name
[...]

Since this is just a high-level overview we've passed over a number of
details.  The interested reader can find a more comprehensive introduction to
using ctypes in Chapter 19: Foreign Function Interface of Real World OCaml.
Dynamic vs static

Up to this point we've been using a single function, foreign, to
make C functions available to OCaml.  Although foreign is simple to
use, there's quite a lot going on behind the scenes.  The two
arguments to foreign are used to dynamically construct an OCaml
function value that wraps the C function: the name is used to resolve
the code for the C function, and the type representation is used to
construct a call frame appropriate to the C types invovled and to the
underlying platform.
The dynamic nature of foreign that makes it convenient for
interactive use, also makes it unsuitable for some environments.
There are three main drawbacks:
Binding functions dynamically involves a certain loss of safety:
 since C libraries typically don't maintain information about the
 types of the functions they contain, there's no way to check whether
 the type representation passed to foreign matches the actual type of
 the C function.
Dynamically constructing calls introduces a certain interpretative
 overhead.  In mitigation, this overhead is much less than might be supposed,
 since much of the work can be done when the function is bound rather than
 when the call is made, and foreign has been used to bind C functions in
 performance-sensitive applications without problems.
The implementation of foreign uses a low-level library, libffi,
 to deal with calling conventions across platforms.  While libffi is mature
 and widely supported, it's not appropriate for use in every environment.
 For example, introducing such a (relatively) large and complex library into
 Mirage would compromise many of the benefits of writing the rest of the
 system in OCaml.


Happily, there's a solution at hand.  As the introduction hints, foreign is
one of a number of binding strategies, and OCaml's module system makes it easy
to defer the choice of which strategy to use when writing the actual code.
Placing the expat bindings in a functor (parameterised module) makes it
possible to abstract over the linking strategy:
module Bindings(F : FOREIGN) =
struct
  let parser_create = F.foreign "XML_ParserCreate"
    (ptr void @-> returning xml_Parser)
  let parser_free = F.foreign "XML_ParserFree"
    (xml_Parser @-> returning void)
  let set_element_handler = F.foreign "XML_SetElementHandler"
    (xml_Parser @-> start_handler @-> end_handler @-> returning void)
  let parse = F.foreign "XML_Parse"
    (xml_Parser @-> string @-> int @-> int @-> returning int)
end

The Bindings module accepts a single parameter of type FOREIGN, which
encodes the binding strategy to use.  Instantiating Bindings with a module
containing the foreign function used above recovers the
dynamically-constructed bindings that we've been using so far.  However, there
are now other possibilities available.  In particular, we can instantiate
Bindings with code generators that output code to expose the bound functions
to OCaml.  The actual instantiation is hidden behind a couple of convenient
functions, write_c and write_ml, which accept Bindings as a parameter
and write to a formatter:
Cstubs.write_c formatter ~prefix:"expat" ~bindings:(module Bindings)
Cstubs.write_ml formatter ~prefix:"expat" ~bindings:(module Bindings)

Generating code in this way eliminates the concerns associated with
constructing calls dynamically:
The C compiler checks the types of the generated calls against the C
 headers (the API), so the safety concerns associated with linking
 directly against the C library binaries (the ABI) don't apply.
There's no interpretative overhead, since the generated code is
 (statically) compiled.
The dependency on libffi disappears altogether.


How easy is it in practice to switch between dynamic and static
binding strategies?  It turns out that it's quite straightforward,
even for code that was originally written without parameterisation.
Bindings written using early releases of ctypes used the dynamic
strategy exclusively, since dynamic binding was then the only option
available.  The commit logs for projects that switched over to static
generation and linking (e.g. ocaml-lz4 and
async-ssl) when it became available show that
moving to the new approach involved only straightforward and localised
changes.
Local vs remote

Generating code is safer than constructing calls dynamically, since it
allows the C compiler to check the types of function calls against
declarations.  However, there are some safety problems that even C's
type checking doesn't detect.  For instance, the following call is
type correct (given suitable definitions of p and q), but is
likely to misbehave at run time:
memcpy(p, q, SIZE_MAX)

In contrast, code written purely in OCaml detects and prevents
attempts to write beyond the bounds of allocated objects:
# StringLabels.blit ~src ~dst ~src_pos:0 ~dst_pos:0 ~len:max_int;;
Exception: Invalid_argument "String.blit".

It seems a shame to weaken OCaml's safety guarantees by linking in C
code that can potentially write to any region of memory, but what is
the alternative?
One possibility is to use privilege separation to separate
trusted OCaml code from untrusted C functions.  The modular design of
ctypes means that privilege separation can be treated as one more
linking strategy: we can run C code in an entirely separate process
(or for Mirage/Xen, in a separate virtual machine), and instantiate
Bindings with a strategy that forwards calls to the process using
standard inter-process communication.  The remote calling strategy is
not supported in the current release of ctypes, but
it's scheduled for a future version.  As with the switch from dynamic
to static bindings, we anticipate that updating existing bindings to
use cross-process calls will be straightforward.
This introductory post should give you a sense of the power of the unikernel
approach in Mirage.  By turning the FFI into just another library (for the C
interface description) and protocol (for the linkage model), we can use code
generation to map application logic onto the privilege model most suitable for
the target hardware platform.  This starts with Unix processes, continues onto Xen
paravirtualization, and could even extend into CHERI fine-grained
compartmentalization.
Further examples

Although ctypes is a fairly new library, it's already in use in a
number of projects across a variety of domains: graphics,
multimedia, compression, cryptography,
security, geospatial data, communication,
and many others.  Further resources (documentation, forums, etc.) are
available via the home page.

   Hide
        
      
                    by Jeremy Yallop at Jul 15, 2014 
      
      
    
  


       
          OCaml-TLS: the protocol implementation and mitigations to known attacks
      (Mirage OS)
    
                                      This is the fifth in a series of posts that introduce new libraries for a pure OCaml implementation of TLS.
You might like to begin with the introduction.
ocaml-tls is the new, clean-slate implementation of TLS in OCaml
that we've been working on for the past six months. In this post we
try to document some of its internal design, the reasons for the
decisions we made, and the current security status of that work. Try
our live interactive demonstration server which visualises TLS
sessions.
The OCaml-TLS architecture

The OCaml ecosystem has several distinct ways of interacting with the outside world
(and the network in particular): straightforward unix interfaces
and the asynchronous programming libraries lwt and async. One of the
early considerations was not to restrict ourselves to any of those -- we wanted
to support them all.
There were also two distinct basic "platforms" we wanted to target from the
outset: the case of a simple executable, and the case of Mirage unikernels.Read more...      This is the fifth in a series of posts that introduce new libraries for a pure OCaml implementation of TLS.
You might like to begin with the introduction.
ocaml-tls is the new, clean-slate implementation of TLS in OCaml
that we've been working on for the past six months. In this post we
try to document some of its internal design, the reasons for the
decisions we made, and the current security status of that work. Try
our live interactive demonstration server which visualises TLS
sessions.
The OCaml-TLS architecture

The OCaml ecosystem has several distinct ways of interacting with the outside world
(and the network in particular): straightforward unix interfaces
and the asynchronous programming libraries lwt and async. One of the
early considerations was not to restrict ourselves to any of those -- we wanted
to support them all.
There were also two distinct basic "platforms" we wanted to target from the
outset: the case of a simple executable, and the case of Mirage unikernels.
So one of the first questions we faced was deciding how to represent
interactions with the network in a portable way. This can be done by
systematically abstracting out the API boundary which gives access to network
operations, but we had a third thing in mind as well: we wanted to exploit the
functional nature of OCaml to its fullest extent!
Our various prior experiences with Haskell and Idris convinced us to adopt
what is called "purely functional" technique. We believe it to be an approach
which first forces the programmer to give principled answers to all the
difficult design questions (errors and global data-flow) in advance, and then
leads to far cleaner and composable code later on. A purely functional system
has all the data paths made completely explicit in the form of function
arguments and results. There are no unaccounted-for interactions between
components mediated by shared state, and all the activity of the parts of the
system is exposed through types since, after all, it's only about computing
values from values.
For these reasons, the library is split into two parts: the directory /lib
(and the corresponding findlib package tls) contains the core TLS logic, and
/mirage and /lwt (packaged as tls.mirage and tls.lwt respectively)
contain front-ends that tie the core to Mirage and Lwt_unix.
Core

The core library is purely functional. A TLS session is represented by the
abstract type Tls.Engine.state, and various functions consume this session
type together with raw bytes (Cstruct.t -- which is by itself mutable, but
ocaml-tls eschews this) and produce new session values and resulting buffers.
The central entry point is handle_tls, which transforms an input state and a
buffer to an output state, a (possibly empty) buffer to send to the
communication partner, and an optional buffer of data intended to be received by
the application:
type state

type ret = [
  | `Ok of [ `Ok of state | `Eof | `Alert of alert ] *
      [ `Response of Cstruct.t ] * [ `Data of Cstruct.t option ]
  | `Fail of alert * [ `Response of Cstruct.t ]
]

val handle_tls : state -> Cstruct.t -> ret

As the signature shows, errors are signalled through the ret type, which is a polymorphic variant. This
reflects the actual internal structure: all the errors are represented as
values, and operations are composed using an error monad.
Other entry points share the same basic behaviour: they transform the prior
state and input bytes into the later state and output bytes.
Here's a rough outline of what happens in handle_tls:
TLS packets consist of a header, which contains the protocol
 version, length, and content type, and the payload of the given
 content type. Once inside our main handler, we
 separate the buffer into TLS records, and
 process each individually. We first check that
 the version number is correct, then decrypt, and verify
  the mac.
Decrypted data is then dispatched to one of four
 sub-protocol handlers (Handshake, Change Cipher Spec, Alert and
 Application Data). Each handler can return a new
 handshake state, outgoing data, application data, the new decryption
 state or an error (with the outgoing data being an interleaved list
 of buffers and new encryption states).
The outgoing buffers and the encryption states are
 traversed to produce the final output to be sent to the
 communication partner, and the final encryption, decryption and
 handshake states are combined into a new overall state which is
 returned to the caller.


Handshake is (by far) the most complex TLS sub-protocol, with an elaborate state
machine. Our client and server encode
this state as a "flat" sum type, with exactly one incoming
message allowed per state. The handlers first parse the
handshake packet (which fails in case of malformed or unknown data) and then
dispatch it to the handling function. The handshake state is
carried around and a fresh one is returned from the handler in case it needs
updates. It consists of a protocol version, the handshake state, configuration,
renegotiation data, and possibly a handshake fragment.
Logic of both handshake handlers is very localised, and does not mutate any
global data structures.
Core API

OCaml permits the implementation a module to be exported via a more
abstract signature that hides the internal representation
details. Our public API for the core library consists of the
Tls.Engine and Tls.Config modules.
Tls.Engine contains the basic reactive function handle_tls, mentioned above,
which processes incoming data and optionally produces a response, together with
several operations that allow one to initiate message transfer like
send_application_data (which processes application-level messages for
sending), send_close_notify (for sending the ending message) and reneg
(which initiates full TLS renegotiation).
The module also contains the only two ways to obtain the initial state:
val client : Config.client -> (state * Cstruct.t)
val server : Config.server -> state

That is, one needs a configuration value to create it. The Cstruct.t
that client emits is the initial Client Hello since in TLS,
the client starts the session.
Tls.Config synthesizes configurations, separately for client and server
endpoints, through the functions client_exn and server_exn. They take a
number of parameters that define a TLS session, check them for consistency, and
return the sanitized config value which can be used to create a state and,
thus, a session. If the check fails, they raise an exception.
The parameters include the pair of a certificate and its private key for the
server, and an X509.Authenticator.t for the client, both produced by our
ocaml-x509 library and described in a previous article.
This design reflects our attempts to make the API as close to "fire and forget"
as we could, given the complexity of TLS: we wanted the library to be relatively
straightforward to use, have a minimal API footprint and, above all, fail very
early and very loudly when misconfigured.
Effectful front-ends

Clearly, reading and writing network data does change the state of the world.
Having a pure value describing the state of a TLS session is not really useful
once we write something onto the network; it is certainly not the case that we
can use more than one distinct state to process further data, as only one
value is in sync with the other endpoint at any given time.
Therefore we wrap the core types into stateful structures loosely inspired by
sockets and provide IO operations on those. The structures of mirage and lwt
front-ends mirror one another.
In both cases, the structure is pull-based in the sense that no processing is
done until the client requires a read, as opposed to a callback-driven design
where the client registers a callback and the library starts spinning in a
listening loop and invoking it as soon as there is data to be processed. We do
this because in an asynchronous context, it is easy to create a callback-driven
interface from a demand-driven one, but the opposite is possible only with
unbounded buffering of incoming data.
One exception to demand-driven design is the initial session creation: the
library will only yield the connection after the first handshake is over,
ensuring the invariant that it is impossible to interact with a connection if it
hasn't already been fully established.
Mirage
The Mirage interface matches the FLOW
signature (with additional TLS-specific operations). We provide a functor that
needs to be applied to an underlying TCP module, to obtain a TLS transport on
top. For example:
module Server (Stack: STACKV4) (Entropy: ENTROPY) (KV: KV_RO) =
struct

  module TLS  = Tls_mirage.Make (Stack.TCPV4) (Entropy)
  module X509 = Tls_mirage.X509 (KV) (Clock)

  let accept conf flow =
    TLS.server_of_tcp_flow conf flow >>= function
    | `Ok tls ->
      TLS.read tls >>= function
      | `Ok buf ->
        TLS.write tls buf >>= fun () -> TLS.close buf

  let start stack e kv =
    TLS.attach_entropy e >>= fun () ->
    lwt authenticator = X509.authenticator kv `Default in
    let conf          = Tls.Config.server_exn ~authenticator () in
    Stack.listen_tcpv4 stack 4433 (accept conf) ;
    Stack.listen stack

end

Lwt
The lwt interface has two layers. Tls_lwt.Unix is loosely based
on read/write operations from Lwt_unix and provides in-place update of
buffers. read, for example, takes a Cstruct.t to write into and returns the
number of bytes read. The surrounding module, Tls_lwt, provides a simpler,
Lwt_io-compatible API built on top:
let main host port =
  Tls_lwt.rng_init () >>= fun () ->
  lwt authenticator = X509_lwt.authenticator (`Ca_dir nss_trusted_ca_dir) in
  lwt (ic, oc)      = Tls_lwt.connect ~authenticator (host, port) in
  let req = String.concat "\r\n" [
    "GET / HTTP/1.1" ; "Host: " ^ host ; "Connection: close" ; "" ; ""
  ] in
  Lwt_io.(write oc req >>= fun () -> read ic >>= print)

We have further plans to provide wrappers for `Async` and plain `Unix` in a
similar vein.
Attacks on TLS

TLS the most widely deployed security protocol on the Internet and, at
over 15 years, is also showing its age. As such, a flaw is a valuable
commodity due to the commercially sensitive nature of data that is
encrypted with TLS. Various vulnerabilities on different layers of TLS
have been found - heartbleed and others are implementation
specific, advancements in cryptanalysis such as collisions of
MD5 lead to vulnerabilities, and even others are due
to incorrect usage of TLS (truncation attack or
BREACH). Finally, some weaknesses are in the protocol
itself. Extensive overviews of attacks on
TLS are available.
We look at protocol level attacks of TLS and how ocaml-tls
implements mitigations against these.  TLS 1.2 RFC provides an
overview of attacks and mitigations, and we track our progress in
covering them. This is slightly out of date as the RFC is roughly six years old and
in the meantime more attacks have been published, such as the renegotiation
flaw.
As already mentioned, we track all our
mitigated and open security issues on our GitHub
issue tracker.
Due to the choice of using OCaml, a memory managed programming
language, we obstruct entire bug classes, namely temporal and spatial
memory safety.
Cryptanalysis and improvement of computational power weaken some
ciphers, such as RC4 and 3DES (see issue 8 and issue
10). If we phase these two ciphers out, there wouldn't be
any matching ciphersuite left to communicate with some compliant TLS-1.0
implementations, such as Windows XP, that do not support AES.
Timing attacks
When the timing characteristics between the common case and the error
case are different, this might potentially leak confidential
information. Timing is a very prominent side-channel and there are a huge
variety of timing attacks on different layers, which are observable by
different attackers. Small differences in timing behaviour might
initially be exploitable only by a local attacker, but advancements to
the attack (e.g. increasing the number of tests) might allow a 
remote attacker to filter the noise and exploit the different timing
behaviour.
Timing of cryptographic primitives
We already mentioned cache timing
attacks on our AES implementation, and that we use blinding
techniques to mitigate RSA timing attacks.
By using a memory managed programming language, we open the attack
vector of garbage collector (GC) timing attacks (also mentioned in
our nocrypto introduction).
Furthermore, research has been done on virtual machine side channels
(l3, cross vm and cache timing), which we
will need to study and mitigate appropriately.
For the time being we suggest to not use the stack on a multi-tenant
shared host or on a shared host which malicious users might have
access to.
Bleichenbacher
In 1998, Daniel Bleichenbacher discovered a timing flaw in the
PKCS1 encoding of the premaster secret: the TLS server
failed faster when the padding was wrong than when the decryption
failed. Using this timing, an attacker can run an adaptive chosen
ciphertext attack and find out the plain text of a PKCS1 encrypted
message. In TLS, when RSA is used as the key exchange method, this
leads to discovery of the premaster secret, which is used to derive the
keys for the current session.
The mitigation is to have both padding and decryption failures use the
exact same amount of time, thus there should not be any data-dependent
branches or different memory access patterns in the code. We
implemented this mitigation in Handshake_server.
Padding oracle and CBC timing
Vaudenay discovered a vulnerability involving block ciphers: if an
attacker can distinguish between bad mac and bad padding, recovery of
the plaintext is possible (within an adaptive chosen ciphertext
attack). Another approach using the same issue is to use
timing information instead of separate error messages.
Further details are described here.
The countermeasure, which we implement here, is to continue
with the mac computation even though the padding is
incorrect. Furthermore, we send the same alert (bad_record_mac)
independent of whether the padding is malformed or the mac is
incorrect.
Lucky 13
An advancement of the CBC timing attack was discovered in 2013, named
Lucky 13. Due to the fact that the mac is computed over the
plaintext without padding, there is a slight (but measurable)
difference in timing between computing the mac of the plaintext and
computing the fake mac of the ciphertext. This leaks information. We
do not have proper mitigation against Lucky 13 in place yet.  You can
find further discussion in issue 7 and pull request
49.
Renegotiation not authenticated
In 2009, Marsh Ray published a vulnerability of the TLS protocol which
lets an attacker prepend arbitrary data to a session due to
unauthenticated renegotiation. The attack
exploits the fact that a renegotiation of ciphers and key material is
possible within a session, and this renegotiated handshake is not
authenticated by the previous handshake. A man in the middle can
initiate a session with a server, send some data, and hand over the
session to a client. Neither the client nor the server can detect the
man in the middle.
A fix for this issue is the secure renegotiation extension,
which embeds authenticated data of the previous handshake into the
client and server hello messages. Now, if a man in the middle
initiates a renegotiation, the server will not complete it due to
missing authentication data (the client believes this is the first
handshake).
We implement and require the secure renegotiation extension by
default, but it is possible to configure ocaml-tls to not require
it -- to be able to communicate with servers and
clients which do not support this extension.
Implementation of the mitigation is on the server side in
ensure_reneg and on the client side in validate_reneg. The
data required for the secure renegotiation is stored in
`handshake_state` while sending and receiving Finished
messages. You can find further discussion in issue 3.
TLS 1.0 and known-plaintext (BEAST)
TLS 1.0 reuses the last ciphertext block as IV in CBC mode. If an attacker
has a (partially) known plaintext, she can find the remaining plaintext.
This is known as the BEAST attack and there is a long discussion
about mitigations. Our mitigation is to prepend each TLS-1.0
application data fragment with an empty fragment to randomize the IV.
We do this exactly here. There is further discussion in
issue 2.
Our mitigation is slightly different from the 1/n-1 splitting proposed
here: we split every application data frame into a 0 byte
and n byte frame, whereas they split into a 1 byte and a n-1 byte
frame.
Researchers have exploited this vulnerability in 2011, although it was
known since 2006. TLS versions 1.1 and 1.2 use an explicit IV,
instead of reusing the last cipher block on the wire.
Compression and information leakage (CRIME)
When using compression on a chosen-plaintext, encrypting this can leak
information, known as CRIME. BREACH furthermore
exploits application layer compression, such as HTTP compression. We
mitigate CRIME by not providing any TLS compression support, while we
cannot do anything to mitigate BREACH.
Traffic analysis
Due to limited amount of padding data, the actual size of transmitted
data can be recovered. The mitigation is to implement length hiding
policies. This is tracked as issue 162.
Version rollback
SSL-2.0 is insecure, a man in the middle can downgrade the version to
SSL-2.0. The mitigation we implement is that we do not support
SSL-2.0, and thus cannot be downgraded. Also, we check that the
version of the client hello matches the first two bytes in the
premaster secret here. You can find further discussion in
issue 5.
Triple handshake
A vulnerability including session resumption and renegotiation was
discovered by the miTLS team, named triple
handshake.  Mitigations include disallowing renegotiation,
disallowing modification of the certificate during renegotiation, or
a hello extension. Since we do not support session resumption yet, we
have not yet implemented any of the mentioned mitigations. There is
further discussion in issue 9.
Alert attack
A fragment of an alert can be sent by a man in the
middle during the initial handshake. If the fragment is not cleared
once the handshake is finished, the authentication of alerts is
broken. This was discovered in 2012; our mitigation is to discard
fragmented alerts.
EOF.

Within six months, two hackers managed to develop a clean-slate TLS
stack, together with required crypto primitives, ASN.1, and X.509
handling, in a high-level pure language. We interoperate with widely
deployed TLS stacks, as shown by our demo server.  The code
size is nearly two orders of magnitude smaller than OpenSSL, the most
widely used open source library (written in C, which a lot of
programming languages wrap instead of providing their own TLS
implementation). Our code base seems to be robust -- the demo
server successfully finished over 22500 sessions in less than a
week, with only 11 failing traces.
There is a huge need for high quality TLS implementations, because
several TLS implementations suffered this year from severe security
problems, such as heartbleed, goto fail, session
id, Bleichenbacher, change cipher
suite and GCM DoS. The main cause is
implementation complexity due to lack of abstraction, and memory
safety issues.
We still need to address some security issues, and improve our performance. We
invite people to do rigorous code audits (both manual and automated) and try
testing our code in their services.
Please be aware that this release is a beta and is missing external code audits.
It is not yet intended for use in any security critical applications.
Acknowledgements

Since this is the final post in our series, we would like to thank all
people who reported issues so far: Anil Madhavapeddy, Török
Edwin, Daniel Bünzli, Andreas Bogk, Gregor Kopf, Graham
Steel, Jerome Vouillon, Amir Chaudhry,
Ashish Agarwal. Additionally, we want to thank the
miTLS team (especially Cedric and Karthikeyan) for fruitful
discussions, as well as the OCaml Labs and
Mirage teams. And thanks to Peter Sewell and
Richard Mortier for funding within the REMS, UCN, and Horizon
projects. The software was started in Aftas beach house in
Mirleft, Morocco.

Posts in this TLS series:
Introducing transport layer security (TLS) in pure OCamlOCaml-TLS: building the nocrypto library coreOCaml-TLS: adventures in X.509 certificate parsing and validationOCaml-TLS: ASN.1 and notation embeddingOCaml-TLS: the protocol implementation and mitigations to known attacks


   Hide
        
      
                    by David Kaloper at Jul 14, 2014 
      
      
    
  


       
          OCaml-TLS: ASN.1 and notation embedding
      (Mirage OS)
    
                                      This is the fourth in a series of posts that introduce new libraries for a pure OCaml implementation of TLS.
You might like to begin with the introduction.
asn1-combinators is a library that allows one to express
ASN.1 grammars directly in OCaml, manipulate them as first-class entities,
combine them with one of several ASN encoding rules and use the result to parse
or serialize values.
It is the parsing and serialization backend for our X.509
certificate library, which in turn provides certificate handling for
ocaml-tls.
We wrote about the X.509 certificate handling yesterday.
What is ASN.1, really?

ASN.1 (Abstract Syntax Notation, version one) is a way to describe
on-the-wire representation of messages. It is split into two components: a way
to describe the content of a message, i.e. a notation for its abstract syntax,
and a series of standard encoding rules that define the exact byte
representations of those syntaxes. It is defined in ITU-T standards X.680-X.683
and X.690-X.6…Read more...      This is the fourth in a series of posts that introduce new libraries for a pure OCaml implementation of TLS.
You might like to begin with the introduction.
asn1-combinators is a library that allows one to express
ASN.1 grammars directly in OCaml, manipulate them as first-class entities,
combine them with one of several ASN encoding rules and use the result to parse
or serialize values.
It is the parsing and serialization backend for our X.509
certificate library, which in turn provides certificate handling for
ocaml-tls.
We wrote about the X.509 certificate handling yesterday.
What is ASN.1, really?

ASN.1 (Abstract Syntax Notation, version one) is a way to describe
on-the-wire representation of messages. It is split into two components: a way
to describe the content of a message, i.e. a notation for its abstract syntax,
and a series of standard encoding rules that define the exact byte
representations of those syntaxes. It is defined in ITU-T standards X.680-X.683
and X.690-X.695.
The notation itself contains primitive grammar elements, such as BIT STRING or
GeneralizedTime, and constructs that allow for creation of compound grammars
from other grammars, like SEQUENCE. The notation is probably best introduced
through a real-world example:
-- Simple name bindings
UniqueIdentifier ::= BIT STRING

-- Products
Validity ::= SEQUENCE {
  notBefore Time,
  notAfter  Time
}

-- Sums
Time ::= CHOICE {
  utcTime     UTCTime,
  generalTime GeneralizedTime
}

(Example from RFC 5280, the RFC that describes X.509
certificates which heavily rely on ASN.)
The first definition shows that we can introduce an alias for any existing ASN
grammar fragment, in this case the primitive BIT STRING. The second and third
definitions are, at least morally, a product and a sum.
At their very core, ASN grammars look roughly like algebraic data types, with a
range of pre-defined primitive grammar fragments like BIT STRING, INTEGER,
NULL, BOOLEAN or even GeneralizedTime, and a number of combining
constructs that can be understood as denoting sums and products.
Definitions such as the above are arranged into named modules. The standard even
provides for some abstractive capabilities: initially just a macro facility, and
later a form of parameterized interfaces.
To facilitate actual message transfer, a grammar needs to be coupled with an
encoding. By far the most relevant ones are Basic Encoding Rules (BER) and
Distinguished Encoding Rules (DER), although other encodings exist.
BER and DER are tag-length-value (TLV) encodings, meaning that every value is
encoded as a triplet containing a tag that gives the interpretation of its
contents, a length field, and the actual contents which can in turn contain
other TLV triplets.
Let's drop the time from the example above, as time encoding is a little
involved, and assume a simpler version for a moment:
Pair ::= SEQUENCE {
  car Value,
  cdr Value
}

Value ::= CHOICE {
  v_str UTF8String,
  v_int INTEGER
}

Then two possible BER encodings of a Pair ("foo", 42) are:
  30         - SEQUENCE            30         - SEQUENCE
  08         - length              0c         - length
  [ 0c       - UTF8String          [ 2c       - UTF8String, compound
    03       - length                07       - length
    [ 66     - 'f'                   [ 0c     - UTF8String
      6f     - 'o'                     01     - length
      6f ]   - 'o'                     [ 66 ] - 'f'
    02       - INTEGER                 0c     - UTF8String
    01       - length                  02     - length
    [ 2a ] ] - 42                      [ 6f   - 'o'
                                         6f ] - 'o'
                                     02       - INTEGER
                                     01       - length
                                     [ 2a ] ] - 42

The left one is also the only valid DER encoding of this value: BER allows
certain freedoms in encoding, while DER is just a BER subset without those
freedoms. The property of DER that any value has exactly one encoding is useful,
for example, when trying to digitally sign a value.
If this piqued your curiosity about ASN, you might want to take a detour and
check out this excellent writeup.
A bit of history

The description above paints a picture of a technology a little like Google's
Protocol Buffers or Apache Thrift: a way to declaratively
specify the structure of a set of values and derive parsers and serializers,
with the addition of multiple concrete representations.
But the devil is in the detail. For instance, the examples above intentionally
gloss over the fact that often concrete tag values leak into
the grammar specifications for various disambiguation reasons. And ASN has more
than 10 different string types, most of which use
long-obsolete character encodings. Not to mention that the full standard is
close to 200 pages of relatively dense language and quite difficult to
follow. In general, ASN seems to have too many features for the relatively
simple task it is solving, and its specification has evolved over decades, apparently
trying to address various other semi-related problems, such as providing a
general Interface Description Language.
Which is to say, ASN is probably not what you are looking for. So why
implement it?
Developed in the context of the telecom industry around 30 years ago, modified
several times after that and apparently suffering from a lack of a coherent
goal, by the early 90s ASN was still probably the only universal, machine- and
architecture-independent external data representation.
So it came easily to hand around the time RSA Security started publishing its
series of PKCS standards, aimed at the standardization of
cryptographic material exchange. RSA keys and digital signatures are often
exchanged ASN-encoded.
At roughly the same time, ITU-T started publishing the X.500 series
of standards which aimed to provide a comprehensive directory service. Much of
this work ended up as LDAP, but one little bit stands out in particular: the
X.509 PKI certificate.
So a few years later, when Netscape tried to build an authenticated and
confidential layer to tunnel HTTP through, they based it on -- amongst other
things -- X.509 certificates. Their work went through several revisions as SSL
and was finally standardized as TLS. Modern TLS still requires X.509.
Thus, even though TLS uses ASN only for encoding certificates (and the odd PKCS1
signature), every implementation needs to know how to deal with ASN. In fact,
many other general cryptographic libraries also need to deal with ASN, as various PKCS
standards mandate ASN as the encoding for exchange of cryptographic material.
The grammar of the grammar

As its name implies, ASN was meant to be used with a specialized compiler. ASN
is really a standard for writing down abstract syntaxes, and ASN compilers
provided with the target encoding will generate code in your programming
language of choice that, when invoked, parses to or serializes from ASN.
As long as your programming language of choice is C, C++, Java or C#, obviously
-- there doesn't seem to be one freely available that targets OCaml. In any case, generating code for such a high-level language feels wrong somehow. In
its effort to be language-neutral, ASN needs to deal with things like modules,
abstraction and composition. At this point, most functional programmers reading
this are screaming: "I already have a language that can deal with modules,
abstraction and composition perfectly well!"
So we're left with implementing ASN in OCaml.
One strategy is to provide utility functions for parsing elements of ASN and
simply invoke them in the appropriate order, as imposed by the target grammar.
This amounts to hand-writing the parser and is what TLS libraries in C
typically do.
As of release 1.3.7, PolarSSL includes ~7,500 lines of rather
beautifully written C, that implement a specialized parser for dealing with
X.509. OpenSSL's libcrypto contains ~50,000 lines of C in its
'asn1', 'x509' and
'x509v3' directories, and primarily deals with X.509
specifically as required by TLS.
In both cases, low-level control flow is intertwined with the parsing logic and,
above the ASN parsing level, the code that deals with interpreting the ASN
structure is not particularly concise.
It is certainly a far cry from the (relatively)
simple grammar description ASN itself provides.
Since in BER every value fully describes itself, another strategy is to parse
the input stream without reference to the grammar. This produces a value that
belongs to the general type of all ASN-encoded trees, after which we need to
process the structure according to the grammar. This is similar to a common
treatment of JSON or XML, where one decouples parsing of bytes from the
higher-level concerns about the actual structure contained therein. The problem
here is that either the downstream client of such a parser needs to constantly
re-check whether the parts of the structure it's interacting with are really
formed according to the grammar (probably leading to a tedium of
pattern-matches), or we have to turn around and solve the parsing problem
again, mapping the uni-typed contents of a message to the actual, statically
known structure we require the message to have.
Surely we can do better?
LAMBDA: The Ultimate Declarative

Again, ASN is a language with a number of built-in primitives, a few combining
constructs, (recursive) name-binding and a module system. Our target language is
a language with a perfectly good module system and it can certainly express
combining constructs. It includes an abstraction mechanism arguably far simpler
and easier to use than those of ASN, namely, functions. And the OCaml compilers
can already parse OCaml sources. So why not just reuse this machinery?
The idea is familiar. Creating embedded languages for highly declarative
descriptions within narrowly defined problem spaces is the staple of functional
programming. In particular, combinatory parsing has been known, studied and
used for decades.
However, we also have to diverge from traditional parser combinators in two major ways.
Firstly, a single grammar expression needs to be able to generate
different concrete parsers, corresponding to different ASN encodings. More
importantly, we desire our grammar descriptions to act bidirectionally,
producing both parsers and complementary deserializers.
The second point severely restricts the signatures we can support. The usual
monadic parsers are off the table because the expression such as:
( (pa : a t) >>= fun (a : a) ->
  (pb : b t) >>= fun (b : b) ->
  return (b, b, a) ) : (b * b * a) t

... "hides" parts of the parser inside the closures, especially the method of
mapping the parsed values into the output values, and can not be run "in
reverse" [1].
We have a similar problem with applicative functors:
( (fun a b -> (b, b, a))
  <$> (pa : a t)
  <*> (pb : b t) ) : (b * b * a) t

(Given the usual <$> : ('a -> 'b) -> 'a t -> 'b t and <*> : ('a -> 'b) t ->
'a t -> 'b t.) Although the elements of ASN syntax are now exposed, the process
of going from intermediate parsing results to the result of the whole is still
not accessible.
Fortunately, due to the regular structure of ASN, we don't really need the
full expressive power of monadic parsing. The only occurrence of sequential
parsing is within SEQUENCE and related constructs, and we don't need
look-ahead. All we need to do is provide a few specialized combinators to handle
those cases -- combinators the likes of which would be derived in a
more typical setting.
So if we imagine we had a few values, like:
val gen_time : gen_time t
val utc_time : utc_time t
val choice   : 'a t -> 'b t -> ('a, 'b) choice t
val sequence : 'a t -> 'b t -> ('a * 'b) t

Assuming appropriate OCaml types gen_time and utc_time that reflect their
ASN counterparts, and a simple sum type choice, we could express the
Validity grammar above using:
type time = (gen_time, utc_time) choice
let time     : time t          = choice gen_time utc_time
let validity : (time * time) t = sequence time time

In fact, ASN maps quite well to algebraic data types. Its SEQUENCE corresponds
to n-ary products and CHOICE to sums. ASN SET is a lot like SEQUENCE,
except the elements can come in any order; and SEQUENCE_OF and SET_OF are
just lifting an 'a-grammar into an 'a list-grammar.
A small wrinkle is that SEQUENCE allows for more contextual information on its
components (so does CHOICE in reality, but we ignore that): elements can carry
labels (which are not used for parsing) and can be marked as optional. So
instead of working directly on the grammars, our sequence must work on their
annotated versions. A second wrinkle is the arity of the sequence combinator.
Thus we introduce the type of annotated grammars, 'a element, which
corresponds to one ,-delimited syntactic element in ASN's own SEQUENCE
grammar, and the type 'a sequence, which describes the entire contents ({ ...
}) of a SEQUENCE definition:
val required : 'a t -> 'a element
val optional : 'a t -> 'a option element
val ( -@ )   : 'a element -> 'b element -> ('a * 'b) sequence
val ( @ )    : 'a element -> 'a sequence -> ('a * 'b) sequence
val sequence : 'a sequence -> 'a t

The following are then equivalent:
Triple ::= SEQUENCE {
  a INTEGER,
  b BOOLEAN,
  c BOOLEAN OPTIONAL
}

let triple : (int * (bool * bool option)) t =
  sequence (
      required int
    @ required bool
   -@ optional bool
  )

We can also re-introduce functions, but in a controlled manner:
val map : ('a -> 'b) -> ('b -> 'a) -> 'a t -> 'b t

Keeping in line with the general theme of bidirectionality, we require functions
to come in pairs. The deceptively called map could also be called iso, and
comes with a nice property: if the two functions are truly inverses,
the serialization process is fully reversible, and so is parsing, under
single-representation encodings (DER)!
ASTs of ASNs

To go that last mile, we should probably also implement what we discussed.
Traditional parser combinators look a little like this:
type 'a p = string -> 'a * string

let bool : bool p = fun str -> (s.[0] <> "\000", tail_of_string str)

Usually, the values inhabiting the parser type are the actual parsing functions,
and their composition directly produces larger parsing functions. We would
probably need to represent them with 'a p * 'a s, pairs of a parser and its
inverse, but the same general idea applies.
Nevertheless, we don't want to do this.
The grammars need to support more than one concrete
parser/serializer, and composing what is common between them and extracting out
what is not would probably turn into a tangled mess. That is one reason. The other is that if we encode the grammar purely as
(non-function) value, we can traverse it for various other purposes.
So we turn from what is sometimes called "shallow embedding" to "deep
embedding" and try to represent the grammar purely as an algebraic data type.
Let's try to encode the parser for bools, boolean : bool t:
type 'a t =
  | Bool
  ...

let boolean : bool t = Bool

Unfortunately our constructor is fully polymorphic, of type 'a. 'a t. We can
constrain it for the users, but once we traverse it there is nothing left to
prove its intended association with booleans!
Fortunately, starting with the release of OCaml 4.00.0,
OCaml joined the ranks of
languages equipped with what is probably the supreme tool of deep embedding,
GADTs. Using them, we can do things like:
type _ t =
  | Bool   : bool t
  | Pair   : ('a t * 'b t) -> ('a * 'b) t
  | Choice : ('a t * 'b t) -> ('a, 'b) choice t
  ...

In fact, this is very close to how the library is actually
implemented.
There is only one thing left to worry about: ASN definitions can be recursive.
We might try something like:
let rec list = choice null (pair int list)

But this won't work. Being just trees of applications, our definitions never
contain statically constructive parts -- this expression could never
terminate in a strict language.
We can get around that by wrapping grammars in Lazy.t (or just closures), but
this would be too awkward to use. Like many other similar libraries, we need to
provide a fixpoint combinator:
val fix : ('a t -> 'a t) -> 'a t

And get to write:
let list = fix @@ fun list -> choice null (pair int list)

This introduces a small problem. So far we simply reused binding inherited
from OCaml without ever worrying about identifiers and references, but with a
fixpoint, the grammar encodings need to be able to somehow express a cycle.
Borrowing an idea from higher-order abstract syntax, we can represent the entire
fixpoint node using exactly the function provided to define it, re-using OCaml's
own binding and identifier resolution:
type _ t =
  | Fix : ('a t -> 'a t) -> 'a t
  ...

This treatment completely sidesteps the problems with variables. We need no
binding environments or De Brujin indices, and need not care about the desired
scoping semantics. A little trade-off is that with this simple encoding it
becomes more difficult to track cycles (when traversing the AST, if we keep
applying a Fix node to itself while descending into it, it looks like an
infinite tree), but with a little opportunistic caching it all plays out well
[2].
The parser and serializer proper then emerge as interpreters for
this little language of typed trees, traversing them with an input string, and
parsing it in a fully type-safe manner.
How does it play out?

The entire ASN library comes down to ~1,700 lines of OCaml, with around ~1,100
more in tests, giving a mostly-complete treatment of BER and DER.
Its main use so far is in the context of the X.509 library
(discussed yesterday). It allowed the
grammar of certificates and RSA keys, together with a number of transformations
from the raw types to more pleasant, externally facing ones, to be written in
~900 lines of OCaml. And the code looks a lot like the
actual standards the grammars were taken from -- the fragment from the beginning
of this article becomes:
let unique_identifier = bit_string_cs

let time =
  map (function `C1 t -> t | `C2 t -> t) (fun t -> `C2 t)
      (choice2 utc_time generalized_time)

let validity =
  sequence2
    (required ~label:"not before" time)
    (required ~label:"not after"  time)

We added ~label to 'a element-forming injections, and have:
val choice2 : 'a t -> 'b t -> [ `C1 of 'a | `C2 of 'b ] t

To get a sense of how the resulting system eases the translation of standardized
ASN grammars into working code, it is particularly instructive to compare
these two definitions.
Reversibility was a major simplifying factor during development. Since the
grammars are traversable, it is easy to generate their random
inhabitants, encode them, parse the result and verify the reversibility still
holds. This can't help convince us the parsing/serializing pair
is actually correct with respect to ASN, but it gives a simple tool to generate
large amounts of test cases and convince us that that pair is equivalent. A
number of hand-written cases then check the conformance to the actual ASN.
As for security, there were two concerns we were aware of. There is a history of
catastrophic buffer overruns in some ASN.1 implementations,
but -- assuming our compiler and runtime system are correct -- we are immune to
these as we are subject to bounds-checking. And
there are some documented problems with security of X.509
certificate verification due to overflows of numbers in ASN OID types, which we
explicitly guard against.
You can check our security status on our issue tracker.
Footnotes

  In fact, the problem with embedding functions in
 combinator languages, and the fact that in a functional language it is not
 possible to extract information from a function other than by applying it,
 was discussed more than a decade ago. Such discussions led to the development of
 Arrows, amongst other things.
  Actually, a version of the library used the more
 proper encoding to be able to inject results of reducing
 referred-to parts of the AST into the referring sites directly, roughly
 like Fix : ('r -> ('a, 'r) t) -> ('a, 'r) t. This approach was abandoned because terms need to be polymorphic in 'r, and this becomes
 impossible to hide from the user of the library, creating unwelcome noise.


Posts in this TLS series:
Introducing transport layer security (TLS) in pure OCamlOCaml-TLS: building the nocrypto library coreOCaml-TLS: adventures in X.509 certificate parsing and validationOCaml-TLS: ASN.1 and notation embeddingOCaml-TLS: the protocol implementation and mitigations to known attacks


   Hide
        
      
                    by David Kaloper at Jul 11, 2014 
      
      
    
  


       
          OCaml-TLS: Adventures in X.509 certificate parsing and validation
      (Mirage OS)
    
                                      This is the third in a series of posts that introduce new libraries for a pure OCaml implementation of TLS.
You might like to begin with the introduction.
The problem of authentication

The authenticity of the remote server needs to be verified while
establishing a secure connection to it, or else an
attacker (MITM) between the client and the server can eavesdrop on
the transmitted data. To the best of our knowledge, authentication
cannot be done solely in-band, but needs external
infrastructure. The most common methods used in practice rely on
public key encryption.
Web of trust (used by OpenPGP) is a decentralised public key
infrastructure. It relies on out-of-band verification of public keys
and transitivity of trust. If Bob signed Alice's public key, and
Charlie trusts Bob (and signed his public key), then Charlie can trust
that Alice's public key is hers.
Public key infrastructure (used by TLS) relies on trust
anchors which are communicated out-of-band (e.g. distributed wit…Read more...      This is the third in a series of posts that introduce new libraries for a pure OCaml implementation of TLS.
You might like to begin with the introduction.
The problem of authentication

The authenticity of the remote server needs to be verified while
establishing a secure connection to it, or else an
attacker (MITM) between the client and the server can eavesdrop on
the transmitted data. To the best of our knowledge, authentication
cannot be done solely in-band, but needs external
infrastructure. The most common methods used in practice rely on
public key encryption.
Web of trust (used by OpenPGP) is a decentralised public key
infrastructure. It relies on out-of-band verification of public keys
and transitivity of trust. If Bob signed Alice's public key, and
Charlie trusts Bob (and signed his public key), then Charlie can trust
that Alice's public key is hers.
Public key infrastructure (used by TLS) relies on trust
anchors which are communicated out-of-band (e.g. distributed with the
client software). In order to authenticate a server, a chain of trust
between a trust anchor and the server certificate (public key) is
established. Only those clients which have the trust anchor deployed
can verify the authenticity of the server.
X.509 public key infrastructure

X.509 is an ITU standard for a public key infrastructure,
developed in 1988. Amongst other things, it specifies the format of
certificates, their attributes, revocation lists, and a path
validation algorithm. X.509 certificates are encoded using abstract
syntax notation one (ASN.1).
A certificate contains a public key, a subject (server name), a
validity period, a purpose (i.e. key usage), an issuer, and
possibly other extensions. All components mentioned in the certificate
are signed by an issuer.
A certificate authority (CA) receives a certificate signing request
from a server operator. It verifies that this signing request is
legitimate (e.g. requested server name is owned by the server
operator) and signs the request. The CA certificate must be trusted by
all potential clients. A CA can also issue intermediate CA
certificates, which are allowed to sign certificates.
When a server certificate or intermediate CA certificate is
compromised, the CA publishes this certificate in its certificate
revocation list (CRL), which each client should poll periodically.
The following certificates are exchanged before a TLS session:
CA -> Client: CA certificate, installed as trust anchor on the clientServer -> CA: certificate request, to be signed by the CACA -> Server: signed server certificate

During the TLS handshake the server sends the certificate chain to the
client. When a client wants to verify a certificate, it has to verify
the signatures of the entire chain, and find a trust anchor which
signed the outermost certificate. Further constraints, such as the
maximum chain length and the validity period, are checked as
well. Finally, the server name in the server certificate is checked to
match the expected identity.
For an example, you can see the sequence diagram of the TLS handshake your browser makes when you visit our demonstration server.
Example code for verification

OpenSSL implements RFC5280 path validation, but there is no
implementation to validate the identity of a certificate. This has to
be implemented by each client, which is rather complex (e.g. in
libfetch it spans over more than 300 lines). A client of the
ocaml-x509 library (such as our http-client) has to
write only two lines of code:
lwt authenticator = X509_lwt.authenticator (`Ca_dir ca_cert_dir) in
lwt (ic, oc) =
  Tls_lwt.connect_ext
    (Tls.Config.client_exn ~authenticator ())
    (host, port)

The authenticator uses the default directory where trust anchors are
stored ('ca_cert_dir'), and this authenticator is
passed to the 'connect_ext' function. This initiates
the TLS handshake, and passes the trust anchors and the hostname to
the TLS library.
During the client handshake when the certificate chain is received by
the server, the given authenticator and hostname are used to
authenticate the certificate chain (in 'validate_chain'):
match
 X509.Authenticator.authenticate ?host:server_name authenticator stack
with
 | `Fail SelfSigned         -> fail Packet.UNKNOWN_CA
 | `Fail NoTrustAnchor      -> fail Packet.UNKNOWN_CA
 | `Fail CertificateExpired -> fail Packet.CERTIFICATE_EXPIRED
 | `Fail _                  -> fail Packet.BAD_CERTIFICATE
 | `Ok                      -> return server_cert

Internally, ocaml-x509 extracts the hostname list from a
certificate in 'cert_hostnames', and the
wildcard or strict matcher compares it to the input.
In total, this is less than 50 lines of pure OCaml code.
Problems in X.509 verification

Several weaknesses in the verification of X.509 certificates have been
discovered, ranging from cryptographic attacks due to
collisions in hash algorithms (practical) over
misinterpretation of the name in the certificate (a C
string is terminated by a null byte), and treating X.509 version 1
certificates always as a trust anchor in GnuTLS.
An empirical study of software that does certificate
verification showed that badly designed APIs are the
root cause of vulnerabilities in this area. They tested various
implementations by using a list of certificates, which did not form a
chain, and would not authenticate due to being self-signed, or
carrying a different server name.
Another recent empirical study (Frankencert) generated random
certificates and validated these with various stacks. They found lots
of small issues in nearly all certificate verification stacks.
Our implementation mitigates against some of the known attacks: we
require a complete valid chain, check the extensions of a certificate,
and implement hostname checking as specified in RFC6125. We have a
test suite with over 3200 tests and multiple CAs. We do not yet discard
certificates which use MD5 as hash algorithm. Our TLS stack
requires certificates to have at least 1024 bit RSA keys.
X.509 library internals

The x509 library uses asn-combinators to parse X.509 certificates and
the nocrypto library for signature verification
(which we wrote about previously).
At the moment we do not yet
expose certificate builders from the library, but focus on certificate parsing
and certificate authentication.
The x509 module provides modules which parse
PEM-encoded (pem) certificates (Cert)
and private keys
(Pk), and an authenticator module
(Authenticators).
So far we have two authenticators implemented:
'chain_of_trust', which implements the basic path
 validation algorithm from RFC5280 (section 6) and the hostname
 validation from RFC6125. To construct such an authenticator, a
 timestamp and a list of trust anchors is needed.'null', which always returns success.

The method 'authenticate', to be called when a
certificate stack should be verified, receives an authenticator, a
hostname and the certificate stack. It returns either Ok or Fail.
Our certificate type is very similar to the described structure in the RFC:
type tBSCertificate = {
  version    : [ `V1 | `V2 | `V3 ] ;
  serial     : Z.t ;
  signature  : Algorithm.t ;
  issuer     : Name.dn ;
  validity   : Time.t * Time.t ;
  subject    : Name.dn ;
  pk_info    : PK.t ;
  issuer_id  : Cstruct.t option ;
  subject_id : Cstruct.t option ;
  extensions : (bool * Extension.t) list
}

type certificate = {
  tbs_cert       : tBSCertificate ;
  signature_algo : Algorithm.t ;
  signature_val  : Cstruct.t
}

The certificate itself wraps the to be signed part ('tBSCertificate'),
the used signature algorithm, and the actual signature. It consists of
a version, serial number, issuer, validity, subject, public key
information, optional issuer and subject identifiers, and a list of
extensions -- only version 3 certificates may have extensions.
The 'certificate' module implements the actual
authentication of certificates, and provides some useful getters such
as 'cert_type', 'cert_usage', and
'cert_extended_usage'. The main entry for
authentication is 'verify_chain_of_trust',
which checks correct signatures of the chain, extensions and validity
of each certificate, and the hostname of the server certificate.
The grammar of X.509 certificates is developed in the
'asn_grammars' module, and the object
identifiers are gathered in the 'registry' module.
Implementation of certificate verification

We provide the function 'valid_cas', which takes a
timestamp and a list of certificate authorities. Each certificate
authority is checked to be valid, self-signed,
correctly signed, and having 
proper X.509 v3 extensions.
As mentioned above, version 1 and version 2
certificates do not contain extensions. For a version 3 certificate,
'validate_ca_extensions' is called: The
basic constraints extensions must be present, and its value must be
true. Also, key usage must be present and the certificate must be
allowed to sign certificates. Finally, we reject the certificate if
there is any extension marked critical, apart from the two mentioned
above.
When we have a list of validated CA certificates, we can use these to
verify the chain of trust, which gets a
hostname, a timestamp, a list of trust anchors and a certificate chain
as input. It first checks that the server certificate is
valid, the validity of the intermediate
certificates, and that the chain is complete
(the pathlen constraint is not validated) and rooted in a trust
anchor. A server certificate is valid if the validity period matches
the current timestamp, the given hostname matches
its subject alternative name extension or common name (might be
wildcard or strict matching, RFC6125), and it does not have a
basic constraints extension which value is true.
Current status of ocaml-x509

We currently support only RSA certificates. We do not check revocation
lists or use the online certificate status protocol (OCSP). Our
implementation does not handle name constraints and policies. However, if
any of these extensions is marked critical, we refuse to validate the
chain. To keep our main authentication free of side-effects, it currently uses
the timestamp when the authenticator was created rather than when it is used
(this isn't a problem if lifetime of the OCaml-TLS process is comparatively
short, as in the worst case the lifetime of the certificates can be extended by
the lifetime of the process).
We invite people to read through the
certificate verification and the
ASN.1 parsing. We welcome discussion on the
mirage-devel mailing list and bug reports
on the GitHub issue tracker.
Posts in this TLS series:
Introducing transport layer security (TLS) in pure OCamlOCaml-TLS: building the nocrypto library coreOCaml-TLS: adventures in X.509 certificate parsing and validationOCaml-TLS: ASN.1 and notation embeddingOCaml-TLS: the protocol implementation and mitigations to known attacks


   Hide
        
      
                    by Hannes Mehnert at Jul 10, 2014 
      
      
    
  


       
          OCaml-TLS: building the nocrypto library core
      (Mirage OS)
    
                                      This is the second in a series of posts that introduce new libraries for a pure OCaml implementation of TLS.
You might like to begin with the introduction.
What is nocrypto?

nocrypto is the small cryptographic library behind the
ocaml-tls project. It is built to be straightforward to use, adhere to
functional programming principles and able to run in a Xen-based unikernel.
Its major use-case is ocaml-tls, which we announced yesterday, but we do intend to provide
sufficient features for it to be more widely applicable.
"Wait, you mean you wrote your own crypto library?"
"Never write your own crypto"

Everybody seems to recognize that cryptography is horribly difficult. Building
cryptography, it is all too easy to fall off the deep end and end up needing to
make decisions only a few, select specialists can make. Worse, any mistake is
difficult to uncover but completely compromises the security of the system. Or
in Bruce Schneier's words:
Building a secure cryptographic system is …Read more...      This is the second in a series of posts that introduce new libraries for a pure OCaml implementation of TLS.
You might like to begin with the introduction.
What is nocrypto?

nocrypto is the small cryptographic library behind the
ocaml-tls project. It is built to be straightforward to use, adhere to
functional programming principles and able to run in a Xen-based unikernel.
Its major use-case is ocaml-tls, which we announced yesterday, but we do intend to provide
sufficient features for it to be more widely applicable.
"Wait, you mean you wrote your own crypto library?"
"Never write your own crypto"

Everybody seems to recognize that cryptography is horribly difficult. Building
cryptography, it is all too easy to fall off the deep end and end up needing to
make decisions only a few, select specialists can make. Worse, any mistake is
difficult to uncover but completely compromises the security of the system. Or
in Bruce Schneier's words:
Building a secure cryptographic system is easy to do badly, and very difficult
to do well. Unfortunately, most people can't tell the difference. In other
areas of computer science, functionality serves to differentiate the good from
the bad: a good compression algorithm will work better than a bad one; a bad
compression program will look worse in feature-comparison charts. Cryptography
is different. Just because an encryption program works doesn't mean it is
secure.


Obviously, it would be far wiser not to attempt to do this and instead reuse
good, proven work done by others. And with the wealth of free cryptographic
libraries around, one gets to take their pick.
So to begin with, we turned to cryptokit, the more-or-less
standard cryptographic library in the OCaml world. It has a decent coverage of
the basics: some stream ciphers (ARC4), some block ciphers (AES, 3DES and
Blowfish) the core hashes (MD5, SHA, the SHA2 family and RIPEMD) and the
public-key primitives (Diffie-Hellman and RSA). It is also designed with
composability in mind, exposing various elements as stream-transforming objects
that can be combined on top of one another.
Unfortunately, its API was a little difficult to use. Suppose you have a secret
key, an IV and want to use AES-128 in CBC mode to encrypt a bit of data. You do
it like this:
let key = "abcd1234abcd1234"
and iv  = "1234abcd1234abcd"
and msg = "fire the missile"

let aes     = new Cryptokit.Block.aes_encrypt key
let aes_cbc = new Cryptokit.Block.cbc_encrypt ~iv aes

let cip =
  let size =
    int_of_float (ceil (float String.(length msg) /. 16.) *. 16.) in
  String.create size

let () = aes_cbc#transform msg 0 cip 0

At this point, cip contains our secret message. This being CBC, both msg and
the string the output will be written into (cip) need to have a size that is a
multiple of the underlying block size. If they do not, bad things will
happen -- silently.
There is also the curious case of hashing-object states:
let md5 = Cryptokit.Hash.md5 ()

let s1 = Cryptokit.hash_string md5 "bacon"
let s2 = Cryptokit.hash_string md5 "bacon"
let s3 = Cryptokit.hash_string md5 "bacon"

(*
  s1 = "x\019%\142\248\198\1822\221\232\204\128\246\189\166/"
  s2 = "'\\F\017\234\172\196\024\142\255\161\145o\142\128\197"
  s3 = "'\\F\017\234\172\196\024\142\255\161\145o\142\128\197"
*)

The error here is to try and carry a single instantiated hashing object around,
while trying to get hashes of distinct strings. But with the convergence after
the second step, the semantics of the hashing object still remains unclear to
us.
One can fairly easily overcome the API style mismatches by making a few
specialized wrappers, of course, except for two major problems:
Cryptokit is pervasively stateful. While this is almost certainly a result of
 performance considerations combined with its goals of ease of
 compositionality, it directly clashes with the fundamental design property of
 the TLS library we wanted to use it in: our ocaml-tls library is stateless. We need to
 be able to represent the state the encryption engine is in as a value.
Cryptokit operates on strings. As a primary target of ocaml-tls was
 Mirage, and Mirage uses separate, non-managed regions of memory to
 store network data in, we need to be able to handle foreign-allocated
 storage. This means Bigarray (as exposed by Cstruct), and it seems just
 plain wrong to negate all the careful zero-copy architecture of the stack
 below by copying everything into and out of strings.


There are further problems. For example, Cryptokit makes no attempts to combat
well-known timing vulnerabilities. It has no support for elliptic curves. And it
depends on the system-provided random number generator, which does not exist
when running in the context of a unikernel.
At this point, with the de facto choice off the table, it's probably worth
thinking about writing OCaml bindings to a rock-solid cryptographic library
written in C.
NaCl is a modern, well-regarded crypto implementation, created by a
group of pretty famous and equally well-regarded cryptographers, and was the
first choice. Or at least its more approachable and packageable fork
was, which already had OCaml bindings. Unfortunately, NaCl
provides a narrow selection of implementations of various cryptographic
primitives, the ones its authors thought were best-of-breed (for example, the
only symmetric ciphers it implements are (X-)Salsa and AES in CTR mode). And
they are probably right (in some aspects they are certainly right), but NaCl
is best used for implementations of newly-designed security protocols. It is
simply too opinionated to support an old, standardized behemoth like TLS.
Then there is crypto, the library OpenSSL is built on top of. It
is quite famous and provides optimized implementations of a wide range of
cryptographic algorithms. It also contains upwards of 200,000 lines of C and a
very large API footprint, and it's unclear whether it would be possible to run
it in the unikernel context. Recently, the parent project it is embedded in has
become highly suspect, with one high-profile vulnerability piling on top of
another and at least two forks so far attempting to
clean the code base. It just didn't feel like a healthy code base to build
a new project on.
There are other free cryptographic libraries in C one could try to bind, but at
a certain point we faced the question: is the work required to become intimately
familiar with the nuances and the API of an existing code base, and create
bindings for it in OCaml, really that much smaller than writing one from
scratch? When using a full library one commits to its security decisions and
starts depending on its authors' time to keep it up to date -- maybe this
effort is better spent in writing one in the first place.
Tantalizingly, the length of the single OCaml source file in Cryptokit is
2260 lines.
Maybe if we made zero decisions ourselves, informed all our work by published
literature and research, and wrote the bare minimum of code needed, it might not
even be dead-wrong to do it ourselves?
And that is the basic design principle. Do nothing fancy. Do only documented
things. Don't write too much code. Keep up to date with security research. Open
up and ask people.
The anatomy of a simple crypto library

nocrypto uses bits of C, similarly to other cryptographic libraries written in
high-level languages.
This was actually less of a performance concern, and more of a security one: for
the low-level primitives which are tricky to implement and for which known,
compact and widely used code already exists, the implementation is probably
better reused. The major pitfall we hoped to avoid that way are side-channel
attacks.
We use public domain (or BSD licenced) C sources for the
simple cores of AES, 3DES, MD5, SHA and SHA2. The impact of errors in this code
is constrained: they contain no recursion, and they perform no allocation,
simply filling in caller-supplied fixed-size buffer by appropriate bytes.
The block implementations in C have a simple API that requires us to provide the
input and output buffers and a key, writing the single encrypted (or decrypted)
block of data into the buffer. Like this:
void rijndaelEncrypt(const unsigned long *rk, int nrounds,
  const unsigned char plaintext[16], unsigned char ciphertext[16]);

void rijndaelDecrypt(const unsigned long *rk, int nrounds,
  const unsigned char ciphertext[16], unsigned char plaintext[16]);

The hashes can initialize a provided buffer to serve as an empty accumulator,
hash a single chunk of data into that buffer and convert its contents into a
digest, which is written into a provided fixed buffer.
In other words, all the memory management happens exclusively in OCaml and all
the buffers passed into the C layer are tracked by the garbage collector (GC).
Symmetric ciphers

So far, the only provided ciphers are AES, 3DES and ARC4, with ARC4 implemented
purely in OCaml (and provided only for TLS compatibility and for testing).
AES and 3DES are based on core C code, on top of which we built some standard
modes of operation in OCaml. At the moment we support ECB, CBC
and CTR. There is also a nascent GCM implementation which is, at the time
of writing, known not to be optimal and possibly prone to timing attacks, and
which we are still working on.
The exposed API strives to be simple and value-oriented. Each mode of each
cipher is packaged up as a module with a similar signature, with a pair of
functions for encryption and decryption. Each of those essentially takes a key
and a byte buffer and yields the resulting byte buffer, minimising hassle.
This is how you encrypt a message:
open Nocrypto.Block

let key = AES.CBC.of_secret Cstruct.(of_string "abcd1234abcd1234")
and iv  = Cstruct.of_string "1234abcd1234abcd"
and msg = Cstruct.of_string "fire the missile"

let { AES.CBC.message ; iv } = AES.CBC.encrypt ~key ~iv msg

The hashes implemented are just MD5, SHA and the SHA2 family. Mirroring the
block ciphers, they are based on C cores, with the HMAC construction provided in
OCaml. The API is similarly simple: each hash is a separate module with the same
signature, providing a function that takes a byte buffer to its digest, together
with several stateful operations for incremental computation of digests.
Of special note is that our current set of C sources will probably soon be
replaced. AES uses code that is vulnerable to a timing attack,
stemming from the fact that substitution tables are loaded into the CPU cache
as-needed. The code does not take advantage of the AES-NI
instructions present in modern CPUs that allow AES to be hardware-assisted. SHA
and SHA2 cores turned out to be (comparatively) ill-performing, and static
analysis already uncovered some potential memory issues, so we are looking for
better implementations.
Public-key cryptography

Bignum arithmetic is provided by the excellent zarith library, which
in turn uses GMP. This might create some portability problems later on,
but as GMP is widely used and well rounded code base which also includes some of
the needed auxiliary number-theoretical functions (its slightly extended
Miller-Rabin probabilistic primality test and the fast next-prime-scanning
function), it seemed like a much saner choice than redoing it from scratch.
The RSA module provides the basics: raw encryption and decryption,
PKCS1-padded versions of the same operations, and PKCS1 signing and
signature verification. It can generate RSA keys, which it does simply by
finding two large primes, in line with Rivest's own
recommendation.
Notably, RSA implements the standard blinding technique which can mitigate
some side-channel attacks, such as timing or acoustic
cryptanalysis. It seems to foil even stronger, cache eviction
based attacks, but as of now, we are not yet completely sure.
The Diffie-Hellman module is also relatively basic. We implement some
widely recommended checks on the incoming public key to
mitigate some possible MITM attacks, the module can generate strong DH groups
(using safe primes) with guaranteed large prime-order subgroup, and we provide
a catalogue of published DH groups ready for use.
Randomness

Random number generation used to be a chronically overlooked part of
cryptographic libraries, so much so that nowadays one of the first questions
about a crypto library is, indeed, "Where does it get randomness from?"
It's an important question. A cryptographic system needs unpredictability in
many places, and violating this causes catastrophic failures.
nocrypto contains its own implementation of Fortuna. Like
Yarrow, Fortuna uses a strong block cipher in CTR mode (AES in our
case) to produce the pseudo-random stream, a technique that is considered as
unbreakable as the underlying cipher.
The stream is both self-rekeyed, and rekeyed with the entropy gathered into its
accumulator pool. Unlike the earlier designs, however, Fortuna is built without
entropy estimators, which usually help the PRNG decide when to actually convert
the contents of an entropy pool into the new internal state. Instead, Fortuna
uses a design where the pools are fed round-robin, but activated with an
exponential backoff. There is recent research showing this
design is essentially sound: after a state compromise, Fortuna wastes no more
than a constant factor of incoming entropy -- whatever the amount of entropy is
-- before coming back to an unpredictable state. The resulting design is both
simple, and robust in terms of its usage of environmental entropy.
The above paper also suggests a slight improvement to the accumulator regime,
yielding a factor-of-2 improvement in entropy usage over the original. We still
haven't implemented this, but certainly intend to.
A PRNG needs to be fed with some actual entropy to be able to produce
unpredictable streams. The library itself contains no provisions for doing this
and its PRNG needs to be fed by the user before any output can be produced. We
are working with the Mirage team on exposing environmental
entropy sources and connecting them to our implementation of Fortuna.
Above & beyond

nocrypto is still very small, providing the bare minimum cryptographic
services to support TLS and related X.509 certificate operations. One of the
goals is to flesh it out a bit, adding some more widely deployed algorithms, in
hopes of making it more broadly usable.
There are several specific problems with the library at this stage:
C code - As mentioned, we are seeking to replace some of the C code we use. The hash
cores are underperforming by about a factor of 2 compared to some other
implementations. AES implementation is on one hand vulnerable to a timing attack
and, on the other hand, we'd like to make use of hardware acceleration for this
workhorse primitive -- without it we lose about an order of magnitude of
performance.
Several options were explored, ranging from looking into the murky waters of
OpenSSL and trying to exploit their heavily optimized primitives, to bringing
AES-NI into OCaml and redoing AES in OCaml. At this point, it is not clear which
path we'll take.
ECC - Looking further, the library still lacks support for elliptic curve cryptography
and we have several options for solving this. Since it is used by TLS, ECC is
probably the missing feature we will concentrate on first.
Entropy on Xen - The entropy gathering on Xen is incomplete. The current prototype uses current
time as the random seed and the effort to expose noisier sources like interrupt
timings and the RNG from dom0's kernel is still ongoing.  Dave Scott, for example, has
submitted patches to upstream Xen to make it easier to establish low-bandwidth
channels to supplies guest VMs with strong entropy from a privileged domain
that has access to physical devices and hence high-quality entropy sources.
GC timing attacks? - There is the question of GC and timing attacks: whether doing
cryptography in a high-level language opens up a completely new surface for
timing attacks, given that GC runs are very visible in the timing profile. The
basic approach is to leave the core routines which we know are potentially
timing-sensitive (like AES) and for which we don't have explicit timing
mitigations (like RSA) to C, and invoke them atomically from the perspective of
the GC. So far, it's an open question whether the constructions built on top
of them expose further side-channels.
Still, we believe that the whole package is a pleasant library to work with. Its
simplicity contributes to the comparative simplicity of the entire TLS library,
and we are actively seeking input on areas that need further improvement.
Although we are obviously biased, we believe it is the best cryptographic base
library available for this project, and it might be equally suited for your next
project too!
We are striving to be open about the current security status of our code. You
are free to check out our issue tracker and invited to contribute
comments, ideas, and especially audits and code.
Posts in this TLS series:
Introducing transport layer security (TLS) in pure OCamlOCaml-TLS: building the nocrypto library coreOCaml-TLS: adventures in X.509 certificate parsing and validationOCaml-TLS: ASN.1 and notation embeddingOCaml-TLS: the protocol implementation and mitigations to known attacks


   Hide
        
      
                    by David Kaloper at Jul 09, 2014 
      
      
    
  


       
          Introducing transport layer security (TLS) in pure OCaml
      (Mirage OS)
    
                                      We announce a beta release of ocaml-tls, a clean-slate implementation of
Transport Layer Security (TLS) in
OCaml.
What is TLS?

Transport Layer Security (TLS) is probably the most widely deployed
security protocol on the Internet. It provides communication privacy
to prevent eavesdropping, tampering, and message forgery. Furthermore,
it optionally provides authentication of the involved endpoints. TLS
is commonly deployed for securing web services (HTTPS), emails,
virtual private networks, and wireless networks.
TLS uses asymmetric cryptography to exchange a symmetric key, and
optionally authenticate (using X.509) either or both endpoints. It
provides algorithmic agility, which means that the key exchange
method, symmetric encryption algorithm, and hash algorithm are
negotiated.
TLS in OCaml

Our implementation ocaml-tls is already able to interoperate with
existing TLS implementations, and supports several important TLS extensions
such as server name indication (RFC4366, enabli…Read more...      We announce a beta release of ocaml-tls, a clean-slate implementation of
Transport Layer Security (TLS) in
OCaml.
What is TLS?

Transport Layer Security (TLS) is probably the most widely deployed
security protocol on the Internet. It provides communication privacy
to prevent eavesdropping, tampering, and message forgery. Furthermore,
it optionally provides authentication of the involved endpoints. TLS
is commonly deployed for securing web services (HTTPS), emails,
virtual private networks, and wireless networks.
TLS uses asymmetric cryptography to exchange a symmetric key, and
optionally authenticate (using X.509) either or both endpoints. It
provides algorithmic agility, which means that the key exchange
method, symmetric encryption algorithm, and hash algorithm are
negotiated.
TLS in OCaml

Our implementation ocaml-tls is already able to interoperate with
existing TLS implementations, and supports several important TLS extensions
such as server name indication (RFC4366, enabling virtual hosting)
and secure renegotiation (RFC5746).
Our demonstration server runs ocaml-tls and renders exchanged
TLS messages in nearly real time by receiving a trace of the TLS
session setup. If you encounter any problems, please give us feedback.
ocaml-tls and all dependent libraries are available via OPAM (opam install tls). The source is available
under a BSD license. We are primarily working towards completeness of
protocol features, such as client authentication, session resumption, elliptic curve and GCM
cipher suites, and have not yet optimised for performance.
ocaml-tls depends on the following independent libraries: ocaml-nocrypto implements the
cryptographic primitives, ocaml-asn1-combinators provides ASN.1 parsers/unparsers, and
ocaml-x509 implements the X509 grammar and certificate validation (RFC5280). ocaml-tls implements TLS (1.0, 1.1 and 1.2; RFC2246,
RFC4346, RFC5246).
We invite the community to audit and run our code, and we are particularly interested in discussion of our APIs.
Please use the mirage-devel mailing list for discussions.
Please be aware that this release is a beta and is missing external code audits.
It is not yet intended for use in any security critical applications.
In our issue tracker we transparently document known attacks against TLS and our mitigations
(checked and unchecked).
We have not yet implemented mitigations against either the
Lucky13 timing attack or traffic analysis (e.g. length-hiding padding).
Trusted code base

Designed to run on Mirage, the trusted code base of ocaml-tls is small. It includes the libraries already mentioned,
`ocaml-tls`, `ocaml-asn-combinators`, `ocaml-x509`,
and `ocaml-nocrypto` (which uses C implementations of block
ciphers and hash algorithms). For arbitrary precision integers needed in 
asymmetric cryptography, we rely on `zarith`, which wraps
`libgmp`. As underlying byte array structure we use
`cstruct` (which uses OCaml Bigarray as storage).
We should also mention the OCaml runtime, the OCaml compiler, the
operating system on which the source is compiled and the binary is executed, as
well as the underlying hardware. Two effectful frontends for
the pure TLS core are implemented, dealing
with side-effects such as reading and writing from the network: Lwt_unix and
Mirage, so applications can run directly as a Xen unikernel.
Why a new TLS implementation?

Update:
Thanks to Frama-C guys for pointing out
that CVE-2014-1266 and CVE-2014-0224 are not memory safety issues, but
logic errors. This article previously stated otherwise.
There are only a few TLS implementations publicly available and most
programming languages bind to OpenSSL, an open source implementation written
in C. There are valid reasons to interface with an existing TLS library,
rather than developing one from scratch, including protocol complexity and
compatibility with different TLS versions and implementations. But from our
perspective the disadvantage of most existing libraries is that they
are written in C, leading to:
Memory safety issues, as recently observed by Heartbleed and GnuTLS
 session identifier memory corruption (CVE-2014-3466) bugs;Control flow complexity (Apple's goto fail, CVE-2014-1266);And difficulty in encoding state machines (OpenSSL change cipher suite
 attack, CVE-2014-0224).

Our main reasons for ocaml-tls are that OCaml is a modern functional
language, which allows concise and declarative descriptions of the
complex protocol logic and provides type safety and memory safety to help
guard against programming errors. Its functional nature is extensively
employed in our code: the core of the protocol is written in purely
functional style, without any side effects.
Subsequent blog posts over the coming
days will examine in more detail
the design and implementation of the four libraries, as well as the security
trade-offs and some TLS attacks and our mitigations against them.  For now
though, we invite you to try out our demonstration server
running our stack over HTTPS.  We're particularly interested in feedback on our issue tracker about
clients that fail to connect, and any queries from anyone reviewing the source code
of the constituent libraries.
Posts in this TLS series:
Introducing transport layer security (TLS) in pure OCamlOCaml-TLS: building the nocrypto library coreOCaml-TLS: adventures in X.509 certificate parsing and validationOCaml-TLS: ASN.1 and notation embeddingOCaml-TLS: the protocol implementation and mitigations to known attacks


   Hide
        
      
                    by Hannes Mehnert at Jul 08, 2014 
      
      
    
  


       
                  MirageOS 1.2 released and the 2.0 runup begins
      (Mirage OS)
    
    
                                      Summer is in full swing here in MirageOS HQ with torrential rainstorms, searing
sunshine, and our OSCON 2014 talk
rapidly approaching in just a few weeks.  We've been steadily releasing point releases
since the first release back in December, and today's MirageOS
1.2.0 is the last of the 1.x series.
The main improvements are usability-oriented:
The Mirage frontend tool now generates a Makefile with a make depend
 target, instead of directly invoking OPAM as part of mirage configure.
 This greatly improves usability on slow platforms such as ARM, since the
 output of OPAM as it builds can be inspected more easily. Users will now
 need to run make depend to ensure they have the latest package set
 before building their unikernel.
Improve formatting of the mirage output, including pretty colours!
 This makes it easier to distinguish complex unikernel configurations
 that have lots of deployment options.  The generated files are built
 more verbosely by default to facilitate debuggi…Read more...      Summer is in full swing here in MirageOS HQ with torrential rainstorms, searing
sunshine, and our OSCON 2014 talk
rapidly approaching in just a few weeks.  We've been steadily releasing point releases
since the first release back in December, and today's MirageOS
1.2.0 is the last of the 1.x series.
The main improvements are usability-oriented:
The Mirage frontend tool now generates a Makefile with a make depend
 target, instead of directly invoking OPAM as part of mirage configure.
 This greatly improves usability on slow platforms such as ARM, since the
 output of OPAM as it builds can be inspected more easily. Users will now
 need to run make depend to ensure they have the latest package set
 before building their unikernel.
Improve formatting of the mirage output, including pretty colours!
 This makes it easier to distinguish complex unikernel configurations
 that have lots of deployment options.  The generated files are built
 more verbosely by default to facilitate debugging, and with debug
 symbols and backtraces enabled by default.
Added several device module types, including ENTROPY for random
 noise, FLOW for stream-oriented connections, and exposed the IPV4
 device in the STACKV4 TCP/IP stack type.
Significant bugfixes in supporting libraries such as the TCP/IP
 stack (primarily thanks to Mindy Preston fuzz testing
 and finding some good zingers).  There are too many
 library releases to list individually here, but you can browse the changelog for more details.


 Towards MirageOS 2.0

We've also been working hard on the MirageOS 2.x series, which introduces
a number of new features and usability improvements that emerged from actually
using the tools in practical projects.  Since there have been so many new
contributors recently,
Amir Chaudhry is coordinating a series of blog
posts in the runup to
OSCON that
explains the new work in depth.  Once the release rush has subsided, we'll
be working on integrating these posts into our documentation
properly.
The new 2.0 features include the Irmin branch-consistent distributed storage
library, the pure OCaml TLS stack, Xen/ARM support and the Conduit I/O
subsystem for mapping names to connections.  Also included in the blog series
are some sample usecases on how these tie together for real applications (as a
teaser, here's a video of Xen VMs booting using
Irmin thanks to Dave
Scott and Thomas Gazagnaire!)
Upcoming talks and tutorials

Richard Mortier and myself will be gallivanting around the world
to deliver a few talks this summer:
The week of OSCON on July 20th-24th.  Please get in touch via the conference website or a direct e-mail, or attend our talk on Thursday morning.
There's a Real World OCaml book signing on Tuesday morning for the super keen as well.The ECOOP summer school in beautiful Uppsala in Sweden on Weds 30th July. I'll be presenting the Irmin and Xen integration at Xen Project Developer Summit in
 Chicago on Aug 18th (as part of LinuxCon North America).  Mort and Mindy (no jokes please) will be
 joining the community panel about GSoC/OPW participation.

As always, if there are any particular topics you would like to see more
on, then please comment on the tracking issue
or get in touch directly.  There will be a lot of releases coming out
in the next few weeks (including a beta of the new version of OPAM,
so bug reports are very much appreciated for those
things that slip past Travis CI!

   Hide
        
      
                    by Anil Madhavapeddy at Jul 08, 2014 
      
      
    
  


       
                  Highlights from recent sessions
      (Compiler Hacking)
    
    
                                Highlights from recent sessions

With the next compiler hacking meeting due to take place in a couple of days it's time for a look back at some results from our last couple of sessions.



The front end

(today I stared a camel in the face by Adam Foster)


The front end (i.e. the parser and type checker) saw a number of enhancements.



Succinct functor syntax

Syntax tweaks are always popular, if often contentious.   However, reaching agreement is significantly easier when adding syntax is a simple matter of extending an existing correspondence between two parts of the language.  For example, it was clear which syntax to use when adding support for lazy patterns: since patterns generally mirror the syntax for the values they match, patterns for destructing lazy values should use the same lazy keyword as the expressions which construct them.

A second correspondence in OCaml's syntax relates modules and values.  Module names and variables are both bound with =; module signatures and…Read more...Highlights from recent sessions

With the next compiler hacking meeting due to take place in a couple of days it's time for a look back at some results from our last couple of sessions.



The front end

(today I stared a camel in the face by Adam Foster)


The front end (i.e. the parser and type checker) saw a number of enhancements.



Succinct functor syntax

Syntax tweaks are always popular, if often contentious.   However, reaching agreement is significantly easier when adding syntax is a simple matter of extending an existing correspondence between two parts of the language.  For example, it was clear which syntax to use when adding support for lazy patterns: since patterns generally mirror the syntax for the values they match, patterns for destructing lazy values should use the same lazy keyword as the expressions which construct them.

A second correspondence in OCaml's syntax relates modules and values.  Module names and variables are both bound with =; module signatures and types are both ascribed with :; module fields and record fields are both projected with ..  The syntax for functors and functions is also similar, but the latter offers a number of shortcuts not available in the module language; you can write
fun x y z -> e

instead of the more prolix equivalent:
fun x -> fun y -> fun z -> e

but multi-argument functors must be written out in full:
functor (X : R) -> functor (Y : S) -> functor (Z : T) -> M

In February's meeting, Thomas wrote a patch that adds an analogue of the shorter syntax to the module language, allowing the repeated functor to be left out:
functor (X : R) (Y : S) (Z : T) -> M

The patch also adds support for a corresponding abbreviation at the module type level.  Defining the type of a multi-argument functor currently involves writing a rather clunky sequence of functor abstractions:
module type F = functor (X : R) -> functor (Y : S) -> functor (Z : T) -> U

With Thomas's patch all but the first occurrence of functor disappear:
module type F = functor (X : R) (Y : S) (Z : T) -> U

Since Thomas's patch has been merged into trunk, you can try out the new syntax using the 4.02.0 beta, which is available as a compiler switch in the OPAM repository:
opam switch 4.02.0+trunk

The next step is to find out whether the verbose syntax was a symptom or a cause of the infrequency of higher-order functors in OCaml code.  Will we see a surge in the popularity of higher-order modules as the syntax becomes more accommodating?



Integer ranges

David started work on extending OCaml's range patterns, which currently support only characters, to support integer ranges.  For example, consider the following code from MLDonkey:
match mdn with
  None       when h >= 0 && h <= 23 -> Some h
| Some false when h > 0 && h <= 11  -> Some h
| Some false when h = 12            -> Some 0
| Some true  when h > 0 && h <= 11  -> Some (h + 12)
| Some true  when h = 12            -> Some 12
| Some _                            -> None
| None                              -> None

Although this is fairly clear, it could be made even clearer if we had a less powerful language for expressing the tests involving h.  Since the whole OCaml language is available in the when guard of a case, the reader has to examine the code carefully before concluding that the tests are all simple range checks.  Perhaps worse, using guards inhibits the useful checks that the OCaml compiler performs to determine whether patterns are exhaustive or redundant.  David's patch makes it possible to rewrite the tests without guards, making the simple nature of the tests on h clear at a glance (and making it possible once again to check exhaustiveness and redundancy):
match mdn, h with
  None      , 0..23
| Some false, 1..11 -> Some h
| Some false, 12    -> Some 0
| Some true , 1..11 -> Some (h + 12)
| Some true , 12    -> Some 12
| _                 -> None

The work on range patterns led to a robust exchange of views about which other types should be supported -- should we support any enumerable type (e.g. variants with nullary constructors)? or perhaps even any ordered type (e.g. floats or strings)?  For the moment, there seems to be a much clearer consensus in favour of supporting integer types than there is for generalising range patterns any further.



Extensible variants

Since the compiler hacking group only meets for an evening every couple of months or so, most of the projects we work on are designed so that it's possible to implement them in a few hours.  Leo's proposal for extensible variants is a notable exception, predating both the compiler hacking group and OCaml Labs itself.

Extensible variants generalise exceptions: with Leo's patch the exception type exn becomes a particular instance of a class of types that can be defined by the user rather than a special builtin provided by the compiler:
(* Define an extensible variant type *)
type exn = ..

(* Extend the type with a constructor *)
type exn += Not_found

(* Extend the type with another constructor *)
type exn += Invalid_argument of string

Even better, extensible variants come with all the power of regular variant types: they can take type parameters, and even support GADT definitions:
(* Define a parameterised extensible variant type *)
type 'a error = ..

(* Extend the type with a constructor *)
type 'a error = Error of 'a

(* Extend the type with a GADT constructor *)
type 'a error : IntError : int -> int error

On the evening of the last compiler hacking meeting, Leo completed the patch; shortly afterwards it was merged to trunk, ready for inclusion in OCaml 4.02!

Extensible variants are a significant addition to the language, and there's more to them than these simple examples show.  A forthcoming post from Leo will describe the new feature in more detail.  In the meantime, since they've been merged into the 4.02 release candidate, you can try them out with OPAM:
opam switch 4.02.0+trunk



Lazy record fields

Not everything we work on makes is destined to make it upstream.  A few years ago, Alain Frisch described an OCaml extension in use at Lexifi for marking record fields lazy, making it possible to delay the evaluation of initializing expressions without writing the lazy keyword every time a record is constructed.  Alain's post was received enthusiastically, and lazy record fields seemed like an obvious candidate for inclusion upstream, so in April's meeting Thomas put together a patch implementing the design.  Although the OCaml team decided not to merge the patch, it led to an enlightening discussion with comments from several core developers, including Alain, who described subsequent, less positive, experience with the feature at Lexifi, and Xavier, who explained the rationale underlying the current design.



The back end

(Relief
by Hartwig HKD)


The OCaml back end (i.e. the code generation portion of the compiler) also saw a proposed enhancement.



Constant arithmetic optimization

Stephen submitted a patch improving the generated code for functions that perform constant arithmetic on integers.

In OCaml, integers and characters are represented as shifted immediate values, with the least significant bit set to distinguish them from pointers.  This makes some arithmetic operations a little more expensive.  For example, consider a function that int_of_digits that builds an integer from three character digits:
int_of_digits '3' '4' '5' => 345

We might define int_of_digits as follows:
let int_of_digits a b c = 
  100 * (Char.code a - Char.code '0') + 
   10 * (Char.code b - Char.code '0') +
    1 * (Char.code c - Char.code '0')

Passing the -dcmm flag to ocamlopt shows the results of compiling the function to the C-- intermediate language. 
ocamlopt -dcmm int_of_digits.ml

The generated code has the following form (reformatted for readability):
200 * ((a - 96) >> 1) +
 20 * ((b - 96) >> 1) +
  2 * ((c - 96) >> 1) + 1

The right shifts convert the tagged representation into native integers, and the final + 1 converts the result back to a tagged integer.

Stephen's patch floats the arithmetic operations that involve constant operands outwards, eliminating most of the tag-munging code in favour of a final correcting addition:
(a * 100) +
(b * 10) +
 c - 10766

Although these changes are not yet merged, you can easily try them out, thanks to Anil's script that makes compiler pull requests available as OPAM switches:
opam switch 4.03.0+pr17



Standard library and beyond

(Literary advocate Dashdondog Jamba, and his mobile library, described in My librarian is a camel)


Our compiler hacking group defines "compiler" rather broadly.  As a result people often work on improving the standard library and tools as well as the compiler proper.  For example, in recent sessions, David added a small patch to expose the is_inet6_addr function, and Philippe proposed a patch that eliminates unnecessary bounds checking in the buffer module.  The last session also saw Raphaël and Simon push a number of patches for integrating merlin with the acme editor to OPAM, improving OCaml support in Plan 9.

Next session

The compiler hacking group is open to anyone with an interest in contributing to the OCaml compiler.  If you're local to Cambridge, you're welcome to join us at the next session!
Hide
        
      
                    by Compiler Hacking at Jun 24, 2014 
      
      
    
  


       
                  Sixth OCaml compiler hacking session
      (Compiler Hacking)
    
    
                                (Update (2014-06-24): Stephen Dolan will be giving a demo of multicore OCaml!)

It's time for the sixth Cambridge OCaml compiler-hacking session!  We'll be meeting in the Computer Lab again next Wednesday evening (25th June).

If you're planning to come along, it'd be helpful if you could indicate interest via Doodle and sign up to the mailing list to receive updates:

Where: Room FW11, Computer Laboratory, Madingley Road

When: 6.30pm, Wednesday 25th June

Who: anyone interested in improving OCaml. Knowledge of OCaml programming will obviously be helpful, but prior experience of working on OCaml internals isn't necessary.

What: fixing bugs, implementing new features, learning about OCaml internals

Wiki: https://github.com/ocamllabs/compiler-hacking/wiki

We're defining "compiler" pretty broadly, to include anything that's part of the standard distribution, which means at least the standard library, runtime, tools (ocamldep, ocamllex, ocamlyacc, etc.), ocamlbuild, the documentation, …Read more...(Update (2014-06-24): Stephen Dolan will be giving a demo of multicore OCaml!)

It's time for the sixth Cambridge OCaml compiler-hacking session!  We'll be meeting in the Computer Lab again next Wednesday evening (25th June).

If you're planning to come along, it'd be helpful if you could indicate interest via Doodle and sign up to the mailing list to receive updates:

Where: Room FW11, Computer Laboratory, Madingley Road

When: 6.30pm, Wednesday 25th June

Who: anyone interested in improving OCaml. Knowledge of OCaml programming will obviously be helpful, but prior experience of working on OCaml internals isn't necessary.

What: fixing bugs, implementing new features, learning about OCaml internals

Wiki: https://github.com/ocamllabs/compiler-hacking/wiki

We're defining "compiler" pretty broadly, to include anything that's part of the standard distribution, which means at least the standard library, runtime, tools (ocamldep, ocamllex, ocamlyacc, etc.), ocamlbuild, the documentation, and the compiler itself. We'll have suggestions for mini-projects for various levels of experience (see also some things we've worked on in previous sessions), but feel free to come along and work on whatever you fancy.

We'll also be ordering pizza, so if you want to be counted for food you should aim to arrive by 6.45pm.
Hide
        
      
                    by Compiler Hacking at Jun 20, 2014 
      
      
    
  


       
                  Python to OCaml: retrospective
      (Thomas Leonard)
    
    
                                In 2013, I spent 6 months converting 0install’s 29,215 lines of Python to OCaml (learning OCaml along the way).
In this post, I’ll describe the approach I took and how it went. There will be graphs.
If you don’t want to read the whole thing, the take-away is this:
The new code is a similar length (slightly shorter), runs around 10x faster, and is statically type checked.



( This post also appeared on Hacker News and Reddit )

Table of Contents

  Background
  OCaml migration overview
  Code size
  Porting method
  Binary size
  Build time
  Speed
  Conclusions
  Epilogue


Background

Several readers said they’ve been following this blog without knowing what 0install actually is. So, a quick summary!



Since 2003, 0install’s goal has been to provide secure, cross-platform, decentralised software installation.

Secure means that 0install doesn’t grant the software root access when you install it (like most package managers do), and doesn’t allow packages to conflict wi…Read more...In 2013, I spent 6 months converting 0install’s 29,215 lines of Python to OCaml (learning OCaml along the way).
In this post, I’ll describe the approach I took and how it went. There will be graphs.
If you don’t want to read the whole thing, the take-away is this:
The new code is a similar length (slightly shorter), runs around 10x faster, and is statically type checked.



( This post also appeared on Hacker News and Reddit )

Table of Contents

  Background
  OCaml migration overview
  Code size
  Porting method
  Binary size
  Build time
  Speed
  Conclusions
  Epilogue


Background

Several readers said they’ve been following this blog without knowing what 0install actually is. So, a quick summary!



Since 2003, 0install’s goal has been to provide secure, cross-platform, decentralised software installation.

Secure means that 0install doesn’t grant the software root access when you install it (like most package managers do), and doesn’t allow packages to conflict with each other (each version of each package goes in its own directory). It should always be safe to “install” a program with 0install, though ideally you’d use a sandbox to actually run it (we’re still waiting for a decent sandbox to turn up).

Cross-platform means it works on Linux (it’s available from the repositories of all the major Linux distributions), Unix, OS X and Windows (the Windows version is a compatible reimplementation in C#, though it does run some of the original code in a subprocess).

Decentralised means that upstream projects publish their software on their own web-sites. They still get automatic dependency handling (including dependencies on other sites), GPG signatures, automatic updates, roll-back, and support for binary and source packages.
0install can work with the native package manager (e.g. rpm or dpkg) to satisfy dependencies, in addition to downloading them as 0install packages itself.

0install was originally written in C as a Linux kernel module and user-space helper.
It made other software easy to install, but getting 0install itself was rather tricky.
My naive hope was that distributions would include it by default, but needless to say that didn’t happen.
In 2005, it was redesigned and reimplemented in Python to simplify distribution.

OCaml migration overview

  Replacing Python (Jun 2013)
  The subject of rewriting 0install in a compiled language had come up a few times, but in 2013 I had just left my job to take a year off and finally had the time for it. I didn’t have any idea what language to use so I collected suggestions and tried them all. My test-case was to read the tutorial for each language and reimplement one trivial (4 line) function of 0install in each one. I looked at various factors, including start-up time, binary size, binary compatibility, safety features, diagnostics, ease of writing, support for shared libraries, and static checking.

    Speed and binary size for the launch helper (dominated by start-up time).

    There was no very clear conclusion. Rust seemed very promising in the long term, but it was years from being ready. ATS was the fastest and smallest, but too difficult to use. Python and C# were too slow (0install needs fast start-up time). Go did poorly in almost every area I tested. But Haskell and OCaml did surprisingly well.
  
  Replacing Python: second round
  I tried Haskell and OCaml on a larger sample, converting 576 lines of Python and comparing the code.
They both did well, especially for detecting problems at compile time, but I found OCaml considerably easier to use. It also ran twice as fast.
  OCaml binary compatibility (Jul)
  OCaml can compile to bytecode or to native code. I’d hoped that the bytecode would allow us to distribute a single binary that would work everywhere (as with Java). In this post I did some experiments to check this. It almost worked, but in the end I gave up; we now build separate binaries for each platform.
  Option handling with OCaml polymorphic variants (Aug)
  Polymorphic Variants are an unusual and very powerful feature of OCaml’s type system. I found I was able to take advantage of them to check statically that all of 0install’s sub-commands handle all their options.
  Experiences with OCaml objects (Sep)
  Even though OCaml programmers rarely use objects, the language’s support for object-oriented programming was a big help in converting the existing Python code. This post looks at OO programming in OCaml and describes the things that confused me at first.
  OCaml tips (Oct)
  I go back over my first OCaml code from June, pointing out better ways to do things.
  Asynchronous Python vs OCaml (Nov)
  I add support for downloads to the OCaml, which requires using OCaml’s support for asynchronous code. I compare it with Python’s new asyncio system.
  Polymorphism for beginners (Dec)
  OCaml code is often written without explicit types, letting the compiler infer everything.
However, it’s helpful to understand the details of the type system when it comes to writing interface files (describing a restricted public interface to a module) and when trying to understand compiler error messages. After muddling through for a while, I decided it was time to understand how it actually worked.

    A lattice showing the subtype relationship.
  
  OCaml: the bugs so far (Jan)
  I’ve found OCaml to be very good at detecting problems at compile time and the code has been very reliable. Still, some bugs slip though. In this post, I go over each discovered bug that made it into a Git commit and try to work out why it happened and whether it could have been prevented.

    Bugs found by type.
  
  OCaml: what you gain (Feb)
  When I first looked at OCaml, I was mainly focused on making sure the things I needed were still available.
With the port complete, I summarise the things you gain from using OCaml.


Code size

The final OCaml code was remarkably similar in length to the original Python:

Lines of code, before and after.

The main code is slightly shorter, while the unit-tests are slightly longer (probably because I added some extra ones).
The functionality is the same, except that the OCaml adds the “0install slave” command (325 lines of OCaml) and uses Lwt
rather than its own asynchronous framework (483 lines of Python).

The Python code also included some XML files for the GTK user interface (shown in orange). In the OCaml, building the widgets
is instead done directly in the code. The OCaml version includes some module interface files (the mli files, shown in green).
These are used to control how much of a module’s implementation is visible to other modules. They make the code easier to understand,
but they’re mostly optional.

Porting method

I wanted to avoid having two separate forks of 0install (Python and OCaml).
Then most people would continue using the Python version until the OCaml version was finished, resulting in a sudden switch over and the risk of some major flaw in the whole idea going undiscovered until the end.
Also, it would encourage people to submit bug fixes and features to the Python fork, creating extra porting work for me.
Instead, I used a mix of both languages, slowly migrating functions from Python to OCaml.
The two parts communicated using JSON.

I made sure the complete set of unit-tests passed for every commit and that the software remained fully functional throughout the whole process. The graph below shows the amount of Python and OCaml code over time:

Lines of code over time.

For the first couple of months I was just adding OCaml code, duplicating lots of common helper code. For example, the OCaml version needs to be able to parse the XML selections documents, so that code is ported, but parts of the Python still need that code too, so it can’t be deleted yet. Once I start deleting Python code, progress is fairly steady until it’s all gone. A nice benefit of this approach is that you can see clearly where you are in the process.

Initially, I tried doing clean implementations of the code from the specifications. However, the existing code has a lot of special cases for weird systems and backwards-compatibility hacks, and not all of them were unit-tested. Soon, I switched to translating more literally from the Python and then cleaning it up once it was in OCaml. I kept the basic structure of the Python in most places (e.g. the same classes with the same methods). That made things much easier. Once the port was complete, I did some larger refactoring (such as making the XML type immutable). I think this worked well - refactoring is very pleasant in OCaml.

Binary size

Executable image size over time.

The binary ended up a bit bigger than I’d like. Adding the GTK and OBus libraries in particular added a lot to the size (though they are optional). The main problem with GTK is that it has to be compiled as a plugin, because we don’t know if the target system will have libgtk. If we used a single binary and the library wasn’t present, it would refuse to start. By having it in a plugin we can try to load it, but fall back to console mode if that fails. However, compiling for plugins prevents OCaml from optimising the binary by removing unused library functions, since it doesn’t know which ones might be needed by the plugins.

The binary is compiled with debug symbols, but compiling without has almost no effect (binary becomes 1.5% smaller).

Build time

Build time changes over time.

A full build takes nearly a minute, which isn’t too bad. The ocamlbuild command automatically discovers dependencies and rebuilds only what is needed, so incremental builds are usually fast and are generally reliable (the exception is that it doesn’t notice if you remove or rename a file, but you always get an error message in that case rather than an incorrect build).

Most errors are picked up by the type checker immediately at the start of the build, rather than by the unit-tests at the end. That saves a lot of time.

Two things did speed it up slightly: building the tests and the main binary with a single invocation (saves having to run the dependency checker twice) and turning on parallel builds. Parallel builds didn’t help as much as I’d hoped however.

Update: edwintorok profiled the build and noticed that 25.5% of the time is spent running a bytecode version of the camlp4 pre-processor (which we use for the Lwt syntax extension and for conditional compilation) and 10.5% is spent on a bytecode version of ocamlfind (looks like an ocamlbuild bug). 
Why ocamlbuild’s parallelization is often disappointing today looks interesting too.

Update 2: I noticed that building while the computer is busy doing something else is much faster! Looks like this is the Linux scaling governor being strange. Echoing “performance” to /sys/devices/system/cpu/cpu[0-3]/cpufreq/scaling_governor takes the build time (on my new laptop) down from 45s to 23s!

There are some changes (module aliases) coming in OCaml 4.02 which should help. Currently, if I change one of the files in the Support module (e.g. Support.Sat) then it first rebuilds Sat, then rebuilds Support with the new Sat module, then rebuilds everything that uses Support (which is everything). In reality, it only needs to rebuild Zeroinstall.Solver when Sat changes.

If you do need to modify one of the early modules and run the unit tests quickly, a good trick is to compile to byte-code rather than to native. The byte-code compiler doesn’t do cross-module inlining optimisations, which means that as long as a module’s interface doesn’t change, it doesn’t need to recompile the things that depend on it.

One interesting feature of the graph is that during December the build time increased faster in proportion to the lines of code added.
This corresponds to the time I was implementing the GTK GUI, so it looks like GUI code takes longer to compile than normal code of the same length.

Speed

And the final result: running various operations with the old and new versions:

            Test
      Python 3
      OCaml
      Speed-up
    
  
            0install --help
      103 ms
      8 ms
      12.9
    
          0install select 0repo
      322 ms
      38 ms
      8.5
    
          0install run -w echo armagetron
      120 ms
      15 ms
      8.0
    
          0install run armagetron --version
      153 ms
      45 ms
      3.4
    
  


The first (--help) shows the overhead of running 0install and producing some simple output.
The extra speed here really helps with tab-completion!
The second test (select) shows 0install running its SAT solver to select a compatible set of libraries to run the “0repo” application.
The third shows 0install setting up the environment to run Armagetron (-w echo echos the executable path rather than actually running it) and the fourth shows it actually running the program.

Speed of 0install in Python vs OCaml.

One other nice win is the time taken to run the unit-tests, which has dropped considerably:

Time to run the unit tests (in seconds).

The spike in the middle is the effect of the JSON bridge, where many tests involved communication between the Python and OCaml parts.

In theory, OUnit should be able to run the tests in parallel on multi-core systems, which would make it even faster, but a bug in OUnit means it doesn’t work.

Conclusions

It’s surprising to me how reliable the initial tests were.
Even though I only converted 4 lines of Python, the tests uncovered pretty much all of OCaml’s weaker aspects (non-portable bytecode, lack of support for shared libraries, relatively large binary size, and somewhat terse error messages from the standard library), meaning there were no nasty surprises during the migration.

However, the testing was less successful at uncovering the benefits (excellent type checking, reliability, exhaustive pattern matching, polymorphic variants, abstract types, easy GTK bindings, and API stability).

Blogging about the whole process was extremely useful, attracting many helpful comments, suggestions and corrections from experienced OCaml users.

Epilogue

The blog attracted the attention of the OCaml folks at Cambridge University, who do all kinds of interesting OCaml things.
As a result, I’m now working there, adding ARM support to the Mirage unikernel - an operating system written in OCaml (the Mirage web-site is all implemented in OCaml, down to and including the TCP/IP stack!).
That will have to be the subject for another blog post though…
Hide
        
      
                    by Thomas Leonard at Jun 06, 2014 
      
      
    
  


       
                  Welcome to the summer MirageOS hackers
      (Mirage OS)
    
    
                                      Following our participation in the Google Summer of Code program, we've now finalised selections.  We've also got a number of other visitors joining us to hack on Mirage over the summer time, so here are introductions!
SSL support: Hannes Mehnert and David Kaloper have been working hard on a safe OCaml TLS implementation. They're going to hack on integrating it all into working under Xen so we can make HTTPS requests (and our Twitter bot will finally be able to tweet!).  Both are also interested in formal verification of the result, and several loooong conversations with Peter Sewell will magically transform into machine specifications by summer's end, I'm reliably informed.Cloud APIs: Jyotsna Prakash will spend her summer break as part of Google Summer of Code working on improving cloud provider APIs in OCaml (modelled from her notes on how the GitHub bindings are built).  This will let the mirage command-line tool have much more natural integration with remote cloud providers …Read more...      Following our participation in the Google Summer of Code program, we've now finalised selections.  We've also got a number of other visitors joining us to hack on Mirage over the summer time, so here are introductions!
SSL support: Hannes Mehnert and David Kaloper have been working hard on a safe OCaml TLS implementation. They're going to hack on integrating it all into working under Xen so we can make HTTPS requests (and our Twitter bot will finally be able to tweet!).  Both are also interested in formal verification of the result, and several loooong conversations with Peter Sewell will magically transform into machine specifications by summer's end, I'm reliably informed.Cloud APIs: Jyotsna Prakash will spend her summer break as part of Google Summer of Code working on improving cloud provider APIs in OCaml (modelled from her notes on how the GitHub bindings are built).  This will let the mirage command-line tool have much more natural integration with remote cloud providers for executing the unikernels straight from a command-line.  If you see Jyotsna wandering around aimlessly muttering darkly about HTTP, JSON and REST, then the project is going well.Network Stack fuzzing: Mindy Preston joins us for the summer after her Hacker School stay, courtesy of the OPW program.  She's been delving into the network stack running on EC2 and figuring out how to debug issues when the unikernel is running a cloud far, far away (see the post series here: 1, 2, 3, 4).Visualization: Daniel Buenzli returns to Cambridge this summer to continue his work on extremely succinctly named graphics libaries.  His Vz, Vg and Gg libaries build a set of primitives for 2D graphics programming.  Since the libraries compile to JavaScript, we're planning to use this as the basis for visualization of Mirage applications via a built-in webserver.Modular implicits: Frederic Bour, author of the popular Merlin IDE tool is also in Cambridge this summer working on adding modular implicits to the core OCaml language. Taking inspiration from Modular Type-classes and Scala's implicits,  modular implcits allow functions to take implicit module arguments which will be filled-in by the compiler by searching the environment for a module with the appropriate type. This enables ad-hoc polymorphism in a very similar way to Haskell's type classes.Irmin storage algorithms: Benjamin Farinier (from ENS Lyon) and Matthieu Journault (from ENS Cachan) will work on datastructures for the Irmin storage system that the next version of Mirage will use.  They'll be grabbing copies of the Okasaki classic text and porting some of them into a branch-consistent form.

Of course, work continues apace by the rest of the team as usual, with a steady stream of releases that are building up to some exciting new features.  We'll be blogging about ARM support, PVHVM, Irmin storage and SSL integration just as soon as they're pushed into the stable branches.  As always, get in touch via the IRC channel (#mirage on Freenode) or the mailing lists with questions.

   Hide
        
      
                    by Anil Madhavapeddy at May 08, 2014 
      
      
    
  


       
                  Writing Planet in pure OCaml
      (Amir Chaudhry)
    
    
                                I’ve been learning OCaml for some time now but not really had a problem that
I wanted to solve. As such, my progress has been rather slow and sporadic
and I only make time for exercises when I’m travelling. In order to focus my
learning, I have to identify and tackle something specific. That’s usually
the best way to advance and I recently found something I can work on.

As I’ve been trying to write more blog posts, I want to be able to keep as
much content on my own site as possible and syndicate my posts out to other
sites I run. Put simply, I want to be able to take multiple feeds from
different sources and merge them into one feed, which will be served from
some other site. In addition, I also want to render that feed as HTML on a
webpage. All of this has to remain within the OCaml toolchain so it can be
used as part of Mirage (i.e. I can use it when
building unikernels). 

What I’m describing might sound familiar and there’s a well-known tool that
does this called Pla…Read more...I’ve been learning OCaml for some time now but not really had a problem that
I wanted to solve. As such, my progress has been rather slow and sporadic
and I only make time for exercises when I’m travelling. In order to focus my
learning, I have to identify and tackle something specific. That’s usually
the best way to advance and I recently found something I can work on.

As I’ve been trying to write more blog posts, I want to be able to keep as
much content on my own site as possible and syndicate my posts out to other
sites I run. Put simply, I want to be able to take multiple feeds from
different sources and merge them into one feed, which will be served from
some other site. In addition, I also want to render that feed as HTML on a
webpage. All of this has to remain within the OCaml toolchain so it can be
used as part of Mirage (i.e. I can use it when
building unikernels). 

What I’m describing might sound familiar and there’s a well-known tool that
does this called Planet. It’s a ‘river of news’ feed reader, which
aggregates feeds and can display posts on webpages and you can find the
original Planet and it’s successor Venus, both written in Python.
However, Venus seems to be unmaintained as there are a number of
unresolved issues and pull requests, which have been
languishing for quite some time with no discussion. There does appear to be
a more active Ruby implementation called Pluto, with recent commits and
no reported issues.



Benefits of a Planet in pure OCaml

Although I could use the one of the above options, it would be much more
useful to keep everything within the OCaml ecosystem.  This way I can make
the best use of the unikernel approach with Mirage (i.e lean,
single-purpose appliances). Obviously, the existing options don’t lend
themselves to this approach and there are known bugs as a lot has
changed on the web since Planet Venus (e.g the adoption of HTML5).
Having said that, I can learn a lot from the existing implementations and
I’m glad I’m not embarking into completely uncharted territory.

In addition, the OCaml version doesn’t need to (and shouldn’t) be written
as one monolithic library.  Instead, pulling together a collection of
smaller, reusable libraries that present clear interfaces to each other
would make things much more maintainable. This would bring substantially
greater benefits to everyone and OPAM can manage the dependencies. 



Breaking down the problem

The first cut is somewhat straightforward as we have a piece that deals with
the consumption and manipulation of feeds and another that takes the result
and emits HTML. This is also how the original Planet is put together, with a
library called feedparser and another for templating pages.  

For the feed-parsing aspect, I can break it down further by considering Atom
and RSS feeds separately and then even further by thinking about how to (1)
consume such feeds and (2) output them. Then there is the HTML component,
where it may be necessary to consider existing representations of HTML. These
are not new ideas and since I’m claiming that individual pieces might be
useful then it’s worth finding out which ones are already available.

Existing components

The easiest way to find existing libraries is via the
OPAM package list. Some quick searches for RSS, XML, HTML
and net bring up a lot of packages. The most relevant of these seem to be
xmlm, ocamlrss, cow and maybe xmldiff. I noticed that
nothing appears, when searching for Atom, but I do know that cow has an
Atom module for creating feeds. In terms of turning feeds into pages and
HTML, I’m aware of rss2html used on the OCaml website and parts of
ocamlnet that may be relevant (e.g nethtml and netstring) as well as
cow.  There is likely to be other code I’m missing but this is useful as a
first pass. 

Overall, a number of components are already out there but it’s not obvious
if they’re compatible (e.g html) and there are still gaps (e.g atom). Since
I also want to minimise dependencies, I’ll try and use whatever works but
may ultimately have to roll my own. Either way, I can learn from what
already exists. Perhaps I’m being overconfident but if I can break things
down sensibly and keep the scope constrained then this should be an
achievable project. 

The first (baby) steps - an Atom parser

As this is an exercise for me to learn OCaml by solving a problem, I need to
break it down into bite-size pieces and take each one at a time. Practically
speaking, this means limiting the scope to be as narrow as possible while
still producing a useful result for me.  That last part is important as I
have specific needs and it’s likely that the first thing I make won’t be
particularly interesting for many others. 

For my specific use-case, I’m only interested in dealing with Atom feeds as
that’s what I use on my site and others I’m involved with. Initial feedback
is that creating an Atom parser will be the bulk of the work and I should
start by defining the types. To keep this manageable, I’m only going to deal
with my own feeds instead of attempting a fully compliant parser (in other
words, I’ll only consider the subset of RFC4287 that’s relevant to me).
Once I can parse, merge and write such feeds I should be able to iterate
from there. 

To make my requirements more concrete:

  Only consider my own Atom feeds for now
  Initially, be able to parse and emit just one Atom feed
  Then be able to merge 2+ feeds, specifically:
          Use tag-based feeds from my personal site as starting points
      Be able to de-dupe content
    
  
  No database or storage (construct it afresh every time)
  Minimise library dependencies




Timeframes and workflow

I’ve honestly no idea how long this might take and I’m treating it as a
side-project. I know there are many people out there who could produce a
working version of everything in a week or two but I’m not one of them (yet).
There are also a lot of ancillary things I need to learn on the way, like
packaging, improving my knowledge of Git and dealing with build systems. If
I had to put a vague time frame on this, I’d be thinking in months rather
than weeks.  It might even be the case that others start work on parts of
this and ship things sooner but that’s great as I’ll probably be able to use
whatever they create and move further along the chain.

In terms of workflow, everything will be done in the open, warts and all, and
I expect to make embarrassing mistakes as I go. You can follow along on my
freshly created OCaml Atom repo, and I’ll be using the issue tracker as
the main way of dealing with bugs and features. Let the fun begin.




Acknowledgements: Thanks to Daniel, Ashish, Christophe,
Philippe and Thomas for discussions on an earlier draft of this post
and providing feedback on my approach.



Hide
        
      
                    by Amir Chaudhry at Apr 29, 2014 
      
      
    
  


       
                  Fifth OCaml compiler hacking session
      (Compiler Hacking)
    
    
                                It's time for the fifth Cambridge OCaml compiler-hacking session!  We'll be meeting in the Computer Lab again next Tuesday evening.

If you're planning to come along, it'd be helpful if you could indicate interest via Doodle and sign up to the mailing list to receive updates:

Where: Room FW11, Computer Laboratory, Madingley Road

When: 6pm, Tuesday 29th April

Who: anyone interested in improving OCaml. Knowledge of OCaml programming will obviously be helpful, but prior experience of working on OCaml internals isn't necessary.

What: fixing bugs, implementing new features, learning about OCaml internals

Wiki: https://github.com/ocamllabs/compiler-hacking/wiki

We're defining "compiler" pretty broadly, to include anything that's part of the standard distribution, which means at least the standard library, runtime, tools (ocamldep, ocamllex, ocamlyacc, etc.), ocamlbuild, the documentation, and the compiler itself. We'll have suggestions for mini-projects for various levels of experience…Read more...It's time for the fifth Cambridge OCaml compiler-hacking session!  We'll be meeting in the Computer Lab again next Tuesday evening.

If you're planning to come along, it'd be helpful if you could indicate interest via Doodle and sign up to the mailing list to receive updates:

Where: Room FW11, Computer Laboratory, Madingley Road

When: 6pm, Tuesday 29th April

Who: anyone interested in improving OCaml. Knowledge of OCaml programming will obviously be helpful, but prior experience of working on OCaml internals isn't necessary.

What: fixing bugs, implementing new features, learning about OCaml internals

Wiki: https://github.com/ocamllabs/compiler-hacking/wiki

We're defining "compiler" pretty broadly, to include anything that's part of the standard distribution, which means at least the standard library, runtime, tools (ocamldep, ocamllex, ocamlyacc, etc.), ocamlbuild, the documentation, and the compiler itself. We'll have suggestions for mini-projects for various levels of experience (see also some things we've worked on in previous sessions), but feel free to come along and work on whatever you fancy.

We'll also be ordering pizza, so if you want to be counted for food you should aim to arrive by 6.30pm.
Hide
        
      
                    by Compiler Hacking at Apr 24, 2014 
      
      
    
  


       
                  From Jekyll site to Unikernel in fifty lines of code.
      (Amir Chaudhry)
    
    
                                Mirage has reached a point where it’s possible to easily set up
end-to-end toolchains to build unikernels! 
My first use-case is to be able to generate a unikernel which can serve my
personal static site but to do it with as much automation as possible. It
turns out this is possible with less than 50 lines of code.

I use Jekyll and GitHub Pages at the moment so I wanted a workflow that’s as
easy to use, though I’m happy to spend some time up front to set up and
configure things.
The tools for achieving what I want are in good shape so
this post takes the example of a Jekyll site (i.e this one) and goes through
the steps to produce a unikernel on
Travis CI (a continuous integration service) which can later be
deployed.  Many of these instructions already exist in various forms but
they’re collated here to aid this use-case.  

I will take you, dear reader, through the process and when we’re finished,
the workflow will be as follows:

  You’ll write your posts on your loca…Read more...Mirage has reached a point where it’s possible to easily set up
end-to-end toolchains to build unikernels! 
My first use-case is to be able to generate a unikernel which can serve my
personal static site but to do it with as much automation as possible. It
turns out this is possible with less than 50 lines of code.

I use Jekyll and GitHub Pages at the moment so I wanted a workflow that’s as
easy to use, though I’m happy to spend some time up front to set up and
configure things.
The tools for achieving what I want are in good shape so
this post takes the example of a Jekyll site (i.e this one) and goes through
the steps to produce a unikernel on
Travis CI (a continuous integration service) which can later be
deployed.  Many of these instructions already exist in various forms but
they’re collated here to aid this use-case.  

I will take you, dear reader, through the process and when we’re finished,
the workflow will be as follows:

  You’ll write your posts on your local machine as normal
  A push to GitHub will trigger a unikernel build for each commit
  The Xen unikernel will be pushed to a repo for deployment


To achieve this, we’ll first check that we can build a unikernel VM locally,
then we’ll set up a continuous integration service to automatically build
them for us and finally we’ll adapt the CI service to also deploy the built
VM.  Although the amount of code required is small, each of these steps is
covered below in some detail.
For simplicity, I’ll assume you already have OCaml and Opam
installed – if not, you can find out how via the
Real Word OCaml install instructions.

Building locally

To ensure that the build actually works, you should run things locally at
least once before pushing to Travis.  It’s worth noting that the
mirage-skeleton repo contains a lot of useful, public domain examples
and helpfully, the specific code we need is in
mirage-skeleton/static_website.  Copy both the config.ml
and dispatch.ml files from that folder into a new _mirage folder in your
jekyll repository.

Edit config.ml so that the two mentions of ./htdocs are replaced with
../_site.  This is the only change you’ll need to make and you should now
be able to build the unikernel with the unix backend.  Make sure you have
the mirage package installed by running $ opam install mirage and then run:

(edit: If you already have mirage, remember to opam update to make sure you’ve got the latest packages.)

$ cd _mirage
$ mirage configure --unix
$ make depend         # needed as of mirage 1.2 onward
$ mirage build
$ cd ..

That’s all it takes!  In a few minutes there will be a unikernel built on
your system (symlinked as _mirage/mir-www).  If there are any errors, make
sure that Opam is up to date and that you have the latest version of the
static_website files from mirage-skeleton. 

Serving the site locally

If you’d like to see this site locally, you can do so from within the
_mirage folder by running unikernel you just built.  There’s more
information about the details of this on the Mirage docs site
but the quick instructions are:

$ cd _mirage
$ sudo mirage run

# in another terminal window
$ sudo ifconfig tap0 10.0.0.1 255.255.255.0

You can now point your browser at http://10.0.0.2/ and see your site!
Once you’re finished browsing, $ mirage clean will clear up all the
generated files. 

Since the build is working locally, we can set up a continuous integration
system to perform the builds for us.

Setting up Travis CI
</img>

We’ll be using the Travis CI service, which is free for open-source
projects (so this assumes you’re using a public repo).  The benefit of using
Travis is that you can build a unikernel without needing a local OCaml
environment, but it’s always quicker to debug things locally.

Log in to Travis using your GitHub ID which will then trigger a scan of your
repositories.  When this is complete, go to your Travis accounts page and
find the repo you’ll be building the unikernel from.  Switch it ‘on’ and
Travis will automatically set your GitHub post-commit hook and token for you.
That’s all you need to do on the website.

When you next make a push to your repository, GitHub will inform Travis,
which will then look for a YAML file in the root of the repo called
.travis.yml.  That file describes what Travis should do and what the build
matrix is.  Since OCaml is not one of the supported languages, we’ll be
writing our build script manually (this is actually easier than it sounds).
First, let’s set up the YAML file and then we’ll examine the build script.

The Travis YAML file - .travis.yml

The Travis CI environment is based on Ubuntu 12.04, with a
number of things pre-installed (e.g Git, networking tools etc).  Travis
doesn’t support OCaml (yet) so we’ll use the c environment to get the
packages we need, specifically, the OCaml compiler, Opam and Mirage. Once
those are set up, our build should run pretty much the same as it did locally.

For now, let’s keep things simple and only focus on the latest releases
(OCaml 4.01.0 and Opam 1.1.1), which means our build matrix is very simple.
The build instructions will be in the file _mirage/travis.sh, which we
will move to and trigger from the .travis.yml file.  This means our YAML
file should look like:

language: c
before_script: cd _mirage
script: bash -ex travis.sh
env:
  matrix:
  - MIRAGE_BACKEND=xen DEPLOY=0
  - MIRAGE_BACKEND=unix

The matrix enables us to have parallel builds for different environments and
this one is very simple as it’s only building two unikernels. One worker
will build for the Xen backend and another worker will build for the Unix
backend. The _mirage/travis.sh script will clarify what each of these
environments translates to.  We’ll come back to the DEPLOY flag later on
(it’s not necessary yet).  Now that this file is set up, we can work on the
build script itself.

The build script - travis.sh

To save time, we’ll be using an Ubuntu PPA to quickly get
pre-packaged versions of the OCaml compiler and Opam, so the
first thing to do is define which PPAs each line of the build matrix
corresponds to.  Since we’re keeping things simple, we only need one PPA
that has the most recent releases of OCaml and Opam.

#!/usr/bin/env bash
ppa=avsm/ocaml41+opam11
echo "yes" | sudo add-apt-repository ppa:$ppa
sudo apt-get update -qq
sudo apt-get install -qq ocaml ocaml-native-compilers camlp4-extra opam

[NB: There are many other PPAs for different combinations of
OCaml/Opam which are useful for testing].  Once the appropriate PPAs have
been set up it’s time to initialise Opam and install Mirage. 

export OPAMYES=1
opam init
opam install mirage
eval `opam config env`

We set OPAMYES=1 to get non-interactive use of Opam (it defaults to ‘yes’
for any user input) and if we want full build logs, we could also set
OPAMVERBOSE=1 (I haven’t in this example).
The rest should be straight-forward and you’ll end up with an
Ubuntu machine with OCaml, Opam and the Mirage package installed.  It’s now
trivial to do the next step of actually building the unikernel!

mirage configure --$MIRAGE_BACKEND
mirage build

You can see how we’ve used the environment variable from the Travis file and
this is where our two parallel builds begin to diverge.  When you’ve saved
this file, you’ll need to change permissions to make it executable by doing
$ chmod +x _mirage/travis.sh.

That’s all you need to build the unikernel on Travis!  You should now commit
both the YAML file and the build script to the repo and push the changes to
GitHub.  Travis should automatically start your first build and you can
watch the console output online to check that both the Xen and Unix backends
complete properly.  If you notice any errors, you should go back over your
build script and fix it before the next step.

Deploying your unikernel
</img>

When Travis has finished its builds it will simply destroy the worker and
all its contents, including the unikernels we just built.  This is perfectly
fine for testing but if we want to also deploy a unikernel, we need to get
it out of the Travis worker after it’s built. In this case, we want to
extract the Xen-based unikernel so that we can later start it on a Xen-based
machine (e.g Amazon, Rackspace or - in our case - a machine on Bytemark).

Since the unikernel VMs are small (only tens of MB), our method for
exporting will be to commit the Xen unikernel into a repository on GitHub.
It can be retrieved and started later on and keeping the VMs in version
control gives us very effective snapshots (we can roll back the site without
having to rebuild).  This is something that would be much more challenging
if we were using the ‘standard’ web toolstack.

The deployment step is a little more complex as we have to send the
Travis worker a private SSH key, which will give it push access to a GitHub
repository.  Of course, we don’t want to expose that key by simply adding it
to the Travis file so we have to encrypt it somehow. 

Sending Travis a private SSH key

Travis supports encrypted environment variables. Each
repository has its own public key and the Travis gem uses
this public key to encrypt data, which you then add to your .travis.yml
file for decryption by the worker.  This is meant for sending things like
private API tokens and other small amounts of data. Trying to encrypt an SSH
key isn’t going to work as it’s too large.  Instead we’ll use
travis-senv, which encodes, encrypts and chunks up the key into smaller
pieces and then reassembles those pieces on the Travis worker.  We still use
the Travis gem to encrypt the pieces to add them to the .travis.yml file.

While you could give Travis a key that accesses your whole GitHub account, my
preference is to create a new deploy key, which will only be used for
deployment to one repository.

# make a key pair on your local machine
$ cd ~/.ssh/
$ ssh-keygen -t dsa -C "travis.deploy" -f travis-deploy_dsa
$ cd -

Note that this is a 1024 bit key so if you decide to use a 2048 bit key,
then be aware that Travis sometimes has issues. Now that we have
a key, we can encrypt it and add it to the Travis file. 

# on your local machine

# install the necessary components
$ gem install travis
$ opam install travis-senv

# chunk the key, add to yml file and rm the intermediate
$ travis-senv encrypt ~/.ssh/travis-deploy_dsa _travis_env
$ cat _travis_env | travis encrypt -ps --add
$ rm _travis_env

travis-senv encrypts and chunks the key locally on your machine, placing
its output in a file you decide (_travis_env).  We then take that output
file and pipe it to the travis ruby gem, asking it to encrypt the input,
treating each line as separate and to be appended (-ps) and then actually
adding that to the Travis file (--add).  You can run $ travis encrypt -h
to understand these options.  Once you’ve run the above commands,
.travis.yml will look as follows.

language: c
before_script: cd _mirage
script: bash -ex travis.sh
env:
  matrix:
  - MIRAGE_BACKEND=xen DEPLOY=0
  - MIRAGE_BACKEND=unix
  global:
  - secure: ".... encrypted data ...."
  - secure: ".... encrypted data ...."
  - secure: ".... encrypted data ...."
  ...

The number of secure variables added depends on the type and size of the key
you had to chunk, so it could vary from 8 up to 29. We’ll commit
these additions later on, alongside additions to the build script.

At this point, we also need to make a repository on GitHub
and add the public deploy key so
that Travis can push to it.  Once you’ve created your repo and added a
README, follow GitHub’s instructions on adding deploy keys
and paste in the public key (i.e. the content of travis-deploy_dsa.pub).  

Now that we can securely pass a private SSH key to the worker
and have a repo that the worker can push to, we need to
make additions to the build script.

Committing the unikernel to a repository

Since we can set DEPLOY=1 in the YAML file we only need to make
additions to the build script.  Specifically, we want to assure that: only
the Xen backend is deployed; only pushes to the repo result in
deployments, not pull requests (we do still want builds for pull requests).

In the build script (_mirage/travis.sh), which is being run by the worker,
we’ll have to reconstruct the SSH key and configure Git.  In addition,
Travis gives us a set of useful environment variables so we’ll
use the latest commit hash ($TRAVIS_COMMIT) to name the the VM (which also
helps us trace which commit it was built from).

It’s easier to consider this section of code at once so I’ve explained the
details in the comments. This section is what you need to add at the end of
your existing build script (i.e straight after mirage build).

# Only deploy if the following conditions are met.
if [ "$MIRAGE_BACKEND" = "xen" \
            -a "$DEPLOY" = "1" \
            -a "$TRAVIS_PULL_REQUEST" = "false" ]; then

    # The Travis worker will already have access to the chunks
    # passed in via the yaml file. Now we need to reconstruct 
    # the GitHub SSH key from those and set up the config file.
    opam install travis-senv
    mkdir -p ~/.ssh
    travis-senv decrypt > ~/.ssh/id_dsa # This doesn't expose it
    chmod 600 ~/.ssh/id_dsa             # Owner can read and write
    echo "Host some_user github.com"   >> ~/.ssh/config
    echo "  Hostname github.com"          >> ~/.ssh/config
    echo "  StrictHostKeyChecking no"     >> ~/.ssh/config
    echo "  CheckHostIP no"               >> ~/.ssh/config
    echo "  UserKnownHostsFile=/dev/null" >> ~/.ssh/config

    # Configure the worker's git details
    # otherwise git actions will fail.
    git config --global user.email "user@example.com"
    git config --global user.name "Travis Build Bot"

    # Do the actual work for deployment.
    # Clone the deployment repo. Notice the user,
    # which is the same as in the ~/.ssh/config file.
    git clone git@some_user:amirmc/www-test-deploy
    cd www-test-deploy

    # Make a folder named for the commit. 
    # If we're rebuiling a VM from a previous
    # commit, then we need to clear the old one.
    # Then copy in both the config file and VM.
    rm -rf $TRAVIS_COMMIT
    mkdir -p $TRAVIS_COMMIT
    cp ../mir-www.xen ../config.ml $TRAVIS_COMMIT

    # Compress the VM and add a text file to note
    # the commit of the most recently built VM.
    bzip2 -9 $TRAVIS_COMMIT/mir-www.xen
    git pull --rebase
    echo $TRAVIS_COMMIT > latest    # update ref to most recent

    # Add, commit and push the changes!
    git add $TRAVIS_COMMIT latest
    git commit -m "adding $TRAVIS_COMMIT built for $MIRAGE_BACKEND"
    git push origin master
    # Go out and enjoy the Sun!
fi

At this point you should commit the changes to ./travis.yml (don’t forget
the deploy flag) and _mirage/travis.sh and push the changes to GitHub.
Everything else will take place automatically and in a few minutes you will
have a unikernel ready to deploy on top of Xen! 

You can see both the complete YAML file and build script in use on my
test repo, as well as the build logs for that repo
and the deploy repo with a VM.

[Pro-tip: If you add *[skip ci] anywhere in your
commit message, Travis will skip the build for that commit.
This is very useful if you’re making minor changes, like updating a
README.]*

Finishing up

Since I’m still using Jekyll for my website, I made a short script in my
jekyll repository (_deploy-unikernel.sh) that builds the site, commits the
contents of _site and pushes to GitHub. I simply run this after I’ve
committed a new blog post and the rest takes care of itself.

#!/usr/bin/env bash
jekyll build
git add _site
git commit -m 'update _site'
git push origin master

Congratulations! You now have an end-to-end workflow that will produce a
unikernel VM from your Jekyll-based site and push it to a repo.  If you
strip out all the comments, you’ll see that we’ve written less than 50 lines
of code! Admittedly, I’m not counting the 80 or so lines that came for free
in the *.ml files but that’s still pretty impressive.

Of course, we still need a machine to take that VM and run it but that’s a
topic for another post.  For the time-being, I’m still using GitHub Pages
but once the VM is hosted somewhere, I will:

  Turn off GitHub Pages and serve from the VM – but still using Jekyll in
the workflow.
  Replace Jekyll with OCaml-based static-site generation.


Although all the tools already exist to switch now, I’m taking my time so
that I can easily maintain the code I end up using.

Expanding the script for testing

You may have noticed that the examples here are not very flexible or
extensible but that was a deliberate choice to keep them readable.  It’s
possible to do much more with the build matrix and script, as you can see
from the Travis files on my website repo, which were based on
those of the Mirage site and Mort’s site.
Specifically, you can note the use of more environment variables and case
statements to decide which PPAs to grab.  Once you’ve got your builds
working, it’s worth improving your scripts to make them more maintainable
and cover the test cases you feel are important.

Not just for static sites (surprise!)

You might have noticed that in very few places in the toolchain above have I
mentioned anything specific to static sites per se.  The workflow is simply
(1) do some stuff locally, (2) push to a continuous integration service
which then (3) builds and deploys a Xen-based unikernel.  Apart from the
convenient folder structure, the specific work to treat this as a static
site lives in the *.ml files, which I’ve skipped over for this post.  

As such, the GitHub+Travis workflow we’ve developed here is quite general
and will apply to almost any unikernels that we may want to construct.
I encourage you to explore the examples in the mirage-skeleton repo and
keep your build script maintainable.  We’ll be using it again the next time
we build unikernel devices.


Acknowledgements: There were lots of things I read over while writing this
post but there were a few particularly useful things that you should look up.
Anil’s posts on Testing with Travis and
Travis for secure deployments are quite succinct (and
were themselves prompted by Mike Lin’s Travis post several
months earlier). Looking over Mort’s build script and that of
mirage-www helped me figure out the deployment steps as well as improve
my own script. Special thanks also to Daniel, Leo and Anil for
commenting on an earlier draft of this post.

Hide
        
      
                    by Amir Chaudhry at Mar 10, 2014 
      
      
    
  


       
                  MirageOS is in Google Summer of Code 2014
      (Mirage OS)
    
    
                                      MirageOS will part of the Google Summer of Code 2014
program, thanks to the Xen Project's participation!  It's been a few years
since I've mentored for GSoc, but I still have fond memories of some great
projects in the past (such as the legendary Quake testing
we were forced to do for hours on end).  I've already received a number of
queries about this year's program from potential students, so here's a few
things to note to become a successful applicant.
Students still need to apply and be accepted. Your chances of being
 selected are much higher if you demonstrate some participation and
 code contributions (even minor) before submitting an application.
 Thus, even if you don't have a copy of Xen around, roll up your sleeves
 and head over to the installation instructions.
Contributions do not have to be just code.  They can be documentation,
 help with packaging, wiki posts about a particular use, or test cases
 to improve code coverage.
It's unlikely that we'll get students w…Read more...      MirageOS will part of the Google Summer of Code 2014
program, thanks to the Xen Project's participation!  It's been a few years
since I've mentored for GSoc, but I still have fond memories of some great
projects in the past (such as the legendary Quake testing
we were forced to do for hours on end).  I've already received a number of
queries about this year's program from potential students, so here's a few
things to note to become a successful applicant.
Students still need to apply and be accepted. Your chances of being
 selected are much higher if you demonstrate some participation and
 code contributions (even minor) before submitting an application.
 Thus, even if you don't have a copy of Xen around, roll up your sleeves
 and head over to the installation instructions.
Contributions do not have to be just code.  They can be documentation,
 help with packaging, wiki posts about a particular use, or test cases
 to improve code coverage.
It's unlikely that we'll get students who are very familiar with both
 OCaml and Xen (if you are, definitely get in touch with us!).  You should
 therefore look over the project ideas
 as a set of guidelines and not firm suggestions.  If you have a particular
 thing you'd like to do with Mirage (for example, work on the JavaScript
 backend, an IPython interface or
 a particular protocol implementation such as XMPP, then that's fine.  Just
 get in touch with us on the mailing lists or directly via
 e-mail, and we can work through them.
Under some circumstances, we can provide resources such as a login to
 a Xen machine, or delegated credits on a cloud provider.  Don't let that
 stop you from applying for a project idea.  In general though, it's best
 to only depend on your own computer resources if practical to do so.

   Hide
        
      
                    by Anil Madhavapeddy at Feb 25, 2014 
      
      
    
  


       
                  OCaml: what you gain
      (Thomas Leonard)
    
    
                                Way back in June, in Replacing Python: second round, I wrote:

  The big surprise for me in these tests was how little you lose going from Python to OCaml.


Of course, I was mainly focused on making sure the things I needed were still available. With the port now complete (0install 2.6 has been released, and contains no Python code), here’s a summary of the main things you gain.



Table of Contents

  Functional programming
  Type-checking
  Data type definitions
  Polymorphic variants
  Immutability
  Abstraction
  Speed
  No dependency cycles
  GUI code
  API stability
  Summary


This post also appeared on Hacker News and Reddit, where there are more comments.

( This post is part of a series in which I
converted 0install from Python to OCaml, learning OCaml in the process. The full code is at GitHub/0install. )

A note on bias

I started these blog posts unemployed (taking a career break), with no particular connection to any of the languages, and motivated to make a good cho…Read more...Way back in June, in Replacing Python: second round, I wrote:

  The big surprise for me in these tests was how little you lose going from Python to OCaml.


Of course, I was mainly focused on making sure the things I needed were still available. With the port now complete (0install 2.6 has been released, and contains no Python code), here’s a summary of the main things you gain.



Table of Contents

  Functional programming
  Type-checking
  Data type definitions
  Polymorphic variants
  Immutability
  Abstraction
  Speed
  No dependency cycles
  GUI code
  API stability
  Summary


This post also appeared on Hacker News and Reddit, where there are more comments.

( This post is part of a series in which I
converted 0install from Python to OCaml, learning OCaml in the process. The full code is at GitHub/0install. )

A note on bias

I started these blog posts unemployed (taking a career break), with no particular connection to any of the languages, and motivated to make a good choice since I’d be using it a lot. I wasn’t biased towards OCaml; it wasn’t even on my list of candidates until a complete stranger suggested it on the mailing list.
But I must now disclose that, since my last blog post, I’m now getting paid for writing OCaml.

Functional programming

Some people commented it was good to see more projects moving to functional programming. So, what’s it like doing functional programming after Python? To be honest, not much has changed. According to OCaml’s What is functional programming?, “In a functional language, functions are first-class citizens” and “The fact is that Perl is actually quite a good functional language”.

So, if you’ve ever used Python’s (built-in) map, reduce, filter or apply functions, ever written or used a decorator or ever passed a function as an argument to another function, you’re already doing functional programming as far as OCaml is concerned. By contrast, “pure functional programming” (as in Haskell) would be a major change.

OCaml does make partially applying functions easier, which is sometimes convenient, and it supports tail recursion. Tail recursion allows you to write loops in a functional style (without needing break, continue or mutable state). That can make it easier to reason about loops, but I couldn’t find any examples in 0install where this style was clearly better than a plain Python loop.

Type-checking

I’ve used statically-typed languages before (I used to program in Java for my day job). That can catch many errors that Python would miss, but OCaml’s type system is far more useful than Java’s. Here’s an example, where we want to display an icon for some program in the GUI:

1
2
get_icon program
|> widget#set_icon	(* Error! *)


Error: This expression has type Icon.t -> unit
       but an expression was expected of type Icon.t option -> 'a


Oops. The program might not have an icon (icons are optional). We’ll need to use a default one in that case:

1
2
3
get_icon program
|> default generic_program_icon
|> widget#set_icon


Downloading some data:

1
2
3
4
match download url with
| `success data -> process data
| `network_failure msg -> show_error_dialog msg
(* Error! *)


Warning 8: this pattern-matching is not exhaustive.
Here is an example of a value that is not matched:
`aborted_by_user


Oops. The user might click the “Cancel” button - we need to handle that too:

1
2
3
4
match download url with
| `success data -> process data
| `network_failure msg -> show_error_dialog msg
| `aborted_by_user -> abort ()


When registering an extra feed to an interface we want to download it first to check it exists:

1
2
3
let add_feed iface (feed_url:feed_url) =
  download_feed feed_url;	(* Error! *)
  register_feed iface feed_url


Error: This expression has type feed_url
       but an expression was expected of type [< `remote_feed of url ]
       The second variant type does not allow tag(s) `local_feed


Oops. The user might specify a local file too:

1
2
3
4
5
let add_feed iface (feed_url:feed_url) =
  begin match feed_url with
  | `remote_feed _ as feed_url -> download_feed feed_url
  | `local_feed path -> check_file_exists path end;
  register_feed iface feed_url


Java makes you do all the work for static type checking, but manages to miss many of the benefits.
No matter how much care you take with your Java types, there’s always a good chance you’re going to crash with a NullPointerException.
By requiring correct handling of None (null) and ensuring pattern matching is exhaustive, OCaml’s type checking is far more useful. As with Haskell, when a piece of OCaml code compiles successfully, there’s a very good chance it will work first time.

And, of course, static checking makes refactoring much easier than in Python. For example, if you remove or rename something, the compiler will always find every place you need to update.

Data type definitions

OCaml makes it really easy to define new data types as you need them. The types are always easy to see, and you know that OCaml will enforce them (unlike comments in Python, which may be incorrect). Here’s a record type for the configuration settings for an interface (an optional stability level and a list of extra feeds):

1
2
3
4
type interface_config = {
  stability_policy : stability_level option;
  extra_feeds : Feed.feed_import list;
}


And here’s a variant (enum / tagged union / sum) type for the result of a download:

1
2
3
4
type download_result =
  [ `aborted_by_user
  | `network_failure of string
  | `success of filepath ]


Polymorphic variants

The OCaml labels tutorial describes polymorphic variants as a way to use the same name (e.g. Open) for different things (e.g. opening a door vs opening a lock) and says:

  “Because of the reduction in type safety, it is recommended that you don’t use these in your code”.


This is quite misleading (and I was quickly corrected when I repeated it).
Their real purpose is to support subsets and supersets, which are useful all over the place. Some examples:

      The “0install” command-line parser accepts a large number of options. The “0install run” subcommand accepts a subset of these. That subset can be further subdivided into common options (present in all commands, such as --verbose), options common to selection commands (e.g. --before=VERSION) and those specific to “0install run” (e.g. --wrapper=COMMAND).
  
      The GUI code that handles dialog responses (OK, Cancel, etc) must handle the union of all the action button responses it added and the always-present window close icon.
  
      The download code only handles the subset of feed URLs that represent remote resources.
  
      Users can only register local and remote feeds to an interface, not distribution-provided virtual feeds.
  
      Cached feeds contain only remote implementations, local feeds contain local and remote implementations, and distribution feeds contain only distribution implementations. All three types get combined together and passed to the solver.
  


Here’s an example, showing the run command dividing its options into sub-groups, with the compiler checking that every option will be handled in all cases:

1
2
3
4
5
6
7
let select_opts = ref [] in
Support.Argparse.iter_options flags (function
  | #common_option as o -> Common_options.process_common_option options o
  | #select_option | `Refresh as o -> select_opts := o :: !select_opts
  | `Wrapper w -> run_opts.wrapper <- Some w
  | `MainExecutable m -> run_opts.main <- Some m
);


Without polymorphic variants, OCaml’s exhaustive matching requirements mean you’d have to provide code to handle cases that (you think) can’t happen. That’s tedious and your program will crash if you get it wrong. Polymorphic variants mean you can prove to the compiler that only the correct subset needs to be handled at each point in the code. This is fantastic, and I can’t think of any other major language that does this (though I’m sure people will suggest some in the comments).

Immutability

In OCaml, all variables and record fields are immutable by default. This is far saner than Java (where the default is mutable and you must use final everywhere to override it). Immutable is a better default because:

  Typically, you want most things to be immutable (in any language).
  If you forget to mark something as mutable, the compiler will quickly let you know, whereas forgetting to mark something as immutable would be missed.


With mutable structures, you are always worrying about whether one piece of code will mutate a structure that another is relying on. For example, I originally made the XML element type mutable, but I found I was writing comments like this:

1
2
(** Note: this returns the actual internal XML; don't modify it *)
val as_xml : selections -> element


After removing the mutable annotations from the element declaration, the compiler showed me each piece of code I needed to modify to make it work again. Then, I was able to remove those notes.

[ The main difficulty in this conversion was handling XML namespace prefixes. Previously, each element had a reference to its owning document, which held a shared (mutable) pool of prefix bindings. Now, each namespaced item holds its preferred prefix, and the output code builds up a set of bindings before writing out the tree. ]

There is one case where Python and Java do better than OCaml: OCaml strings are mutable! The convention is to treat them as immutable, though.

Update: OCaml 4.02 has an option for immutable strings, with a separate Bytes.t for mutable byte arrays.

Abstraction

OCaml makes it very easy to hide a module’s implementation details from its users using abstract types.

I gave one example in the bugs post, where hiding the fact that a sorted tree is really the same type as an unsorted one prevents bugs due to mixing them up.

Here’s another example. In the Python code, we would parse a selections XML document into a Selections object, like this:

1
2
3
class Selections(object):
	def __init__(self, root_element):
		...


I found all this parsing and serialising complicated things and so in the OCaml rewrite I decided to use the plain XML element type everywhere.
That did simplify things, but it also removed some safety and clarity from the APIs.
Consider the Selections.create function (which now does nothing unless the document is in an old format and needs to be upgraded):

1
2
3
4
let create root =
  ZI.check_tag "selections" root;
  if is_latest_format root then root
  else convert_to_latest root


It’s nice and simple, but it just returns an element. It would be easy to pass some other XML element to a function that only works on selection documents (or to pass a document that’s still in the old format).
We can solve this simply by declaring an abstract type for selection documents in the interface file (selections.mli):

1
2
type t
val create : element -> t


(note: it’s an OCaml convention for a module’s main type to be called t; other modules will refer to this type as Selections.t)

I think this gets the best of both worlds. Internally, a selections object is just the XML root element, which is simple and efficient, but code using it can’t mix up the types. And, of course, we can change the internal type later if needed without breaking anything.

This isn’t a particularly novel idea (you can do something similar in C). However, Python and Java would require you to write a wrapper object around the object you want to hide, and Python makes it easy for users of the API to access the internal representation even then. If you’re writing a library, OCaml (like C) makes it clear when you’re changing the module’s interface vs merely changing its implementation.

( There is another interesting feature, which I haven’t used yet: you can use the “private” modifier to say that users of the module can see the structure of the type but can’t create their own instances of it. For example, saying type t = private element would allow users to cast a selections value to an XML element, but not to treat any old XML as a selections value. )

I did experience one case where abstraction didn’t work as intended. In the SAT solver, I declared the type of a literal abstractly as type lit and, internally, I used type lit = int (an array index). That worked fine. Later, I changed the internal representation from an int to a record. Ideally, that would have no effect on users of the module, but OCaml allows testing abstract types for equality, which resulted in each comparison recursively exploring the whole SAT problem. It can also cause runtime crashes if it encounters a function in this traversal. Haskell’s type classes avoid this problem by letting you control which types can be compared and how the comparison should be done.

Speed

Python is well known for being slow, but much of what real programs do is simply calling C libraries.
For example, when calculating a SHA256 digest, C does all the heavy lifting.

Despite this, I’ve found OCaml to be fairly consistently 10 times faster in macro benchmarks (measuring a complete run of 0install from start to finish). Also, although I’ve added a lot of code and dependencies since the initial benchmarks, it still runs almost as quickly.
The 0release benchmark took 8ms with June’s minimal version, compared to 10ms with the final version.

When doing pure calculations (e.g. a tight loop adding integers), OCaml is typically more than 100x faster than Python.

Even so, OCaml is probably not a great choice for CPU-intensive programs. Like Python, it has a global lock, so you can’t have multiple threads all using the CPU at once. But if you’re writing small utilities that need to run quickly, it’s perfect.

Update: There is a multicore OCaml branch under development which removes the global lock.

No dependency cycles

Perhaps I’m making a virtue of a flaw here, but I like the fact that OCaml doesn’t allow cyclic dependencies between source files.
I think this leads to cleaner code (back when I was writing Java, I wrote a script to extract all module dependencies and graph them so I could find and eliminate cycles).

What this means is that in any OCaml code-base, no matter how complex, there’s always at least one module that doesn’t depend on any of the others and which you can therefore read first.
Then there’s a second module that only depends on the first one, etc.
For example, here are the modules that make up 0install’s GTK plugin (note the lack of cycles):



Cycles can be a problem when converting existing code to OCaml, though. For example, the Python had a helpers.py module containing various high-level helper functions (e.g. get_selections_gui to run the GUI and return the user’s selections, and ensure_cached to make sure some selections are cached and download them if not). That doesn’t work in OCaml, because the helpers module depends on the GUI, but the GUI also depends on the helpers (the GUI sometimes needs to ensure things are cached). The result is that I had to move each helper function to the module it uses, but I don’t mind because the result is a clearer API.

Another example is the Config object. When I started the Python code back in 2005, I was very excited about using the idea of dependency injection for connecting together software modules (this is the basis of how 0install runs programs). Yet, for some reason I can’t explain, it didn’t occur to me to use a dependency injection style within the code. Instead, I made a load of singleton objects. Later, in an attempt to make things more testable, I moved all the singletons to a Config object and passed that around everywhere. I wasn’t proud of this design even at the time, but it was the simplest way forward. It looked like this:

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
class Config(object):
	@property
	def fetcher(self):
		if not self._fetcher:
			from zeroinstall.injector import fetch
			self._fetcher = fetch.Fetcher(self)
		return self._fetcher
	@property
	def trust_mgr(self):
		if not self._trust_mgr:
			from zeroinstall.injector import trust
			self._trust_mgr = trust.TrustMgr(self)
		return self._trust_mgr
	...


OCaml really didn’t like this design! config.py depends on all the other modules because it calls their constructors, while they all depend on it to get their dependencies.

Note that this design isn’t very safe: Fetcher’s constructor could ask for config.trust_mgr, and TrustMgr’s constructor could ask for config.fetcher. In Python, we have to remember not to do that, but in OCaml we’d like the type checker to prove it can’t happen.

In most places, I fixed this by passing to each constructor just the objects it actually needs, which is cleaner.

Another approach, which I used when lots of objects were needed, is that instead of requiring a config object, a class can take simply “an object with at least fetcher and trust_mgr methods”.
Then we know statically that it will only call those methods, even though we actually give it the full config object.

The result of all this is that you can look at e.g. the fetch.mli interface file and see exactly which other modules it depends on, none of which will depend on it.

GUI code

Converting the GTK GUI to OCaml (using the LablGtk bindings), I replaced 5166 lines of Python (plus 1736 lines of GtkBuilder XML) with 4017 lines of OCaml (and no XML). I’m not sure why, but writing GTK code in OCaml just seems to be much easier than with Python.

I used GtkBuilder in the Python code in the hope that it would make it easier to modify the layouts, and to improve reliability (since the XML should always be valid, whereas Python code might not be). However, it actually made things harder because Glade (the editor) is constantly trying to force you to upgrade to the latest (incompatible) XML syntax, and I ended up having to run an old OS in a VM any time I wanted to edit things.

In the OCaml, the static type checking gives us similar confidence that the layout code won’t crash. Also, with GtkBuilder you name each widget in the XML and then search for these names in the code. If they don’t match, it will fail at runtime. Having everything in OCaml meant that couldn’t happen. [ Note: I later discovered that LablGtk doesn’t support GtkBuilder anyway. ]

Here’s an example of some OCaml GTK code and the corresponding Python code. This shows how to build and display a menu (simplified to have just one item):

OCaml 
1
2
3
4
let menu = GMenu.menu () in
let explain = GMenu.menu_item ~packing:menu#add ~label:"Explain this decision" () in
explain#connect#activate ~callback:(fun () -> show_explanation impl) |> ignore;
menu#popup ~button:(B.button bev) ~time:(B.time bev);


Python 
1
2
3
4
5
6
7
8
9
10
11
global menu		# Fix GC problem with PyGObject
menu = gtk.Menu()
item = gtk.MenuItem()
item.set_label('Explain this decision')
item.connect('activate', lambda item: self.show_explanation(impl))
item.show()
menu.append(item)
if sys.version_info[0] < 3:
	menu.popup(None, None, None, bev.button, bev.time)
else:
	menu.popup(None, None, None, None, bev.button, bev.time)


Some points to note:

  LablGtk allows you to specify many properties at once in the constructor call, whereas in PyGTK we need separate calls for each.
  The Python API broke between Python 2 and Python 3, so we have to make sure to use the right one. It’s not sufficient to test the Python code using only one version of Python!
  The Python bindings have always suffered from garbage collection bugs. If we don’t store menu in a global variable, it may garbage collect the menu while the user is still choosing - this makes the menu disappear suddenly from the screen!
  Actually, I see that Python’s MenuItem takes a label argument, so maybe I could save a line. Or maybe that doesn’t work on some older version. It’s not worth the risk of changing it.


Update: I used the original layout for the OCaml above as I was comparing line counts, but it’s a bit wide for this narrow blog and some people are finding it hard to read. Here’s an expanded version which uses less special syntax:

1
2
3
4
5
6
7
8
let menu = GMenu.menu () in
let explain = GMenu.menu_item
  ~packing:menu#add
  ~label:"Explain this decision"
  () in
let callback () = show_explanation impl in
let _signal_id = explain#connect#activate ~callback in
menu#popup ~button:(B.button bev) ~time:(B.time bev)


Here, ~ indicates a named argument and # is a method call. So menu_item ~packing:menu#add ... is like menu_item(packing = menu.add, ...) in Python.

However, I did still have a few problems with the OCaml GTK bindings:

  There’s no support for custom cell renderers. I used three of these in the Python version and had to find alternative UIs for each.
  Various minor functions aren’t included for some reason. The ones I wanted but couldn’t find were Dialog.add_action_widget, Style.paint_arrow, MessageDialog.BUTTONS_NONE, Dialog.set_keep_above, icon_size_lookup and Selection_data.get_uris.
  I had to work around a bug in the IconView support (reported but with no response and fixed immediately, although the bug report wasn’t updated).
  You usually don’t need the result of creating a label or attaching a signal so you need to use ignore, which can cause silent failures if you forgot an argument (it will ignore the partial function rather than the widget or signal result). Probably I should make ignore_signal and ignore_widget utility functions.


Update: I did add ignore_widget, but for signals I found a better solution: I created a new ==> operator to connect a signal and ignore the resulting signal ID. It’s used like this:

1
explain#connect#activate ==> (fun () -> show_explanation impl)


API stability

This is a community thing rather than a language issue, but OCaml and OCaml libraries seem to be very good at maintaining backwards compatibility at the source level. 0install supports the old OCaml 3.12 and libraries in Ubuntu 12.04 up to the latest OCaml 4.01 release without any problems. The only use of conditional compilation for compatibility is that we don’t define the |> operator on 4.01 because it’s already a built-in (this avoids a warning).

On the other hand, binary compatibility is very poor. You can replace the implementation of a module with a newer version as long as the public interface doesn’t change (good), but any change at all to the interface requires everything that depends on it to be recompiled, and then everything that depends on them, and so on.

For example, if the List module adds a new function then the signature of the List module changes. Now any program using the new version of the List module is incompatible with every library binary compiled against the old version. Even if nothing is even using the new function! This means that distributing OCaml libraries in binary form is effectively impossible.

Summary

OCaml’s main strengths are correctness and speed. Its type checking is very good at catching errors, and its “polymorphic variants” are a particularly useful feature, which I haven’t seen in other languages. Separate module interface files, abstract types, cycle-free dependencies, and data structures that are immutable by default help to make clean APIs.

Surprisingly, writing GTK GUI code in OCaml was easier than in Python. The resulting code was significantly shorter and, I suspect, will prove far more reliable. OCaml’s type checking is particularly welcome here, as GUI code is often difficult to unit-test.

The OCaml community is very good at maintaining API stability, allowing the same code to compile on old and new systems and (hopefully) minimising time spent updating it later.
Hide
        
      
                    by Thomas Leonard at Feb 13, 2014 
      
      
    
  


       
                  MirageOS 1.1.0: the eat-your-own-dogfood release
      (Mirage OS)
    
    
                                      We've just released MirageOS 1.1.0 into OPAM.  Once the
live site updates, you should be able to run opam update -u and get the latest
version.  This release is the "eat our own
dogfood" release; as I
mentioned earlier in January, a number of the MirageOS developers have decided to
shift our own personal homepages onto MirageOS.  There's nothing better than
using our own tools to find all the little annoyances and shortcomings, and so
MirageOS 1.1.0 contains some significant usability and structural improvements
for building unikernels.
Functional combinators to build device drivers

MirageOS separates the
application logic from the concrete backend in use by writing the application
as an OCaml functor
that is parameterized over module types that represent the device driver
signature.  All of the module types used in MirageOS can be browsed in one
source file.
In MirageOS 1.1.0, Thomas Gazagnaire implemented a
a combinator library
that makes it easy to separate the definition of…Read more...      We've just released MirageOS 1.1.0 into OPAM.  Once the
live site updates, you should be able to run opam update -u and get the latest
version.  This release is the "eat our own
dogfood" release; as I
mentioned earlier in January, a number of the MirageOS developers have decided to
shift our own personal homepages onto MirageOS.  There's nothing better than
using our own tools to find all the little annoyances and shortcomings, and so
MirageOS 1.1.0 contains some significant usability and structural improvements
for building unikernels.
Functional combinators to build device drivers

MirageOS separates the
application logic from the concrete backend in use by writing the application
as an OCaml functor
that is parameterized over module types that represent the device driver
signature.  All of the module types used in MirageOS can be browsed in one
source file.
In MirageOS 1.1.0, Thomas Gazagnaire implemented a
a combinator library
that makes it easy to separate the definition of application logic from the details
of the device drivers that actually execute the code (be it a Unix binary or a
dedicated Xen kernel).  It lets us write code of this form
(taken from mirage-skeleton/block):
let () =
  let main = foreign "Unikernel.Block_test" (console @-> block @-> job) in
  let img = block_of_file "disk.img" in
  register "block_test" [main $ default_console $ img]

In this configuration fragment, our unikernel is defined as a functor over a
console and a block device by using console @-> block @-> job.  We then
define a concrete version of this job by applying the functor (using the $
combinator) to a default console and a file-backed disk image.
The combinator approach lets us express complex assemblies of device driver
graphs by writing normal OCaml code, and the mirage command line tool
parses this at build-time and generates a main.ml file that has all the
functors applied to the right device drivers. Any mismatches in module signatures
will result in a build error, thus helping to spot nonsensical combinations
(such as using a Unix network socket in a Xen unikernel).
This new feature is walked through in the tutorial, which
now walks you through several skeleton examples to explain all the different
deployment scenarios.  It's also followed by the website tutorial
that explains how this website works, and how our Travis autodeployment
throws the result onto the public Internet.
Who will win the race to get our website up and running first?  Sadly for Anil,
Mort is currently in the
lead with an all-singing, all-dancing shiny
new website.  Will he finish in the lead though? Stay tuned!
Less magic in the build

Something that's more behind-the-scenes, but important for easier development,
is a simplication in how we build libraries.  In MirageOS 1.0, we had several
packages that couldn't be simultaneously installed, as they had to be compiled 
in just the right order to ensure dependencies.
With MirageOS 1.1.0, this is all a thing of the past.  All the libraries can
be installed fully in parallel, including the network stack.  The 1.1.0
TCP/IP stack is now built in the
style of the venerable FoxNet network
stack, and is parameterized across its network dependencies.  This means
that once can quickly assemble a custom network stack from modular components,
such as this little fragment below from mirage-skeleton/ethifv4/:
module Main (C: CONSOLE) (N: NETWORK) = struct

  module E = Ethif.Make(N)
  module I = Ipv4.Make(E)
  module U = Udpv4.Make(I)
  module T = Tcpv4.Flow.Make(I)(OS.Time)(Clock)(Random)
  module D = Dhcp_clientv4.Make(C)(OS.Time)(Random)(E)(I)(U)
  

This functor stack starts with a NETWORK (i.e. Ethernet) device, and then applies
functors until it ends up with a UDPv4, TCPv4 and DHCPv4 client.  See the full
file
to see how the rest of the logic works, but this serves to illustrate how
MirageOS makes it possible to build custom network stacks out of modular
components.  The functors also make it easier to embed the network stack in
non-MirageOS applications, and the tcpip OPAM package installs pre-applied Unix
versions for your toplevel convenience.
To show just how powerful the functor approach is, the same stack can also
be mapped onto a version that uses kernel sockets simply by abstracting the
lower-level components into an equivalent that uses the Unix kernel to provide
the same functionality.  We explain how to swap between these variants in
the tutorials.
Lots of library releases

While doing the 1.1.0 release in January, we've also released quite a few libraries
into OPAM.  Here are some of the highlights.
Low-level libraries:
mstruct is a streaming layer for handling lists of memory buffers with a simpler read/write interface.nbd is an implementation of the Network Block Device protocol for block drivers.

Networking and web libraries:
ipaddr now has IPv6 parsing support thanks to Hugo Heuzard and David Sheets.  This is probably the hardest bit of adding IPv6 support to our network stack!cowabloga is slowly emerging as a library to handle the details of rendering Zurb Foundation websites.  It's still in active development, but being used for a few of our personal websites as well as this website.cohttp has had several releases thanks to external contributions, particular from Rudy Grinberg who added s-expression support and several other improvements.uri features performance improvements and the elimination of Scanf (considered rather slow by OCaml standards).cow continues its impossible push to make coding HTML and CSS a pleasant experience, with better support for Markdown now.The github bindings are now also in use as part of an experiment to make upstream OCaml development easier for newcomers, thanks to Gabriel Scherer.

Dave Scott led the splitting up of several low-level Xen libraries as part of the build simplication.  These now compile on both Xen (using the direct hypercall interface) and Unix (using the dom0 /dev devices) where possible.
xen-evtchn for the event notification mechanism. There are a couple of wiki posts that explain how event channels and suspend/resume work in MirageOS/Xen guests.xen-gnt for the grant table mechanism that controls inter-process memory.The io-page library no longer needs Unix and Xen variants, as the interface has been standardized to work in both.

All of Dave's hacking on Xen device drivers is showcased in this xen-disk wiki post that 
explains how you can synthesize your own virtual disk backends using MirageOS.  Xen uses a split device model,
and now MirageOS lets us build backend device drivers that service VMs as well as the frontends!
Last, but not least, Thomas Gazagnaire has been building a brand new storage system for MirageOS guests that uses git-style branches under the hood to help coordinate clusters of unikernels.  We'll talk about how this works in a future update, but there are some cool libraries and prototypes available on OPAM for the curious.
lazy-trie is a lazy version of the Trie data structure, useful for exposing Git graphs.git is a now-fairly complete implementation of the Git protocol in pure OCaml, which can interoperate with normal Git servers via the ogit command-line tool that it installs.irmin is the main library that abstracts Git DAGs into an OCaml programming API.  The homepage has instructions on how to play with the command-line frontend to experiment with the database.git2fat converts a Git checkout into a FAT block image, useful when bundling up unikernels.

We'd also like to thank several conference organizers for giving us the opportunity to demonstrate MirageOS.  The talk video from QCon SF is now live, and we also had a great time at FOSDEM recently (summarized by Amir here). 
So lots of activities, and no doubt little bugs lurking in places (particularly around installation).  As always, please do let us know of any problem by reporting bugs, or feel free to contact us via our e-mail lists or IRC.  Next stop: our unikernel homepages!

   Hide
        
      
                    by Anil Madhavapeddy at Feb 11, 2014 
      
      
    
  


       
                  Fourth OCaml compiler hacking session
      (Compiler Hacking)
    
    
                                It's time for the fourth Cambridge OCaml compiler-hacking session!  We'll be meeting in the Computer Lab again next Tuesday evening.

If you're planning to come along, it'd be helpful if you could indicate interest via Doodle and sign up to the mailing list to receive updates:

Where: Room FW11, Computer Laboratory, Madingley Road

When: 6pm, Tuesday 18th February

Who: anyone interested in improving OCaml. Knowledge of OCaml programming will obviously be helpful, but prior experience of working on OCaml internals isn't necessary.

What: fixing bugs, implementing new features, learning about OCaml internals

Wiki: https://github.com/ocamllabs/compiler-hacking/wiki

We're defining "compiler" pretty broadly, to include anything that's part of the standard distribution, which means at least the standard library, runtime, tools (ocamldep, ocamllex, ocamlyacc, etc.), ocamlbuild, the documentation, and the compiler itself. We'll have suggestions for mini-projects for various levels of experi…Read more...It's time for the fourth Cambridge OCaml compiler-hacking session!  We'll be meeting in the Computer Lab again next Tuesday evening.

If you're planning to come along, it'd be helpful if you could indicate interest via Doodle and sign up to the mailing list to receive updates:

Where: Room FW11, Computer Laboratory, Madingley Road

When: 6pm, Tuesday 18th February

Who: anyone interested in improving OCaml. Knowledge of OCaml programming will obviously be helpful, but prior experience of working on OCaml internals isn't necessary.

What: fixing bugs, implementing new features, learning about OCaml internals

Wiki: https://github.com/ocamllabs/compiler-hacking/wiki

We're defining "compiler" pretty broadly, to include anything that's part of the standard distribution, which means at least the standard library, runtime, tools (ocamldep, ocamllex, ocamlyacc, etc.), ocamlbuild, the documentation, and the compiler itself. We'll have suggestions for mini-projects for various levels of experience (see also some things we've worked on in previous sessions), but feel free to come along and work on whatever you fancy.

We'll also be ordering pizza, so if you want to be counted for food you should aim to arrive by 6.30pm.
Hide
        
      
                    by Compiler Hacking at Feb 11, 2014 
      
      
    
  


       
                  How to handle success
      (Compiler Hacking)
    
    
                                (Update (2014-05-28): Added notes on delimcc and Catch me if you can to the Discoveries section.)

(Update (2014-05-05): The match/exception variant of this proposal has been merged into OCaml trunk, ready for release in 4.02.)

(Update: there's a Mantis issue open to discuss this proposal.)

OCaml's try construct is good at dealing with exceptions, but not so good at handling the case where no exception is raised.  This post describes a simple extension to try that adds support for handling the "success" case.

Here's an example of code that benefits from the extension.  On a recent caml-list thread, Simon Cruanes posted the following function for iterating over a stream:

let rec iter_stream f s =
  match (try Some (MyStream.get s) with End_of_stream -> None) with
  | None -> ()
  | Some (x, s') ->
      f x;
      iter_stream f s'


For each element of a stream, iter_stream wraps the element with Some, then unwraps it again and passes it to f.  At first glance, wrapping and immedia…Read more...(Update (2014-05-28): Added notes on delimcc and Catch me if you can to the Discoveries section.)

(Update (2014-05-05): The match/exception variant of this proposal has been merged into OCaml trunk, ready for release in 4.02.)

(Update: there's a Mantis issue open to discuss this proposal.)

OCaml's try construct is good at dealing with exceptions, but not so good at handling the case where no exception is raised.  This post describes a simple extension to try that adds support for handling the "success" case.

Here's an example of code that benefits from the extension.  On a recent caml-list thread, Simon Cruanes posted the following function for iterating over a stream:

let rec iter_stream f s =
  match (try Some (MyStream.get s) with End_of_stream -> None) with
  | None -> ()
  | Some (x, s') ->
      f x;
      iter_stream f s'


For each element of a stream, iter_stream wraps the element with Some, then unwraps it again and passes it to f.  At first glance, wrapping and immediately unwrapping in this way seems like needless obfuscation.  However, moving the last two lines out of the body of the try in this way serves two essential purposes: it turns the recursive call to iter_stream into a tail call, and it allows exceptions raised by f to propagate.  More generally, this use of options makes it easy to specify the success continuation of a try expression, i.e. the piece of code that receives the value of the body when no exception is raised.

As Simon notes, the match (try Some ...) idiom is widely used in OCaml code.  Examples can be found in the source of lwt, batteries, liquidsoap, sexplib, opa, uri, coq, unison, and many other packages.  

In response to Simon's message, Oleg pointed out a solution: the 2001 paper Exceptional Syntax  (Benton and Kennedy) extends try with a let-like binding construct that supports the success continuation idiom directly without the need for the option value.



This post describes a patch to OCaml that implements a variant of Benton and Kennedy's design called handler case.  Like Exceptional Syntax, handler case extends try with explicit success continuation handling.  However, unlike Exceptional syntax, handler case uses match binding for both the success continuation and the exception-handling clauses.  Here's the extended try syntax:
try expr
with pattern_1 -> expr_1
   | ...
   | pattern_n -> expr_n
   | val pattern_1' -> expr_1'
   | ...
   | val pattern_n' -> expr_n'

As in current OCaml, the clauses pattern_1 -> expr_1 ... pattern_n -> expr_n handle exceptions raised during the evaluation of expr.  The clauses  val pattern_1' -> expr_1' ... val pattern_n' -> expr_n' handle the case where no exception is raised; in this case the value of expr is matched against pattern_1' ... pattern_n' to select the expression to evaluate to produce the result value.  (The actual syntax is implemented slightly more permissively: it allows value-matching and exception-matching clauses to be freely interleaved.)

Using handler case we can rewrite iter_stream to remove the extraneous option value:
let rec iter_stream f s =
  try MyStream.get s
  with End_of_stream -> ()
     | val (x, s') -> f x;
                      iter_stream f s'

We don't need to look far to find other code that benefits from the new construct.  Here's a function from the Big_int module in the standard library: 
let int_of_big_int bi =
  try let n = int_of_nat bi.abs_value in
    if bi.sign = -1 then - n else n
  with Failure _ ->
    if eq_big_int bi monster_big_int then monster_int
    else failwith "int_of_big_int"

The core of the function --- the call to int_of_nat --- is rather buried in the complex control flow.  There are two if-then-else constructs, a let binding, and a try expression with a complex body.  Using handler case we can disentangle the code to make the four possible outcomes from the call to int_of_nat explicit:
let int_of_big_int bi =
  try int_of_nat bi.abs_value with
  | val n when bi.sign = -1 ->
     -n
  | val n ->
     n
  | Failure _ when eq_big_int bi monster_big_int ->
     monster_int
  | Failure _ ->
     failwith "int_of_big_int"

Here's a simpler example from the String module, which also involves code that cannot raise an exception in the body of a try block:
try ignore (index_rec s l i c); true with Not_found -> false

Using handler case we can separate the code that may raise an exception (the call to index_rec) from the expression that produces the result:
try index_rec s l i c with val _ -> true | Not_found -> false

Trying it out

Using opam you can install an OCaml compiler extended with handler case as follows:
$ opam remote add ocamllabs git@github.com:ocamllabs/opam-repo-dev.git
ocamllabs Fetching git@github.com:ocamllabs/opam-repo-dev.git
[...]
$ opam switch 4.02.0dev+handler-syntax
# To complete the configuration of OPAM, you need to run:
eval `opam config env`
$ eval `opam config env`

jsofocaml

You can also try out the handler case construct in your browser, using the following modified version of OCamlPro's Try OCaml application:

The discoveries of success continuations

As Philip Wadler notes, constructs for handling success continuations have been independently discovered multiple times.  In fact, the history goes back even further than described in Wadler's blog; constructs like handler case date back over thirty years and have been introduced, apparently independently, into at least four languages.  Curiously, all the languages use let-binding for success continuations and match binding for failure continuations.

Lisp

In Common Lisp the construct analogous to try is handler-case (from which the construct discussed here borrows its name).  A handler-case expression has a body and a sequence of clauses which specify how various conditions (exceptions) should be handled.  The special condition specification :no-error specifies the code to run when no condition is signalled.  The iter_stream function might be written as follows in Common Lisp:
(defun iter-stream (f s)
   (handler-case (get-stream s)
      (end-of-stream (_) nil)
      (:no-error (x |s'|)
         (funcall f x)
         (iter-stream f |s'|))))

The Common Lisp specification was completed in 1994 but the handler-case construct and its :no-error clause were present in some of Common Lisp's progenitors.  The construct was apparently introduced to Symbolics Lisp some time around 1982: it appears in the 5th edition of the Lisp Machine manual (January 1983) but not the 4th edition from 18 months earlier (July 1981).

Python

Python has supported success continuations in exception handlers since May 1994, when the else clause was added to try blocks.  The Changelog in old versions of the Grammar/Grammar file has an entry

# 3-May-94:
#Added else clause to try-except


introduced in a commit from August 1994:

changeset:   1744:6c0e11b94009
branch:      legacy-trunk
user:        Guido van Rossum <guido@python.org>
date:        Mon Aug 01 11:00:20 1994 +0000
summary:     Bring alpha100 revision back to mainline


Unlike the :no-error clause in Lisp's handler-case, Python's else clause doesn't bind variables.  Since Python variables have function scope, not block scope, bindings in the body of the try block are visible throughout the function.  In Python we might write iter_stream as follows:
def iter_stream(f, s):
   try:
      (x, s_) = MyStream.get(s)
   except End_of_stream:
      pass
   else:
      f(x)
      iter_stream(f, s_)

The provenance of the else clause is unclear, but it doesn't seem to derive from Lisp's handler-case.  The design of Python's exception handling constructs comes from Modula-3, but the exception handling construct described in the Modula-3 report does not include a way of specifying the success continuation.  The syntax of the Modula-3 TRY/EXCEPT statement (found on p21 of the report) does include an ELSE clause:
   TRY    
     Body
   EXCEPT
     id1 (v1) => Handler1
   | ...
   | idn (vn) => Handlern
   ELSE Handler0
   END

However, whereas Python's else handles the case where no exception is raised, Modula-3's ELSE handles the case where an exception not named in one of the EXCEPT clauses is raised: it is equivalent to Python's catch-all except:.

Python also adds success handlers to other constructs.  Both the for and the while statements have an optional else clause which is executed unless the loop terminates prematurely with an exception or break.



Exceptional Syntax

The 2001 paper Exceptional Syntax (Benton and Kennedy) proposed the following construct for handling exceptions in Standard ML:
let val pattern_1 <= expr_1
    ...
    val pattern_n <= expr_n
 in
    expr
unless
    pattern_1' => expr_1'
  | ...
  | pattern_n' => expr_n'
end

Evaluation of the let binding proceeds as normal, except that if any of expr_1 to expr_n raises an exception, control is transferred to the right hand side of the first of the clauses after unless whose left hand side matches the exception.  The construct is largely similar to our proposed variation, except that the bindings used in the success continuation are based on let, so scrutinising the values requires a separate case (i.e. match) expression.

Using the Exceptional Syntax construct we might write iter_stream as follows:
fun iter_stream f s =
 let val (x, s') <= MyStream.get s in
     f x;
     iter_stream f s'
 unless End_of_stream => ()
 end

Exceptional Syntax has been implemented in the SML-to-Java compiler MLj.

Erlang

The 2004 paper Erlang's Exception Handling Revisited (Richard Carlsson, Björn Gustavsson and Patrik Nyblom) proposed an exception-handling construct for Erlang along the same lines as exceptional syntax, although apparently developed independently.  In the proposed extension to Erlang we might write iter_stream as follows:
iter_stream(F, S) ->
   try Mystream:get(S) of
      {X, S_} ->
        _ = F(X),
        iter_stream(F, S_)
   with
    End_of_stream -> {}

Eff

Plotkin and Pretnar's work on handlers for algebraic effects generalises Exceptional Syntax to support effects other than exceptions.  The programming language eff implements a design based on this work, and supports Exceptional Syntax, again with let binding for the success continuation.  (Although the success continuation is incorporated into the exception matching construct, only a single success continuation pattern is allowed.)  In eff we might write iter_stream as follows:
let rec iter_stream f s =
  handle my_stream_get s
  with exn#end_of_stream _ _ -> ()
     | val (x, s') -> f x;
                      iter_stream f s'

The second argument in the end_of_stream clauses binds the continuation of the effect, allowing handling strategies other than the usual stack unwinding.  Since we ignore the continuation argument the behaviour is the same as for a regular exception handler.

The eff implementation uses the term "handler case" for the clauses of the handle construct.

OCaml

Several OCaml programmers have proposed or implemented constructs related to handler case.


Oleg's delimcc library for delimited continuations provides the operations needed to support the success continuation style.  The programmer can use push_prompt to establish a context, then call shift or shift0 to return control to that context later, much as try establishes a context to which raise can transfer control.  If shift is not called then control returns normally from the continuation argument to push_prompt.  Using delimcc we might implement iter_stream as follows:
let rec iter_stream f s =
    let p = new_prompt () in
    match push_prompt p (fun () -> `Val (my_stream_get s p)) with
  | `Val (x, s') -> f x; iter_stream f s'
  | `End_of_stream -> ()

Martin Jambon has implemented a construct equivalent to Exceptional Syntax for OCaml as part of the micmatch extension.  His implementation allows us to write iter_stream in much the same way as Benton and Kennedy's proposal:
let rec iter_stream f s =
  let try (x, s') = my_stream_get s
   in f x;
      iter_stream f s'
  with End_of_stream -> ()

The details of the implementation are discussed in Jake Donham's articles on Camlp4.  The micmatch implementation has a novel feature: the let binding associated with the success continuation may be made recursive.

Alain Frisch has proposed and implemented a more powerful extension to OCaml, Static Exceptions, which allow transfer of control to lexically-visible handlers (along the lines of Common Lisp's block and return-from).  Static exceptions are based on an equivalent feature in OCaml's intermediate language.

There is a straightforward translation from OCaml extended with handler case into OCaml extended with static exceptions by wrapping the body of each try expression in raise (`Val (...)), and changing the val keyword in the binding section to `Val.  For example, iter_stream can be written using static exceptions as follows:
let rec iter_stream f s =
  try raise (`Val (MyStream.get s))
  with End_of_stream -> ()
     | `Val (x, s') -> f x;
                       iter_stream f s'

Of course, static exceptions allow many other programs to be expressed that are not readily expressible using handler case.


In their 2008 paper Catch me if you can: Towards type-safe, hierarchical, lightweight, polymorphic and efficient error management in OCaml David Teller Arnaud Spiwack and Till Varoquaux added an attempt keyword to OCaml that extends match-style pattern matching with both a single optional value case and an optional finally clause.

Finally, I discovered while writing this article that Christophe Raffalli proposed the handler case design fifteen years ago in a message to caml-list!  Christophe's proposal wasn't picked up back then, but perhaps the time has now come to give OCaml programmers a way to handle success.



Postscript: a symmetric extension

The try construct in current OCaml supports matching against raised exceptions but not against the value produced when no exception is raised.  Contrariwise, the match construct supports matching against the value produced when no exception is raised, but does not support matching against raised exceptions.  As implemented, the patch addresses this asymmetry, extending match with clauses that specify the "failure continuation":
match expr
with pattern_1 -> expr_1
   | ...
   | pattern_n -> expr_n
   | exception pattern_1' -> expr_1'
   | ...
   | exception pattern_n' -> expr_n'

With this additional extension the choice between match and try becomes purely stylistic.  We might optimise for succinctness, and use try in the case where exceptions are expected (for example, where they're used for control flow), reserving match for the case where exceptions are truly exceptional.

For the sake of completeness, here's iter_stream written with the extended match construct:
let rec iter_stream f s =
  match MyStream.get s with
     (x, s') -> f x;
                iter_stream f s'
   | exception End_of_stream -> ()

Since both val and exception are existing keywords, the extensions to both try and match are fully backwards compatible. 
Hide
        
      
                    by Compiler Hacking at Feb 04, 2014 
      
      
    
  


       
                  OCaml: the bugs so far
      (Thomas Leonard)
    
    
                                OCaml’s static typing allows it to detect many problems at compile-time. Still, some bugs slip though. In this post, I go over each discovered bug that made it into a Git commit and try to work out why it happened and whether it could have been prevented.



Note: As this post is entirely about bugs, it may appear rather negative. So let me say first that, overall, I’ve been very impressed with the reliability of the OCaml code: I’d have expected to find more bugs than this in 27,806 lines of new code!

Table of Contents

  Methodology
  Core OCaml issues          Out-of-range integers
      Fails to start on Windows
      Spawning a daemon fails on Windows
      0install select ignores --refresh
      Not found errors
      Octal value
      Parsing a path as a URL
      Interrupted waitpid
      HTTP redirects with data cause corrupted downloads
      Setting wrong mtime
      Strict sequences
    
  
  Lwt-related bugs          Lwt process fails with empty string
      EPIP…Read more...OCaml’s static typing allows it to detect many problems at compile-time. Still, some bugs slip though. In this post, I go over each discovered bug that made it into a Git commit and try to work out why it happened and whether it could have been prevented.



Note: As this post is entirely about bugs, it may appear rather negative. So let me say first that, overall, I’ve been very impressed with the reliability of the OCaml code: I’d have expected to find more bugs than this in 27,806 lines of new code!

Table of Contents

  Methodology
  Core OCaml issues          Out-of-range integers
      Fails to start on Windows
      Spawning a daemon fails on Windows
      0install select ignores --refresh
      Not found errors
      Octal value
      Parsing a path as a URL
      Interrupted waitpid
      HTTP redirects with data cause corrupted downloads
      Setting wrong mtime
      Strict sequences
    
  
  Lwt-related bugs          Lwt process fails with empty string
      EPIPE on Windows
      Deleting temporary files
      Race shutting down test HTTP server
      Reactive event handler gets garbage collected
      Stuck reading output from subprocess
      Downloads never complete
      Event loop doesn’t work on OS X
    
  
  Curl-related bugs          Failing to reset Curl connections
      Error with cancelled downloads
    
  
  GTK bugs          Sorted treeview iter mix-up
      Crashes with GtkIconView
    
  
  Logic errors
  Python bugs
  Summary


( This post is part of a series in which I am
converting 0install from Python to OCaml, learning OCaml as I go. The code is at GitHub/0install. )

Methodology

I’ve gone back through the Git commit log and selected all the ones that say they fix a bug in the comment, starting from when I merged the initial OCaml code to master (on 2013-07-03). It’s possible that I sneakily fixed some bugs while making other changes, but this should cover most of them. Any bug that made it into a released version of 0install should certainly have its own commit because it would have to go on the release branch. I also included a few “near misses” (bugs I spotted before committing, but which I could plausibly have missed).

In a number of cases, I wrote and committed the new OCaml code first, and then ported the Python unit-tests in a later commit and discovered the bug that way (so some of these could never have made it into an actual release). Compile-time bugs have been ignored (e.g. code that didn’t compile on older versions of OCaml); I’m only interested in run-time errors here.

I’ve classified each bug as follows:

  Inexperience
  This bug was caused by my inexperience with OCaml. Making proper use of OCaml’s features would avoid this class of bug entirely.
  Third-party
  Caused by a bug in a library I was using (and hopefully now fixed). Could only have been discovered by testing.
  Poor API
  This bug was my fault, but could have been avoided if the library had a better API.
  Warning needed
  My fault, but the compiler could have detected the problem and issued a warning.
  Testing only
  I only know how to find such bugs through testing. Similar bugs could happen again.


Core OCaml issues

Note: I’m grouping the bugs by the library the code was interacting with, regardless of whether that library was at fault. This section is for bugs that occurred when just using OCaml itself and its standard library.

Out-of-range integers

Everything seemed to be working nicely on my Arch system, but the first run on a Debian VM gave this unhelpful error:

Failure "int_of_string"


I was trying to store a Unix timestamp in an int. Unlike Python, OCaml’s integers are limited precision, having 1 bit less than the machine word size. The 32-bit VM only had 31-bit integers and the time value was out of range for it.

This was entirely my fault. OCaml always uses floats to represent times, and they work fine. I was just converting to ints to be consistent with the Python code.

However, the error message is very poor. I replaced all calls to int_of_string with my own version, which at least displays the number it was trying to convert. This should make debugging any similar problems easier in future.

Type: Inexperience

Fails to start on Windows

Windows kept complaining that my program didn’t exist and to check that the path was correct, even when I was double-clicking on the executable in Explorer!
Turns out, Windows refuses to run binaries with certain character sequences in their names (“instal” being one such sequence). See Vista doesn’t start application called “install” w/o being elevated.

Solution: you have to embed an XML “manifest” in Windows binaries to avoid this behaviour. Would be nice if OCaml did that automatically for you.

Type: Third-party

Spawning a daemon fails on Windows

Windows doesn’t have fork, so the usual double-fork trick doesn’t work.
Solution: Use create_process on Windows.

Would be nice if OCaml grouped all the POSIX-only functions together and made you check which platform you were on. Then you’d know when you were using platform-specific functions. e.g.

1
2
3
match Sys.platform_specific_ops with
| POSIX ops -> ops#fork ...
| Windows ops -> ops#spawn ...


Type: Poor API

0install select ignores --refresh

I forget to handle the Refresh case for the “select” command.
Different commands need to handle different subsets of the options. I was using a plain variant (enum) type and throwing an exception if I got an option I wasn’t expecting:

1
2
3
4
Support.Argparse.iter_options extra_options (function
  | ShowXML -> select_opts.xml <- true
  | _ -> raise_safe "Unknown option"
)


(Note: Several people have asked why I used a default match case here. It’s needed because there are many options that don’t apply to the “select” command. The option parser makes sure that each sub-command’s handler function is only called with options it claims to support.)

Solution: I switched from plain variants to polymorphic variants and removed the default case. Now, the type-checker verifies at compile-time that each subcommand handles all its options:

1
2
3
4
Support.Argparse.iter_options extra_options (function
  | `ShowXML -> select_opts.xml <- true
  | `Refresh -> select_opts.refresh <- true
)


See Option Handling With OCaml Polymorphic Variants for a write-up of that.

Type: Inexperience

Not found errors

When printing diagnostics about a failed solve, we check each interface to see if it has a replacement that it conflicts with. e.g. the new “Java” interface replaces (and conflicts with) the old “Java 6” interface. But if the conflicting interface wasn’t used in the solve, we’d crash with:

Exception: Not_found


I use a lot of maps with strings as the keys. I therefore created a StringMap module in my common.ml file like this:

1
module StringMap = Map.Make(String)


StringMap.find raises Not_found if the key isn’t found, which is never what you want. These exceptions are awkward to deal with and it’s easy to forget to handle them.

A nice solution is to replace the definition with:

1
2
3
4
5
6
7
module StringMap = struct
  include Map.Make(String)
  let find key map =
    try Some (find key map)
    with Not_found -> None
end


This redefines the find method to return an option type. Now you can’t do a StringMap.find without the compiler forcing you to consider the case of the key not being there.

Would be nice if the OCaml standard library did this. Perhaps providing a Map.get function with the new behaviour and deprecating Map.find?

Type: Poor API

Octal value

I used 0700 instead of 0o700 to set a file mode. Would be nice if OCaml warned about decimals that start with 0, as Python 3 does.

Type: Warning needed

Parsing a path as a URL

This didn’t actually get committed, but it’s interesting anyway. Downloaded feeds are signed with GPG keys, which are trusted only for their particular domains. At one point, I used Trust.domain_from_url feed.url to get the domain. It was defined as:

1
let domain_from_url url = ...


However, feeds come in different types: there are remote feeds with URLs, and local feeds with local paths (there are also virtual “distribution” feeds representing the output from the distribution’s package manager).

I was trying to get the trust domain for all feeds, not just remote ones where it makes sense.

Once again, the solution was to use polymorphic variants. The three different types of feed get three different constructors. A method (such as domain_from_url) that only works on remote feeds is declared as:

1
let domain_from_url (`remote_feed url) = ...


Then, it’s impossible to call it without first ensuring you’ve got a remote feed URL.

This change also improves the type-safety of many other parts of the code (e.g. you can’t try to download a local feed now either), and uncovered another bug: you couldn’t use the GUI to set the stability rating for a distribution-provided implementation, because one of the functions used only worked for non-distribution feeds.

Type: Inexperience (x2)

Interrupted waitpid

The Unix.waitpid function can raise EINTR if the system call is interrupted, although the documentation doesn’t mention this. It would be nice if OCaml would automatically restart the call in this case (as Python’s subprocess module does).

Type: Poor API

HTTP redirects with data cause corrupted downloads

We download to a temporary file. If we get an HTTP redirect, we truncate the file and try the new address. However, ftruncate doesn’t reset the position in the file. So, if the redirecting site sent any data in its reply, you’d get that many zeroes at the start of the download. As with waitpid, OCaml’s behaviour is standard POSIX, but not mentioned in the OCaml documentation.

Solution: seek_out ch 0.

Also, I updated the test server used in the unit-tests to send back some data when doing a redirect.

Type: Testing only

Setting wrong mtime

Unix.utimes is supposed to set the mtime and atime of a file to the given values. However, the behaviour is a little odd:

  When 1 <= time < infinity, it sets it to the requested time.
  When 0 <= time < 1 however, it sets it to the current time instead.


That’s a problem for us, because we often use “0” as the time for files which don’t have a timestamp, and the time is part of the secure hashes we calculate.

Solution: I wrote a C function to allow setting the time to whatever value you like.

This bug didn’t make it into a commit because I hit it while writing a test script (I was trying to reset a timestamp file to time zero), and the unit-tests would have caught it if not, but it’s still a poor API. Not only does it fail to use a variant type to handle different cases, but it chooses a magic value that’s a valid input!

Or, rather than using a variant type for these two cases, it could just drop the magic current time feature completely - it’s easy enough to read the current time and pass it explicitly if you need it. That would make the code clearer too.

(note: the documentation does say “A time of 0.0 is interpreted as the current time”, but it’s easy to forget this if it wasn’t relevant the first time you read the docs)

Type: Poor API

Strict sequences

This didn’t make it into a commit, but it’s interesting anyway. Simplified version of the problem:

1
2
3
4
5
6
7
let trace fn =
  print_endline "START";
  fn ();
  print_endline "END"
let () =
  trace (fun () -> Printf.printf "Hello %s!\n")


This prints:

START
END


Why is there no warning? You might expect OCaml would infer the type of fn as unit -> unit and then complain that the function we pass has the wrong type (unit -> string -> unit).

In fact, although OCaml warns if you ignore the result of a function that doesn’t return unit, it’s not actually an error. So it actually infers the type of fn as (unit -> 'a), and it compiles fine.

Solution: always compile with -strict-sequence (or put true: strict_sequence in your _tags file)

Type: Inexperience

Lwt-related bugs

Lwt process fails with empty string

When spawning a process, the Lwt docs say you can pass an empty string as the binary to run and it will search for the first argument in $PATH for you. However, that behaviour was added only in Lwt 2.4 and using the older version in Debian it failed at runtime with a confusing error.

Probably I should have been reading the old version of the docs (which the web-site helpfully lets you do).

I’m classifying this as a poor API because it was caused by using "" as a magic value, rather than defining a new constructor function.

Type: Poor API

EPIPE on Windows

On Windows, we couldn’t read the output of GnuPG. This was due to a bug in Lwt, which they quickly fixed:

Lwt_io.read fails on Windows with EPIPE

Type: Third-party

Deleting temporary files

We download various files to a temporary directory. In some cases, they weren’t being deleted afterwards.

Solution: the downloader now takes a mandatory Lwt switch and deletes the file when the switch is turned off. Callers just have to wrap the download call in a try ... finally block, like this:

1
2
3
4
5
6
7
8
let switch = Lwt_switch.create () in
try_lwt
  match_lwt downloader#download ~switch key_url with
  | `network_failure msg -> raise_safe "%s" msg
  | `aborted_by_user -> raise Aborted
  | `tmpfile tmpfile -> U.read_file system tmpfile |> G.import_key system
finally
  Lwt_switch.turn_off switch


To make this completely foolproof, you’d need something like the linear types from Rust or ATS, but this is good enough for me.

Type: Inexperience

Race shutting down test HTTP server

Some of the unit tests run a simple HTTP server. When the test is over, they use Lwt.cancel to kill it. However, it appears that this call is unreliable: depending on exactly what the server is doing at the time it might just ignore it and continue.

Solution: we both cancel the task and set a boolean flag, which we test just before calling accept. If we’re in accept at the time of the cancel, the thread will abort correctly. If it’s anywhere else, it may continue handling the current request, but will quit as soon as it finishes and checks the flag.

Would perhaps be nice if Lwt remembered that an attempt was made to cancel the thread during a non-cancellable operation, and killed it at the next opportunity.

Type: Poor API

A related race occurred if we spawned a child process while handling an HTTP request, because the child would inherit the client socket and it would never get closed.

Solution: Use Lwt_unix.set_close_on_exec connection as soon as the connection is accepted.

Note that both these hacks should be race-free, because Lwt is cooperatively multi-threaded (e.g. we can’t spawn a subprocess between accepting a connection and marking it close_on_exec). I think.

Ideally, when spawning a child process you’d specify the file descriptors you wanted it to inherit explicitly (Go does this, but really it needs to be at the POSIX level).

Type: Testing only

(although these bugs only occurred in the unit-tests, I’m including them because they could just as easily appear in the main code)

Reactive event handler gets garbage collected

The OCaml D-BUS bindings use functional reactive programming to report property values.
The idea is that you get an object representing the continuously-varying value of the property, rather than a particular sample of it.
Then you can handle the signal as a whole (for example, you can get the “progress” signal from a PackageKit transaction and pass it to a GUI progress monitor widget, so that the widget always shows the current progress).
You can build up chains of signal processors. For example, you might transform a “bytes downloaded” signal into a “percentage complete” one.

The technique seems to come from Haskell. Being purely functional, it’s always safe to garbage collect a signal if no-one is holding a reference to it.

However, OCaml is not purely functional. You might easily want to evaluate a signal handler for its side-effects.
I created a handler to monitor the transaction status signal to see when it was finished, and attached the resulting signal to a Lwt_switch.
My plan was that the switch would keep it alive until it fired.
That didn’t work, because there was a subtle circular reference in the whole scheme, and OCaml would sometimes garbage-collect the handler and the switch. Then the process would ignore the finished event and appear to hang. I asked on StackOverflow and got some suggestions:

  How to stop OCaml garbage collecting my reactive event handler?


The solution seems to be to keep references to all active signals in a global variable. Rather messy.

Type: Testing only

Stuck reading output from subprocess

When using the Lwt_glib module to integrate with the GTK mainloop, the HUP response from poll is ignored. This means that it will call poll endlessly in a tight loop.
Patch

Type: Third-party

Downloads never complete

When using Lwt_glib, downloads may never complete. This is because OCaml, like Python, has a global lock and Lwt_glib fails to release it when calling poll. Therefore, no other thread (such as the download thread) can make progress while the main thread is waiting (e.g. for the download thread to finish).
Patch

Type: Third-party

Event loop doesn’t work on OS X

Lwt_glib passes -1000 to poll to mean “no timeout”. This works on Linux, but not on BSD-type systems.
Patch

Type: Third-party

Curl-related bugs

Failing to reset Curl connections

For efficiency, Curl encourages the reuse of connections. However, I forgot to reset some parameters (max file size and expected modification time). If the next call didn’t use them, it would reuse the old values and possibly fail.

Newer versions of ocurl have a reset function, which avoids these problems.

Type: Poor API

Error with cancelled downloads

Downloads were sometimes failing with this confusing error:

easy handled already used in multi handle


It happened when reusing connections (which Curl encourages, for efficiency).
There was no direct way to cancel a download, so I handled cancellation by closing the channel the download was writing to.
Then, next time some data arrived, my write callback would fail to write the new data and throw an exception, aborting the download.
It turned out that this was leaving the connection in an invalid state.

Solution: return 0 from the handler instead of throwing an exception.

Ideally, ocurl should catch exceptions from callbacks and allow the C code to clean up properly. Now fixed.

Type: Third-party

GTK bugs

Sorted treeview iter mix-up

(I caught this before committing it, but it’s a nasty bug that could easily be missed. It was present for a while in the original Python version.)

The cache explorer dialog allows you to delete implementations from the cache by selecting an item and pressing the Delete button.
It also allows you to sort the table by clicking on the column headings.
However, if you sort the table and then delete something, it deletes the wrong thing!

To make a sortable table (which is just a special case of a tree to GTK), you first create an underlying (unsorted) list model, then wrap it with a sorted model, then pass that to the display widget (GtkTreeView), like so:

1
2
3
let model = GTree.list_store cols
let sorted_model = GTree.model_sort model
let view = GTree.view ~model:sorted_model ()


To do things with the model, you pass it a GtkTreeIter, which says which item you want to act on, e.g.

1
model#remove iter


The trouble is, sorted and unsorted GtkTreeIters both have the same type, so you can easily pass an iterator of the sorted model as an argument to the unsorted model.
Then it will act on the wrong item.
If the view isn’t sorted then everything works fine, so you might not notice the problem while testing.

Solution: I created a new module for unsorted lists. The implementation (unsorted_list.ml) just proxies calls to the real code:

unsorted_list.ml 
1
2
3
4
5
6
7
8
9
10
11
12
13
type t = GTree.list_store
type iter = Gtk.tree_iter
let list_store cols = GTree.list_store cols
let clear (model:t) = model#clear ()
let model_sort model = GTree.model_sort model
let get_iter_first (model:t) = model#get_iter_first
let set (model:t) ~(row:iter) ~column value = model#set ~row ~column value
let get (model:t) ~(row:iter) ~column = model#get ~row ~column
let remove (model:t) (row:iter) = model#remove row
let convert_iter_to_child_iter (model:GTree.model_sort) (iter:Gtk.tree_iter) = model#convert_iter_to_child_iter iter
let append (model:t) = model#append ()
let iter_next (model:t) (row:iter) = model#iter_next row


However, the interface (unsorted_list.mli) makes the types t (the model) and iter (its GtkTreeIters) abstract, so that code outside of the module isn’t allowed to know their real types:

unsorted_list.mli 
1
2
3
4
5
6
7
8
9
10
11
12
type t
type iter
val list_store : GTree.column_list -> t
val clear : t -> unit
val model_sort : t -> GTree.model_sort
val get_iter_first : t -> iter option
val set : t -> row:iter -> column:'a GTree.column -> 'a -> unit
val get : t -> row:iter -> column:'a GTree.column -> 'a
val remove : t -> iter -> bool
val convert_iter_to_child_iter : GTree.model_sort -> Gtk.tree_iter -> iter
val append : t -> iter
val iter_next : t -> iter -> bool


Now it’s impossible to mix up sorted and unsorted types:

1
2
3
let model = Unsorted_list.list_store cols     (* Unsorted_list.t *)
let sorted_model = Unsorted_list.model_sort model (* GTree.model *)
let view = GTree.view ~model:sorted_model ()


It’s still possible to mix up iterators in some cases (e.g. between two different instances of a sorted model), but that’s a much less likely mistake to make.

Another way to solve the problem would be to bundle the owning model with each iterator, but that would be a big change to how the underlying GTK library works.
And ATS could solve this easily using its dependant types, by declaring the iterator type as iter(m) (“iterator of model at address m”), linking models to iterators in the type system.

Type: Poor API

Crashes with GtkIconView

As with GtkTreeView, you can make a sorted GtkIconView with a pair of models.
For some reason, clearing the underlying model didn’t clear the sorted version, and repopulating it corrupted memory:

Program received signal SIGSEGV, Segmentation fault.


Solution: since there is no UI to let the user change the sort column, I just removed the sorted model and sorted the underlying model myself in OCaml. I guess this is probably a GTK bug.

Type: Third-party

A second crashing bug with GtkIconView is caused by a bug in lablgtk.
The C wrapper get_path_at_pos returns a tree_path option (None if you clicked in a blank area), but the OCaml declaration says it returns a plain tree_path.

Solution: use Obj.magic to do an unchecked cast to the correct type:

1
2
let path_unsafe = view#get_path_at_pos x y in
let path : Gtk.tree_path option = Obj.magic path_unsafe in


(reported as segfault due to GtkIconView type mismatch)

Two interesting things about this bug:

  Even the low-level GTK bindings are presumably not generated automatically. If they were, this kind of mismatch surely couldn’t happen.
  An OCaml optional pointer doesn’t have the same representation as a non-optional pointer. If it did, the code wouldn’t crash. This suggests OCaml is being a bit inefficient about option types, which is disappointing.


Type: Third-party

Logic errors

The remaining bugs aren’t very interesting, but I’ve included them for completeness:

  Failed to handle ambiguous short options (e.g. -r)
  Missing tab completion for 0install add
  0install run ignores --gui option
  Infinite loop handling recursive dependencies
  Allow absolute local paths in local implementations
  Default command for --source should be compile, not run
  Allow machine:null in JSON response
  Typo: scripts start #! not !#
  Report an error if we need to confirm keys during a headless background update
  Wrong attribute name in overrides XML file
  Allow executing selections with no command but an absolute main
  Handle local-path on <group> elements
  Wrong attribute name when saving user overrides
  Typo: "https://" not "https://'"
  Don’t try to use GUI in --dry-run mode
  Support old version of selections XML format
  Support old selections without a from-feed attribute
  Don’t send an empty GetDetails request to PackageKit
  Cope with dpkg-query returning a non-zero exit status
  Detect Python even if python-gobject is missing
  Race when cancelling downloads


Type: Testing only (x21)

Python bugs

Just for interest, here are the Python bugs discovered over the same period (it doesn’t make sense to compare bug counts, because these are bugs in mature code, often several years old, not just-written new code).

I think these would be impossible or unlikely in OCaml (the problem would be detected at compile time):

  Error setting machine type for Cygwin packages (type error)
  When loading selections from a file, convert last-check-mtime attribute to an int (type error)
  UnicodeError extracting or generating a manifest for archives with non-ASCII file names (no encoding of file names in OCaml :-)
  Crash when specifying bold text (PyGTK API change)
  Broken clipboard handling in cache explorer (PyGTK API change)
  Broken filtering in cache explorer (PyGTK API change)
  Broken cache explorer menu when using a filter (model mix up; could avoid with abstract types, as above)
  Fails running an app when the master feed is no longer cached (would be forced to handle None case in OCaml)


These would likely still be bugs in OCaml:

  Fix mtime check in selections.get_unavailable_selections for native packages
  Always return False for native packages in needs_download if include_packages is False
  Always use the prefix “xml” for the XML namespace
  Don’t abort solving just because a local feed is missing
  Escape tooltips in cache explorer
  32-bit size limit in cache explorer


This was a third-party bug:

  Workaround for PyGTK garbage collection bug


Summary

Despite the newness of the code, the bug-rate has been surprisingly low so far. Of the (detected) bugs that did make it past the compiler, about a sixth were due to bugs in third-party libraries, another sixth could have been avoided with better third-party APIs and a sixth we due to my inexperience with OCaml. For the remaining half, more testing is still the only way I can see to find such bugs.



It’s a shame that OCaml seems to have no system for deprecating old APIs. This means that poor API choices made years ago are still causing trouble today. It would be good if OCaml could flag functions as being there for historical reasons only, and issue a compiler warning if you used them. I do, however, like the fact that they stay around - breaking existing code (as Python 3 did) is not the solution either!

Two of the bugs (“Deleting temporary files” and “Reactive event handler gets garbage collected”) could have been avoided if OCaml had linear types, but I have reasonable solutions to both. The XML / JSON handling bugs could have been avoided by using proper schemas, but such schemas didn’t exist (my fault).

Overall, I’m pretty happy with the bug rate so far. No doubt more bugs will be discovered as the new code makes its way into the distributions and gains more users, but I think this code will be easier to maintain than the Python code, and much less likely to break silently due to changes in the language or third-party libraries.
Hide
        
      
                    by Thomas Leonard at Jan 07, 2014 
      
      
    
  


       
          Presenting Decks
      (Mirage OS)
    
                                      A few months ago, partly as a stunt, mostly because we could, Anil and I put together a presentation for OSCON'13 about Mirage in Mirage. That is, as a self-hosting Mirage web application that served up slides using RevealJS. It was a bit of a hack, but it was cool (we thought!) and it worked. Several more presentations were written and given this way, at venues ranging from the XenSummit 2013 to ACM FOCI 2013 to the Cambridge Computer Lab's MSc in Advanced Computer Science.
With the release of Mirage 1.0, CoHTTP, Cowabloga and
the new Zurb Foundation based website, it was time to refresh them
and as a little seasonal gift, give them a shiny new index with some actual CSS
styling. So here they are, a set of presentations that have been given
by various members of the Mirage team over the last 6 months or so. They cover
a range of topics, from general introductions to the Xen roadmap to more
detailed technical background. And, of course, as Mirage is under constant
rapid developm…Read more...      A few months ago, partly as a stunt, mostly because we could, Anil and I put together a presentation for OSCON'13 about Mirage in Mirage. That is, as a self-hosting Mirage web application that served up slides using RevealJS. It was a bit of a hack, but it was cool (we thought!) and it worked. Several more presentations were written and given this way, at venues ranging from the XenSummit 2013 to ACM FOCI 2013 to the Cambridge Computer Lab's MSc in Advanced Computer Science.
With the release of Mirage 1.0, CoHTTP, Cowabloga and
the new Zurb Foundation based website, it was time to refresh them
and as a little seasonal gift, give them a shiny new index with some actual CSS
styling. So here they are, a set of presentations that have been given
by various members of the Mirage team over the last 6 months or so. They cover
a range of topics, from general introductions to the Xen roadmap to more
detailed technical background. And, of course, as Mirage is under constant
rapid development, some of the older content may already be outdated. But the
code for the site itself serves as another example of a simple --
somewhat simpler than the Mirage website in fact -- Mirage web
application.

   Hide
        
      
                    by Richard Mortier at Jan 03, 2014 
      
      
    
  


       
                  Dec 2013 news update
      (OCL Monthly News)
    
    
                                      This time last year in 2012, I had just
announced the
formation of a new group called OCaml Labs in the
Cambridge Computer Lab that would combine research
and community work towards the practical application of functional programming.
An incredible year has absolutely flown by, and I've put together this post to
summarise what's gone on, and point to our future directions for 2014.
The theme of our group was not to be pure research, but rather a hybrid group
that would take on some of the load of day-to-day OCaml maintenance from
INRIA, as well as help grow the wider OCaml community.
To this end, all of our projects have been highly collaborative, often
involving colleagues from OCamlPro,
INRIA, Jane Street,
Lexifi and Citrix.
This post covers progress in tooling, the compiler and
language, community efforts, research
projects and concludes with our priorities for
2014.
Tooling

At the start of 2013, OCaml was in the interesting position of being a mature
decades-old language wi…Read more...      This time last year in 2012, I had just
announced the
formation of a new group called OCaml Labs in the
Cambridge Computer Lab that would combine research
and community work towards the practical application of functional programming.
An incredible year has absolutely flown by, and I've put together this post to
summarise what's gone on, and point to our future directions for 2014.
The theme of our group was not to be pure research, but rather a hybrid group
that would take on some of the load of day-to-day OCaml maintenance from
INRIA, as well as help grow the wider OCaml community.
To this end, all of our projects have been highly collaborative, often
involving colleagues from OCamlPro,
INRIA, Jane Street,
Lexifi and Citrix.
This post covers progress in tooling, the compiler and
language, community efforts, research
projects and concludes with our priorities for
2014.
Tooling

At the start of 2013, OCaml was in the interesting position of being a mature
decades-old language with a small, loyal community of industrial users who built
mission critical applications using it.  We had the opportunity to sit down
with many of them at the OCaml Consortium
meeting and prioritise where we started work.  The answer came back clearly:
while the compiler itself is legendary for its stability, the tooling around it
(such as package management) was a pressing problem.
OPAM

Our solution to this tooling was centered around the
OPAM package manager that
OCamlPro released into beta just at the end of 2012, and
had its first stable release in March 2013.  OPAM differs from most system
package managers by emphasising a flexible distributed workflow that uses
version constraints to ensure incompatible libraries aren't mixed up (important
for the statically-typed OCaml that is very careful about dependencies).
Working closely with OCamlPro we developed a git-based
workflow to make it possible for users (both individual or industrial) to
easily build up their own package repositories and redistribute OCaml code, and
started curating the package
repository.
The results have been satisfying: we started with an initial set of around 100 packages in
OPAM (mostly imported by the 4 developers), and ended 2013 with 587 unique packages and 2000 individual versions, with contributions from 160 individuals.  We now have a curated
central package repository for anyone
to submit their OCaml code,
several third-party remotes are maintained (e.g. the Xen Project
and Ocsigen).  We also regularly
receive releases of the Core libraries
from Jane Street, and updates from sources as varied as Facebook,
Coherent PDF,
to the Frenetic SDN research.
A notable contribution from OCamlPro during this time was to
clarify the licensing on
the package repository to be the liberal
CC0, and also to pass ownership to
the OCaml organization on GitHub, where it's now
jointly maintained by OCaml Labs, OCamlPro and anyone else that wishes to
contribute.
A lens into global OCaml code

It's been quite interesting just watching all the varied code fly into the
repository, but stability quickly became a concern as the new packages piled
up.  OCaml compiles to native code on not just x86, but also PowerPC, Sparc and
ARM CPUs.
We kicked off various efforts into automated testing: firstly David Sheets
built the
OCamlot daemon
that would schedule builds across all the exotic hardware. Later in the year,
the Travis service launched support for testing from GitHub pull requests,
and this became the front line of automated
checking for all
incoming new packages to OPAM.
A major headache with automated testing is usually setting up the right build
environment with external library dependencies, and so we added Docker
support to make it
easier to bulk-build packages for local developer use, with the results of
builds available publically for
anyone to help triage.  Unfortunately fixing the bugs themselves is still a
very manual process, so
more volunteers are always welcome to help out!
We're going to be really seeing the rewards from all this effort as OCaml
4.02 development proceeds, since we can now adopt a data-driven approach
to changing language features instead of guessing how much third-party
code will break.  If your code is in OPAM, then it'll be tested as new
features such as module aliases,
injectivity and
extension points show up.
Better documentation

The venerable
OCamlDoc tool
has done an admirable job for the last decade, but is increasingly showing its
age due to a lack of support for cross-referencing across packages.  We started
working on this problem in the summer when Vincent Botbol
visited us on an internship, expecting it to be a quick job to come up with
something as good as Haskell's excellent Haddock online documentation.
Instead, we ran into the "module wall": since OCaml makes it so easy to
parameterise code over other modules, it makes it hard to generate static
documentation without outputting hundreds of megabytes of HTML every time.
After some hard work from Vincent and Leo, we've got a working prototype that
lets you simply run opam install opam-doc && opam doc core async to generate
package documentation.  You can see the results for
Mirage online, but expect to see this integrated
into the main OCaml site for all OPAM packages as we work through polishing up
the user interface.
Turning OPAM into libraries

The other behind-the-scenes effort for OPAM has been to keep the core command-line
tool simple and stable, and to have it install OCaml libraries that can be
interfaced with by other tools to do domain-specific tasks.  Thomas Gazagnaire,
Louis Gesbert and David Sheets have been steadily hacking away at this and
we now have opamfu to run operations
over all packages, and an easy-to-template opam2web
that generates the live opam.ocaml.org website.
This makes OPAM easier to deploy within other organizations that want to integrate
it into their workflow.  For example, the software
section of the OCaml Labs website is regularly
generated from a search of all OPAM packages tagged ocamllabs.  We also used
it to rewrite the entire OPAM repository in one epic diff to add external
library dependencies via a command-line shim.
OPAM-in-a-Box

All of this effort is geared towards making it easier to maintain reusable
local OPAM installations.  After several requests from big universities to help
out their teaching needs, we're putting together all the support needed to
easily redistribute OPAM packages via an
"OPAM-in-a-Box" command that uses
Docker containers to let you clone and do lightweight
modifications of OCaml installations.
This will also be useful for anyone who'd like to run tutorials or teach OCaml,
without having to rely on flaky network connectivity at conference venues: a problem we've  suffered from too!
Core Compiler

Starting to work on a real compiler can often be a daunting prospect, and so
one initiative we started this year is to host regular compiler hacking sessions where people could find a curated list of features to work on, with the regular developers at hand to help out when people get stuck, and free beer and pizza to oil the coding wheels.  This has worked out well, with around 20 people showing up on average for the three we held, and several patches submitted upstream to OCaml.  Gabriel Scherer and Damien Doligez have been helping this effort by tagging junior jobs in the OCaml Mantis bug tracker as they are filed.
Syntax transformations and extension points

Leo White started the year fresh out of completing his PhD with Alan Mycroft, and before he realized what he'd
gotten himelf into was working with Alain Frisch on the
future of syntax transformations in OCaml.  We started off our first
wg-camlp4 working group on the new
lists.ocaml.org host, and a spirited discussion
started that
went on
and on for
several months.  It ended with a very satisfying design for a simpler extension
points mechanism which Leo presented
at the OCaml 2013 workshop at ICFP, and is now merged into OCaml 4.02-trunk.
Namespaces

Not all of the working groups were quite as successful in coming to a conclusion as the Camlp4 one.  On the Platform mailing list, Gabriel Scherer started a discussion on the design for namespaces in OCaml.  The resulting discussion was useful in separating multiple concerns that were intermingled in the initial proposal, and Leo wrote a comprehensive blog post on a proposed namespace design.
After further discussion at ICFP 2013 with Jacques Garrigue later in the year, it turns out adding support for module aliases would solve much of the cost associated with compiling large libraries such as Core, with no backwards compatibility issues.  This solution has now been integrated into OCaml 4.02.0dev and is being tested with Core.
Delving into the bug tracker

Jeremy Yallop joined us in April, and he and Leo also leapt into the core
compiler and started triaging issues on the OCaml bug
tracker.  This seems unglamorous in the
beginning, but there rapidly turned out to be many fascinating threads that
shed light on OCaml's design and implementation through seemingly harmless
bugs.  Here is a pick of some interesting threads through the year that we've
been involved with:
An unexpected interaction between variance and GADTs
 that led to Jacques Garrigue's talk at OCaml 2013.Type unsoundness by pattern matching lazy mutable values, thus
 shedding light on the precise semantics of the order of pattern matching.Leo proposed an open types extension
 to allow abstract types to be declared open. You can try it via opam switch 4.00.1+open-types.Designing the popular, but controversial record disambiguation feature in OCaml 4.01.0, and debating the right warnings needed to prevent programmer surprise.Exposing a GADT representation for Bigarray.

This is just a sample of some of the issues solved in Mantis; if you want to
learn more about OCaml, it's well worth browsing through it to learn from over
a decade of interesting discussions from all the developers.
Thread-local storage runtime

While OCamlPro was working on their reentrant OCaml runtime, we took a different tack by adding thread-local storage to the runtime instead, courtesy of Stephen Dolan.  This is an important choice to make at the outset of adding multicore, so both approaches are warranted.  The preemptive runtime adds a lot of code churn (due to adding a context parameter to most function calls) and takes up a register, whereas the thread-local storage approach we tried doesn't permit callbacks to different threads.
Much of this work isn't interesting on its own, but forms the basis for a fully multicore runtime (with associated programming model) in 2014. Stay tuned!
Ctypes

One other complaint from the Consortium members was quite surprising: the difficulty of using the OCaml foreign function interface safely to interface with C code.  Jeremy Yallop began working on the ctypes library that had the goal of eliminating the need to write any C code at all for the vast majority of foreign bindings.
Instead, Ctypes lets you describe any C function call as an OCaml value, and provides various linkage options to invoke that function into C.  The first option he implemented was a dlopen interface, which immediately brought us the same level of functionality as the Python or Haskell Ctypes equivalents.  This early code was in itself startlingly useful and more pleasant to use than the raw FFI, and various folk (such as David Sheets' libsodium cryptography bindings) started adopting it.
At this point, I happened to be struggling to write the Foreign Function Interface chapter of Real World OCaml without blowing through our page budget with a comprehensive explanation of the existing system.  I decided to take a risk and write about Ctypes instead, since it let new users to the language have a far more productive experience to get started.  Xavier Leroy pointed out some shortcomings of the library in his technical book review, most notably with the lack of an interface with C.  The design of Ctypes fully supports alternate linking mechanisms than just dlopen though, and Jeremy has added automatic C stub generation support as well.  This means that if you use Ctypes to build an OCaml binding in 2014, you can choose several mechanisms for the same source code to link to the external system.  Jeremy even demonstrated a forking model at OCaml 2013 that protects the OCaml runtime from the C binding via process separation.
The effort is paying off: Daniel Bünzli ported SDL2 using ctypes, and gave us extensive feedback about any missing corner cases, and the resulting bindings don't require any C code to be written. Jonathan Protzenko even used it to implement an OCaml controlle
r for the Adafruit Raspberry Pi RGB LCD!
Community Efforts

Our community efforts were largely online, but we also hosted visitors over the year and regular face-to-face tutorials.
Online at OCaml.org

While the rest of the crew were hacking on OPAM and OCaml, Amir Chaudhry and Philippe Wang teamed up with Ashish Agarwal and Christophe Troestler to redesign and relaunch the OCaml website.  Historically, OCaml's homepage has been the caml.inria.fr domain, and the ocaml.org effort was begun by Christophe and Ashish some years ago to modernize the web presence.
The webpages were already rather large with complex scripting (for example, the 99 Problems page runs the OCaml code to autogenerate the output). Philippe developed a template DSL that made it easier to unify a lot of the templates around the website, and also a Markdown parser that we could link to as a library from the rest of the infrastructure without shelling out to Pandoc.
Meanwhile, Amir designed a series of interactive wireframe sketches  and gathered feedback on it from the community. A local design agency in Cambridge helped with visual look and feel, and finally at the end of the summer we began the migration to the new website, followed by a triumphant switchover in November to the design you see today.
The domain isn't just limited to the website itself.  Leo and I set up a SVN-to-Git mirror of the OCaml compiler Subversion repository on the GitHub OCaml organization, which is proving popular with developers.  There is an ongoing effort to simplify the core compiler tree by splitting out some of the larger components, and so camlp4 is also now hosted on that organization, along with OASIS.  We also administer several subdomains of ocaml.org, such as the mailing lists and the OPAM repository, and other services such as the OCaml Forge are currently migrating over.  This was made significantly easier thanks to sponsorship from Rackspace Cloud (users of XenServer which is written in OCaml). They saw our struggles with managing physical machines and gave us developer accounts, and all of the ocaml.org infrastructure is now hosted on Rackspace.  We're very grateful to their ongoing help!
If you'd like to contribute to infrastructure help  (for example, I'm experimenting with a GitLab mirror), then please join the infrastructure@lists.ocaml.org mailing list and share your thoughts.  The website team also need help with adding content and international translations, so head over to the website issue tracker and start proposing improvements you'd like to see.
Next steps for ocaml.org

The floodgates requesting features opened up after the launch of the new look and feel.  Pretty much everyone wanted deeper OPAM integration into the main website, for features such as:
Starring and reviewing packagesIntegrating the opam-doc documentation with the metadataDisplay test results and a compatibility matrix for non-x86 and non-Linux architectures.Link to blog posts and tutorials about the package.

Many of these features were part of the original wireframes but we're being careful to take a long-term view of how they should be create
d and maintained.Rather than building all of this as a huge bloated opam2web extension, David Sheets (our resident relucant-to-admit-it web expert) has designed an overlay directory scheme that permits the overlaying of different metadata onto the website. This lets one particular feature (such as blog post aggregation) be handled separately from the others via Atom aggregators.
 Real World OCaml

A big effort that took up most of the year for me was finishing and publishing an O'Reilly book called Real World OCaml with Yaron Minsky and Jason Hickey.  Yaron describes how it all started in his blog post, but I learnt a lot from developing a book using the open commenting scheme that we
developed just for this.
In particular, the book ended up shining a bright light into dark language corners that we might otherwise not have explored in OCaml Labs.  Two chapters of the book that I wasn't satisfied with were the objects and classes chapters, largely since neither Yaron nor Jason nor I had ever really used their full power in our own code.  Luckily, Leo White decided to pick up the baton and champion these oft-maligned (but very powerful) features of OCaml, and the result is the clearest explanation of them that I've read yet.  Meanwhile, Jeremy Yallop helped out with extensive review of the Foreign Function Interface chapter that used his ctypes library.  Finally, Jeremie Dimino at Jane Street worked hard on adding several features to his utop toplevel that made it compelling enough to become our default recommendation for newcomers.
All in all, we ended up closing over 2000 comments in the process of writing the book, and I'm very proud of the result (freely available online, but do buy a copy if you can to support it).  Still, there's more I'd like to do in 2014 to improve the ease of using OCaml further.  In particular, I removed a chapter on packaging and build systems since I wasn't happy with its quality, and both Thomas Gazagnaire and I intend to spend time in 2014 on improving this part of the ecosystem.
Tutorials and Talks

We had a lively presence at ICFP 2013 this year, with the third iteration of the OCaml 2013 held there, and Stephen Dolan presenting a paper in the main conference.  I liveblogged the workshop as it happened, and all the talks we gave are linked from the program.  The most exciting part of the conference for a lot of us were the two talks by Facebook on their use of OCaml: first for program analysis using Pfff and then to migrate their massive PHP codebase using an OCaml compiler.  I also had the  opportunity to participate in a panel at the Haskell Workshop on whether Haskell is too big to fail yet; lots of interesting perspectives on scaling another formerly academic language into the real world.
Yaron Minsky and I have been giving tutorials on OCaml at ICFP for several years, but the release of Real World OCaml has made it significantly easier to give tutorials without the sort of labor intensity that it took in previous years (one memorable ICFP 2011 tutorial that we did took almost 2 hours to get everyone installed with OCaml.  In ICFP 2013, it took us 15 minutes or so to get everyone started).  Still, giving tutorials at ICFP is very much preaching to the choir, and so we've started speaking at more general-purpose events.
Our first local effort was FPDays in Cambridge, where Jeremy Yallop and Amir Chaudhry ran the tutorial with help from Phillipe Wang, Leo White and David Sheets. The OCaml session there ended up being the biggest one in the entire two days, and Amir wrote up their experiences.  One interesting change from our ICFP tutorial is that Jeremy used js_of_ocaml to teach OCaml via JavaScript by building a fun Monty Hall game.
Visitors and Interns

Since OCaml Labs is a normal group within the Cambridge Computer Lab, we often host academic visitors and interns who pass through.  This year was certainly diverse, and we welcomed a range of colleagues:
Mathias Bourgoin has just finished his work on interfacing OCaml with GPUs, and gave us a seminar on how his SPOC tool works (also available in OPAM via a custom remote).Benjamin Canou (now at OCamlPro) practised his OCaml 2013 talk on building high-level interfaces to JavaScript with OCaml by giving a departmental seminar.Roberto Di Cosmo, who directs the IRILL organization on Free Software in Paris delivered a seminar on constraint solving for package systems that are as large-scale as Debian's.Thomas Gazagnaire visited during the summer to help plot the Mirage 1.0 and OPAM 1.1 releases.  He has also since joined OCaml Labs fulltime to work on Nymote.Louis Gesbert from OCamlPro visited for 2 weeks in December and kicked off the inaugral OPAM developers summit (which was, admittedly, just 5 developers in the Kingston Arms, but all good things start in a pub, right?)Jonathan Protzenko presented his PhD work on Mezzo (which is now merged into OPAM), and educated us on the vagaries of Windows support.Gabriel Scherer from the Gallium INRIA group visited to discuss the direction of OPAM and various language feature discussions (such as namespaces).  He didn't give a talk, but promises to do so next time!Benoît Vaugon gave a seminar on his OCamlCC OCaml-to-C compiler, talked about porting OCaml to 8-bit PICs, and using GADTs to implement Printf properly.

We were also visited several times by Wojciech Meyer from ARM, who was an OCaml developer who maintained (among other things) the ocamlbuild system and worked on DragonKit (an extensible LLVM-like compiler written in OCaml).  Wojciech very sadly passed away on November 18th, and we all fondly remember his enthusiastic and intelligent contributions to our small Cambridge community.
We also hosted visitors to live in Cambridge and work with us over the summer.  In addition to Vincent Botbol (who worked on OPAM-doc as described earlier) we had the pleasure of having Daniel Bünzli and Xavier Clerc work here.  Here's what they did in their own words.
Xavier Clerc: OCamlJava

Xavier Clerc took a break from his regular duties at INRIA to join us over the summer
to work on OCaml-Java and adapt it to the latest
JVM features.  This is an incredibly important project to bridge OCaml with the huge
Java community, and here's his report:
After a four-month visit to the OCaml Labs dedicated to the OCaml-Java
project, the time has come for an appraisal! The undertaken work can be split
into two areas: improvements to code generation, and interaction between the
OCaml & Java languages.  Regarding code generation, several classical
optimizations have been added to the compiler, for example loop unrolling,
more aggressive unboxing, better handling of globals, or partial evaluation
(at the bytecode level). A new tool, namely ocamljar, has been introduced
allowing post-compilation optimizations. The underlying idea is that some
optimizations cannot always be applied (e.g. depending whether multiple
threads/programs will coexist), but enabling them through command-line flags
would lead to recompilation and/or multiple installations of each library
according to the set of chosen optimizations. It is thus far more easier to
first build an executable jar file, and then modify it according to these
optimizations. Furthermore, this workflow allows the ocamljar tool to take
advantage of whole-program information for some optimizations.  All these
improvements, combined, often lead to a gain of roughly 1/3 in terms of
execution time.
Regarding language interoperability, there are actually two directions 
depending on whether you want to call OCaml code from Java, or want to call 
Java code from OCaml. For the first direction, a tool allows to generate Java
source files from OCaml compiled interfaces, mapping the various constructs
of the OCaml language to Java classes. It is then possible to call functions, 
and to manipulate instances of OCaml types in pure Java, still benefiting 
from the type safety provided by the OCaml language. In the other direction,
an extension of the OCaml typer is provided allowing to create and manipulate
Java instances directly from OCaml sources. This typer extension is indeed a
thin layer upon the original OCaml typer, that is mainly responsible for
encoding Java types into OCaml types.  This encoding uses a number of
advanced elements such as polymorphic variants, subtyping, variance
annotations, phantom typing, and printf-hack, but the end-user does not have
to be aware of this encoding. On the surface, the type of instances of the
Java Object classes is java'lang'Object java_instance, and instances can be
created by calling Java.make Object().
While still under heavy development, a working prototype is available, and bugs can be reported. Finally, I would like to thank the OCaml Labs for providing a great working
environment.


Daniel Bünzli: Typography and Visualisation

Daniel joined us from Switzerland, and spent some time at Citrix before joining us in OCaml Labs.  All of his software is now on OPAM, and is seeing ever-increasing adoption from the community.
Released a first version of Vg [...] I'm
especially happy about that as I wanted to use and work on these ideas since
at least 2008. The project is a long term project and is certainly not
finished yet but this is already a huge step.
Adjusted and released a first version of
Gg. While the module was already mostly
written before my arrival to Cambridge, the development of Vg and Vz prompted
me to make some changes to the module.
[...] released Otfm, a
module to decode OpenType fonts. This is a work in progress as not every
OpenType table has built-in support for decoding yet. But since it is needed
by Vg's PDF renderer I had to cut a release. It can however already be used
to implement certain simple things like font kerning with Vg, this can be
seen in action in the vecho binary installed by Vg.
Started to work on Vz, a
module for helping to map data to Vg images. This is really unfinished and is
still considered to be at a design stage. There are a few things that are
however well implemented like (human) perceptually meaningful color
palettes and the
small folding stat module (Vz.Stat). However it quickly became evident that
I needed to have more in the box w.r.t. text rendering in Vg/Otfm. Things
like d3js entirely rely on the SVG/CSS support for text which makes it easy
to e.g. align things (like tick labels on such
drawings). If you can't
rely on that you need ways of measuring rendered text. So I decided to
suspend the work on Vz and put more energy in making a first good release of
Vg. Vz still needs quite some design work, especially since it tries to be
independent of Vg's backend and from the mechanism for user input.
Spent some time figuring out a new "opam-friendly" release workflow in 
pkgopkg. One of my problem is that by designing in the small for programming
in the large --- what a slogan --- the number of packages I'm publishing is
growing (12 and still counting). This means that I need to scale horizontally
maintenance-wise unhelped by the sad state of build systems for OCaml. I need
tools that make the release process flawless, painless and up to my quality
standards. This lead me to enhance and consolidate my old scattered
distribution scripts in that repo, killing my dependencies on Oasis and
ocamlfind along the way. (edited for brevity, see here)


Daniel also left his bicycle here for future visitors to use, and the "Bünzli-bike"
is available for our next visitor! (Louis Gesbert even donated lights, giving
it a semblance of safety).
Industrial Fellows

Most of our regular funding bodies such as EPSRC or EU FP7 provide funding, but leave all the intellectual input to the academics.  A compelling aspect of OCaml Labs has been how involved our industrial colleagues have been with the day-to-day problems that we solve.  Both Jane Street and Citrix have senior staff regularly visiting our group and working alongside us as industrial fellows in the Computer Lab.
Mark Shinwell from Jane Street Europe has been working on improving the state of native debugging in OCaml, by adding extended DWARF debugging information to the compiler output.  Mark is also a useful source of feedback about the forthcoming design of multicore, since he has daily insight into a huge production codebase at Jane Street (and can tell us about it without us requiring access!).Dave Scott is the principal architect of XenServer at Citrix in Cambridge.  This year has been transformative for that project, since Citrix open-sourced XenServer to GitHub and fully adopted OPAM into their workflow.  Dave is the author of numerous libraries that have all been released to OPAM, and his colleagues Jon Ludlam and Euan Harris are also regular visitors who have also been contributors to the OPAM and Mirage ecosystems.

Research Projects

The other 100% of our time at the Labs is spent on research projects.  When we started the group, I wanted to set up a feedback loop between local people using OCaml to build systems, with the folk developing OCaml itself.  This has worked out particularly well with a couple of big research projects in the Lab.
 Mirage

Mirage is a library operating system written in OCaml that compiles source code into specialised Xen microkernels, developed at the Cambridge Computer Lab, Citrix and the Horizon Digital Economy institute at Nottingham.  This year saw several years of effort culminate in the first release of Mirage 1.0 as a self-hosting entity.  While Mirage started off as a quick experiment into building specialised virtual appliances, it rapidly became useful to make into a real system for use in bigger research projects.  You can learn more about Mirage here, or read the Communications of the ACM article that Dave Scott and I wrote to close out the year.
This project is where the OCaml Labs "feedback loop" has been strongest.  A typical Mirage application consists of around 50 libraries that are all installed via OPAM.  These range from device drivers to protocol libraries for HTTP or DNS, to filesystems such as FAT32.  Coordinating regular releases of all of these would be near impossible without using OPAM, and has also forced us to use our own tools daily, helping to sort out bugs more quickly.  You can see the full list of libraries on the OCaml Labs software page.
Mirage is also starting to share code with big projects such as XenServer now, and we have been working with Citrix engineers to help them to move to the Core library that Jane Street has released (and that is covered in Real World OCaml).  Moving production codebases this large can take years, but OCaml Labs is turning out to be a good place to start unifying some of the bigger users of OCaml into one place.  We're also now an official Xen Project incubator project, which helps us to validate functional programming  to other Linux Foundation efforts.
Nymote and User Centric Networking

The release of Mirage 1.0 has put us on the road to simplifying embedded systems programming. The move to the centralized cloud has led to regular well-publicised privacy and security threats to the way we handle our digital infrastructure, and so Jon Crowcroft, Richard Mortier and I are leading an effort to build an alternative privacy-preserving infrastructure using embedded devices as part of the User Centric Networking project, in collaboration with a host of companies led by Technicolor Paris.
This work also plays on the strong points of OCaml: it already has a fast ARM backend, and Mirage can easily be ported to the new Xen/ARM target as hardware becomes available.
One of the most difficult aspects of programming on the "wide area" Internet are dealing with the lack of a distributed identity service that's fully secure.  We published our thoughts on this at the USENIX Free and Open Communications on the Internet workhsop, and David Sheets is working towards a full implementation using Mirage.  If you're interested in following this effort, Amir Chaudhry is blogging at the Nymote project website, where we'll talk about the components as they are released.
 Data Center Networking

At the other extreme from embedded programming is datacenter networking, and we started the Network-as-a-Service research project with Imperial College and Nottingham.  With the rapid rise of Software Defined Networking this year, we are investigating how application-specific customisation of network resources can build fast, better, cheaper infrasructure.  OCaml is in a good position here: several other groups have built OpenFlow controllers in OCaml (most notably, the Frenetic Project), and Mirage is specifically designed to assemble such bespoke infrastructure.
Another aspect we've been considering is how to solve the problem of optimal connectivity across nodes.  TCP is increasingly considered harmful in high-through, high-density clusters, and George Parisis led the design of Trevi, which is a fountain-coding based alternative for storage networking.  Meanwhile, Thomas Gazagnaire (who joined OCaml Labs in November), has been working on a branch-consistent data store called Irminsule which supports scalable data sharing and reconciliation using Mirage.  Both of these systems will see implementations based on the research done this year.
 Higher Kinded Programming

Jeremy Yallop and Leo White have been developing an approach that makes it possible to write programs with higher-kinded polymorphism (such as monadic functions that are polymorphic in the monad they use) without using functors. It's early days yet, but there's a library available on OPAM that implements the approach, and a draft
paper that outlines the design.
Priorities for 2014

This year has been a wild ride to get us up to speed, but we now have a solid sense of what to work on for 2014.  We've decided on a high-level set of priorities led by the senior members of the group:
Multicore: Leo White will be leading efforts in putting an end-to-end multicore capable OCaml together.Metaprogramming: Jeremy Yallop will direct the metaprogramming efforts, continuing with Ctypes and into macros and extension points.Platform: Thomas Gazagnaire will continue to drive OPAM development towards becoming the first OCaml Platform.Online: Amir Chaudhry will develop the online and community efforts that started in 2013.

These are guidelines to choosing where to spend our time, but not excluding other work or day-to-day bugfixing. Our focus on collaboration with Jane Street, Citrix, Lexifi, OCamlPro and our existing colleagues will continue, as well as warmly welcoming new community members that wish to work with us on any of the projects, either via internships, studentships or good old-fashioned open source hacking.
I appreciate the whole team's feedback in editing this long post into shape, the amazing professorial support from Jon Crowcroft, Ian Leslie and Alan Mycroft throughout the year, and of course the funding and support from Jane Street, Citrix, RCUK, EPSRC, DARPA and the EU FP7 that made all this possible.  Roll on 2014, and please do get in touch with me with any queries!

   Hide
        
      
                    by Anil Madhavapeddy at Dec 29, 2013 
      
      
    
  


       
                  Polymorphism for beginners
      (Thomas Leonard)
    
    
                                OCaml makes heavy use of parametric polymorphism (which you may also know as “generics” in other languages).
The OCaml tutorials mention it from time to time, but the information is spread about over many articles and they don’t go into much detail.
I’m not a type theorist, just a Python/Java/C programmer who finds this stuff interesting.
I wanted to write this guide while I still remember the things that confused me.
I know several OCaml experts keep an eye on this blog, so hopefully any inaccuracies will be corrected in the comments.



Table of Contents

  Subtyping          Top and bottom
    
  
  Polymorphism          Polymorphic objects
      Polymorphic variants
    
  
  Example : a polymorphic dialog box
  Other issues          Monomorphic types
      Partial application loses polymorphism
      Universal qualification
      Polymorphism in module signatures
    
  
  Cheat-sheet
  Summary


( This post is part of a series in which I am
converting 0install from Py…Read more...OCaml makes heavy use of parametric polymorphism (which you may also know as “generics” in other languages).
The OCaml tutorials mention it from time to time, but the information is spread about over many articles and they don’t go into much detail.
I’m not a type theorist, just a Python/Java/C programmer who finds this stuff interesting.
I wanted to write this guide while I still remember the things that confused me.
I know several OCaml experts keep an eye on this blog, so hopefully any inaccuracies will be corrected in the comments.



Table of Contents

  Subtyping          Top and bottom
    
  
  Polymorphism          Polymorphic objects
      Polymorphic variants
    
  
  Example : a polymorphic dialog box
  Other issues          Monomorphic types
      Partial application loses polymorphism
      Universal qualification
      Polymorphism in module signatures
    
  
  Cheat-sheet
  Summary


( This post is part of a series in which I am
converting 0install from Python to OCaml, learning OCaml as I go. )

Subtyping

The first thing that confused me was that OCaml tends to use parametric polymorphism where other languages use subtyping (“subtype polymorphism”), so I’ll start with a brief summary of subtyping and then show how OCaml uses parametric polymorphism to achieve similar ends.

Note: in the rest of this article I will always use “polymorphism” to mean “parametric polymorphism”, which is the way the OCaml documentation uses it.

When you think of object oriented programming, you probably think of the types arranged in a tree. In fact, with multiple-inheritance of interfaces,
the types form a lattice, which is easier to draw than to explain:



On the left, we have some example primitive types, int and unit.

In the middle, we have some GUI object types. widget represents “things that can appear on the screen”. button and window are types of widget and a dialog is a type of window. For example, any function that can operate on windows can also operate on dialogs. A button is both a widget and an action. In Java terms, we might write class Button implements Widget, Action.

On the right, we have some variant types (enums). The type “yes or no” is a sub-type of “yes, no or maybe”. For example, a function that can format a yes/no/maybe value as a string will also work on the simpler yes/no type.

The rule is that you can always safely cast a type to a super type (going upwards). However, casting in Java and OCaml work differently:

In Java:

      Upcasting (converting to a type higher up in the lattice) is automatic and implicit. e.g. in Widget w = new Dialog();
  
      Downcasting requires an explicit cast, and may throw an exception: Dialog d = (Dialog) w.
  


In OCaml:

      Upcasting must be explicit, e.g. let w : widget = (new dialog :> widget).
  
      Downcasting is impossible (types are not recorded at runtime, so it wouldn’t be able to check).
  


Here’s an example that might surprise you if you’re expecting automatic upcasts:

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
class window =
  object
    method close = ...
  end
class dialog =
  object
    inherit window
    method get_response = ...
  end
let close_window (w:window) : unit =
  w#close
let () =
  let d = new dialog in
  close_window d		(* ERROR! *)


Error: This expression has type dialog but an expression was expected of type window
       The second object type has no method get_response


OCaml won’t let you pass a dialog to close_window because it’s a dialog, not a window and there are no automatic coercions in OCaml. However, OCaml does know the subtyping relationship and will therefore let you cast it:

1
2
3
let () =
  let d = new dialog in
  close_window (d :> window)	(* OK *)


Functions also have types and can be cast too, so here’s another way to solve this problem:

1
2
3
let () =
  let d = new dialog in
  (close_window :> (dialog -> unit)) d


Here, we upcast close_window to the type dialog -> unit and then pass it a dialog.

By the way, notice that dialog is a subtype of window but dialog -> unit is a super-type of window -> unit. See covariance and contravariance for the details about that.

Top and bottom

The types top and bottom don’t seem to exist in OCaml, but I included them for completeness and because they’re conceptually interesting and you’ll probably run across these terms when reading anything about types.

Top is a super-type of everything, like object in Python (or Object in Java, if you ignore unboxed types). Everything is a top and, therefore, knowing that something is a top tells you nothing at all. Because OCaml doesn’t allow downcasting, there’s not much you could do with a top in OCaml anyway.

Bottom is a sub-type of everything. A value of type bottom can be used as an int, a widget, a yes/no enum, etc. Needless to say, instances of this type don’t actually exist.

Although OCaml doesn’t have a bottom type, it achieves the same effect with polymorphism.
For example, the result of these expressions can be used as any type you like:

  exit 1 (exits the program)
  failwith msg (throws an exception)
  let rec loop () = loop () in loop () (infinite loop)
  Obj.magic x (unsafe cast; program may segfault if you get it wrong!)


So, this code compiles fine, even though we use bot as an int and as a string:

1
2
3
4
5
let () =
  let bot = exit 1 in
  let x : int = bot in
  let y : string = bot in
  Printf.printf "x = %d, y = %s" x y


Polymorphism

If you treat OCaml like Java then things mostly work fine; you just end up doing a lot of explicit casting. However, where Java uses implicit upcasts, OCaml generally prefers using (parametric) polymorphism.

A polymorphic value doesn’t have a single concrete type. Instead, it can take on many different types as needed.
For example, the OCaml function Queue.create can create queues of ints, queues of strings, etc. Its type is unit -> 'a Queue.t, where 'a is a type variable. When you want to use this function, you can use any type (e.g. int or string) as the value of 'a, to get a function that makes queues of ints or queues of strings, as needed.

Note that unlike e.g. C++ templates, using polymorphism does not create any extra code. There is only ever one Queue.create function compiled into your binary, not one for each type you use. The same generic code works for queues of ints and queues of strings.

In Java, upcasting types is implicit, while using polymorphism (generics) requires extra annotations. In OCaml, it’s the other way around. Polymorphism is implicit, while upcasting requires annotations. So, in Java:

1
2
3
List<String> items = new LinkedList<String>();
items.add("hello");
items.add("world");


The <String> parts show where we convert a generic type (list of X) to a concrete type (list of String). In OCaml, this happens implicitly:

1
2
3
let items = new linked_list in
items#add "hello";
items#add "world";


(Note: I used a made-up linked_list class rather than Queue to keep this example similar to the Java)

This saves a lot of typing, which is good, but it also makes it far harder to understand what’s going on.
The way I think of it, OCaml adds type variables at certain points in the code and then uses type inference to work out what they are.
So in this case, we have (note: this is not valid OCaml syntax):

1
2
3
let items = new linked_list<t> in
items#add "hello";
items#add "world";


Then OCaml infers that type t = string.

Polymorphic objects

The problem with our close_window function in the subtyping section was that we gave it a fixed concrete type (window -> unit):

1
let close_window (w:window) : unit = w#close


If we give it a polymorphic type then there’s no problem:

1
2
3
4
5
let close_window_poly (w:#window) : unit = w#close
let () =
  let d = new dialog in
  close_window_poly d


Here, #window means any type with at least the methods of window (not to be confused with w#close, which is a method call, not a type). The syntax is a bit confusing because OCaml hides the type variable by default. If you wanted to declare the type of close_window_poly explicitly, you’d have to make the type variable explicit using as. For comparison, here are the types of the two versions:

1
2
type window_closer = window -> unit
type 'a closer = (#window as 'a) -> unit


Thus the type window closer is the type of functions that close a window, while dialog closer is the type of functions that close a dialog. OCaml will automatically apply the appropriate type at compile time.

As OCaml will infer polymorphic types automatically, we can define and call the function without any type annotations at all:

# let close_window_inf w = w#close;;
val close_window_inf : < close : 'a; .. > -> 'a = <fun>
# close_window_inf d;;


Here is yet more new syntax! OCaml didn’t infer that this requires a #window, only that it requires something with a suitable close method. Also, it couldn’t infer that close must return unit, so it left the return type generic as 'a. 

The .. indicates more polymorphism, with another hidden type variable. If we wanted to define the type for this function, it would be:

type ('a, 'b) closer2 = (< close : 'a; .. > as 'b) -> 'a


This is the polymorphic type for functions which take objects of type 'b, where 'b includes a close method that returns an 'a, and return an 'a.

For example, the original close_window function has the type (unit, window) closer2.

What all this means is that defining objects and functions without explicit types usually works fine, but if you later try to add type annotations (e.g. by declaring an interface for your module in an .mli file) then you’re likely to remove the polymorphism accidentally unless you’re careful. If you don’t understand what happened, you’ll end up doing a load of explicit casting to make things work again.

A good way around this is to use ocamlc -i to generate an initial .mli file with all the inferred types, with all the polymorphism still there.

In another bit of inconsistent syntax, when defining a class type you need to put the type parameter in brackets, but you don’t when declaring an object type:

1
2
3
4
5
6
7
8
9
10
(* An object with a "get" method returning an 'a *)
type 'a poly_type = <
  get : 'a
>
(* A class type with a "get" method *)
class type ['a] poly_class =
  object
    method get : 'a
  end


(see my previous Experiences With OCaml Objects post for more on objects and classes)

Polymorphic variants

The situation with variants (enums) is similar. Let’s start with some non-polymorphic code:

1
2
3
4
5
6
7
8
9
10
11
12
13
14
let paint_sky (colour : [ `blue | `black ]) =
  match colour with
  | `blue -> print_endline "A clear blue sky"
  | `black -> print_endline "A dark black sky"
let paint_balloon (colour : [`blue | `black | `red ]) =
  match colour with
  | `black -> print_endline "A black balloon"
  | `blue -> print_endline "A blue balloon"
  | `red -> print_endline "A red balloon"
let draw_scene (colour : [`blue | `black]) =
  paint_sky colour;
  paint_balloon colour	(* Error *)


Error: This expression has type [ `black | `blue ]
       but an expression was expected of type [ `black | `blue | `red ]
       The first variant type does not allow tag(s) `red


Again, if you’re expecting subtyping behaviour then this is confusing. If we can draw a balloon blue, black or red, why can’t we draw it blue or black? Again, we can use an upcast (which is checked by the compiler and is entirely type-safe):

1
2
3
let draw_scene (colour : [`blue | `black]) =
  paint_sky colour;
  paint_balloon (colour :> [`blue | `black | `red])


As before, OCaml generally expects you to use polymorphism instead. There’s more new syntax for this:

1
2
3
4
5
let paint_balloon (colour : [< `blue | `black | `red ]) =
  match colour with
  | `black -> print_endline "A black balloon"
  | `blue -> print_endline "A blue balloon"
  | `red -> print_endline "A red balloon"


Here [< `blue | `black | `red ] means all subtypes of [ `blue | `black | `red ]. The < introduces another hidden type variable. If you wanted to define a type for this function explicitly, you could do it with:

type 'a balloon_painter = ([<`blue | `black | `red ] as 'a) -> unit


Instead of <, there’s also >, which means the variant must have at least the given elements.

As before, just removing the type annotations and letting OCaml infer the polymorphic type is easy:

1
2
3
4
let paint_balloon = function
  | `black -> print_endline "A black balloon"
  | `blue -> print_endline "A blue balloon"
  | `red -> print_endline "A red balloon"


Example : a polymorphic dialog box

Here’s a neat polymorphic dialog box (based on lablgtk’s GTK bindings):

dialog.mli 
1
2
3
4
5
6
class type ['a] dialog =
  object
    constraint 'a = [> `close]
    method add_button : label:string -> code:'a -> unit
    method get_response : 'a
  end


prog.ml 
1
2
3
4
5
6
let run_dialog () =
  let d = new Dialog.dialog in
  d#add_button ~label:"Delete" ~code:`delete;
  match d#get_response with
  | `delete -> print_endline "Deleting..."
  | `close -> print_endline "Aborted"


OCaml will automatically infer 'a as [`close | `delete] - a dialog with close and delete responses.
OCaml will force you to handle every response code, so you can’t add a button but forget to handle it!
The constraint 'a = [> `close] line forces you to handle the close response in all cases (because the user could always just close the window).

For another example, see my earlier post 
Option Handling With OCaml Polymorphic Variants.

Other issues

Monomorphic types

There’s a subtle but important distinction between polymorphic types (which can be used to generate many concrete types) and monomorphic types (which OCaml uses to mean a single type that is currently undecided). Monomorphic types only occur while OCaml is still working out the types, so you’ll only see them in compiler error messages or in the interactive toplevel. They look like regular type variables but start with an underscore. e.g.

# let x = [];;
val x : 'a list = []

# let y = ref None;;
val y : '_a option ref = {contents = None}


Here, x has a polymorphic type (using 'a). It can be used as an empty list of ints, or as an empty list of strings, or both. Every time you use x, you get to pick a type for it.

y has a monomorphic type (using '_a). It can be used as a mutable container of ints, or of strings, but not both. As soon as OCaml sees it used with a concrete type, it will assign that type for it:

# let y = ref None;;
# y;;
- : '_a option ref = {contents = None}

# y := Some 3;;
# y := None;;
# y;;
- : int option ref = {contents = None}

# y := Some "hello";;
Error: This expression has type string but an expression was expected of type int


Notice that the second time we print y, OCaml has worked out the concrete type.

Note: The term “weakly polymorphic” seems to be used as an alias for “monomorphic” in some OCaml documentation.

Partial application loses polymorphism

Partially-applied functions lose their polymorphism. Consider:

# let p1 = Printf.fprintf;;
val p1 : out_channel -> ('a, out_channel, unit) format -> 'a = <fun>

# let p2 = Printf.fprintf stdout;;
val p2 : ('_a, out_channel, unit) format -> '_a = <fun>

# let p3 fmt = Printf.fprintf stdout fmt;;
val p3 : ('a, out_channel, unit) format -> 'a = <fun>


fprintf is a polymorphic function. It takes an output channel, a format string (with a polymorphic type) and values of the appropriate type. p1 retains the polymorphic type.
However, p2, which partially applies the function to stdout, has a monomorphic type ('_a). You could use p2 to print a string, or to print an int, but you couldn’t use it twice to do both.
p3, which just makes the format argument explicit, is polymorphic again!

1
2
3
4
5
6
7
8
9
10
11
12
let p1 = Printf.fprintf in
let p2 = Printf.fprintf stdout in
let p3 fmt = Printf.fprintf stdout fmt in
p1 stdout "%d" 3;
p1 stdout "%s" "Hello";	(* OK *)
p2 "%d" 3;
p2 "%s" "Hello";  (* Error *)
p3 "%d" 3;
p3 "%s" "Hello";  (* OK! *)


So, what’s going on here?
The OCaml FAQ explains what to do about it (use p3), but doesn’t explain why.
As fprintf has a really complicated type, let’s switch to a simpler example:

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
let logged_id msg x =
  print_endline msg;
  x
let () =
  let i1 = logged_id in
  let i2 = logged_id "called" in
  let i3 x = logged_id "called" x in
  assert (i1 "called" 1 = 1);
  assert (i1 "called" "hi" = "hi");	(* OK *)
  assert (i2 1 = 1);
  assert (i2 "hi" = "hi");	(* Error! *)
  assert (i3 1 = 1);
  assert (i3 "hi" = "hi");	(* OK *)


logged_id msg x prints the message and then just returns x.

The polymorphism is lost in i2 because OCaml doesn’t know, from the type of logged_id, whether partially applying it will create any mutable state. Consider:

1
2
3
4
5
6
7
let logged_mem msg =
  let prev = ref None in
  fun x ->
    print_endline msg;
    match !prev with
    | None -> prev := Some x; x
    | Some p -> p


When logged_mem is partially applied, it creates a mutable cell and returns a function that remembers the first value it is called with and always returns that.
The result of logged_mem "called" cannot be polymorphic: it must always be called with the same argument type. Yet logged_mem has the same type as logged_id (string -> 'a -> 'a), so OCaml can’t distinguish the two cases.

OCaml assumes that every function call potentially creates mutable state. Therefore, the result of calling a function is never polymorphic. Normally that’s what you want, but it can be surprising in the case of partial functions. The solution (i3) is to avoid partial application and do a complete fresh invocation each time.

There are actually some cases where OCaml can turn a monomorphic result type back into a polymorphic one. For example:

# let make_empty () = [];;
val make_empty : unit -> 'a list = <fun>

# let x = make_empty ();;
val x : 'a list = []


Here, x gets a polymorphic type despite being the result of a function call.
See Relaxing the Value Restriction for how it does that.

Universal qualification

This surprised me at first:

# let id = fun x -> x;;
val id : 'a -> 'a = <fun>

# let id2 : ('a -> 'a) = fun x -> x + 1;;
val id2 : int -> int = <fun>


OCaml tells us that the identity function id has the type 'a -> 'a. OK. Then I declare another function with the same type, and OCaml tells me its type is int -> int!

OCaml is hiding part of the type! The real type of the identity function is 'a. 'a -> 'a (given 'a, the type is 'a -> 'a). Using this full type, we get the expected error:

# let id : 'a. 'a -> 'a = fun x -> x;;
val id : 'a -> 'a = <fun>

# let id2 : 'a. 'a -> 'a = fun x -> x + 1;;
Error: This definition has type int -> int which is less general than
         'a. 'a -> 'a


The input to the polymorphic type expression (the 'a. bit) can only go at the start of a type expression in OCaml. That means that you can’t write a function that takes a polymorphic function as an argument. For example:

# let use_id (id : 'a. 'a -> 'a) =
    assert (id 3 = 3);
    assert (id "hi" = "hi") ;;
Error: Syntax error: type expected.


However, you can put them in object method and record field types:

1
2
3
4
5
6
7
type id_holder = {
  id : 'a. 'a -> 'a
}
let use_id id_holder =
  assert (id_holder.id 3 = 3);
  assert (id_holder.id "hi" = "hi")


Or:

1
2
3
4
5
6
7
8
class type id_class =
  object
    method id : 'a. 'a -> 'a
  end
let use_id (id_obj:id_class) =
  assert (id_obj#id 3 = 3);
  assert (id_obj#id "hi" = "hi")


So, you may have to wrap some things up in objects or records. If you use this in a method type, make sure you declare the type of the object you’re creating. Otherwise, OCaml will infer the wrong type (it will assume that 'a is scoped to the whole object, not a single method):

1
2
3
4
5
6
7
8
9
10
11
class type id_class =
  object
    method id : 'a. 'a -> 'a
  end
let make_id_obj () =
  object
    method id x = x
  end
let id_obj : id_class = make_id_obj ()	 (* Error *)


Error: This expression has type < id : 'a -> 'a >
       but an expression was expected of type id_class
       The universal variable 'a0 would escape its scope


You have to give the type at the point where you define the object:

1
2
3
4
let make_id_obj () =
  object (self : id_class)
    method id x = x
  end


Polymorphism in module signatures

When you define a module, you can (optionally) write an .mli file giving it a more limited public interface.
In this interface, you can make types abstract, only expose certain functions, etc. 
If you include a value with concrete type, the signature must be the same in the module and in the interface.
But when your module contains polymorphic values, you are allowed to limit the polymorphism in the module signature. For example:

test.mli 
1
val close_window : #dialog -> unit


test.ml 
1
2
3
4
5
let close_window w = w#close
let () =
  let w = new window in
  close_window w


Inside the module, close_window will close anything with a close method and return whatever it returns.
But outside the module, close_window can only close subclasses of dialog (and always returns unit).

Cheat-sheet

Some concrete types:

            Type expression
      Meaning
    
  
            int list
      a list of integers
    
          int -> int
      a function that takes an int and returns an int
    
          widget
      the concrete type widget exactly
    
          <close : unit>
      an object type with a single close method
    
          [`red|`green]
      the variant type “red or green”
    
  


Some polymorphic types (supplying the type parameter 'a will produce some concrete type):

            Type expression
      Can produce types for…
    
  
            'a list
      any list of items, each of the same type
    
          'a -> 'a
      any function that returns something of the same type as its input
    
          (#widget as 'a)
      any type that includes all the methods of widget
    
          (<close : unit; ..> as 'a)
      any type with at least a close : unit method
    
          ([< `red|`green] as 'a)
      any variant with some subset of red and green as options
    
          ([> `red|`green] as 'a)
      any variant with some superset of red and green as options
    
  


You can omit the as 'a bits, unless you need to refer to 'a somewhere else.

Summary

Java uses subtyping by default (automatically), with extra syntax for using generics (polymorphism). OCaml uses polymorphism by default, with extra syntax for using subtypes. Polymorphism is powerful, but can be confusing. If you allow OCaml to infer types, it will infer polymorphic types automatically and everything should work, but you’ll want to understand polymorphism when you come to writing module signatures and you need to write out the types.

When the compiler and the interactive OCaml interpreter display polymorphic types, they frequently omit the type variables, which can make learning OCaml more difficult. Polymorphic object types, class types and variants all use different syntax to indicate polymorphism, which can also be confusing.

OCaml uses “monomorphic type” to mean a single (non-polymorphic) type which has not yet been inferred. Monomorphic types occur when you create mutable state or call functions (which may create mutable state internally). This explains why partially applying a function loses its polymorphism.

OCaml does not allow functions that take polymorphic arguments (arguments that remain polymorphic within the function, rather than being resolved to a particular concrete type when the function is called). However, you can work around this using record or object types.

When defining a module’s signature (its external API), you can’t change concrete types but you can expose less polymorphism in polymorphic types if you want.
Hide
        
      
                    by Thomas Leonard at Dec 20, 2013 
      
      
    
  


       
                  MirageOS 1.0.3 released; tutorial on building this website available
      (Mirage OS)
    
    
                                      We've had a lot of people trying out MirageOS since the 1.0 release last week, and so we've been steadily cutting point releases and new libraries to OPAM as they're done.
The most common build error by far has been people using outdated OPAM packages.  Do make sure that you have at least OPAM 1.1 installed, and that you've run opam update -u to get the latest package lists from the package repository.
MirageOS 1.0.3 improves
Xen configuration generation, cleans up HTTP support, and adds support for FAT
filesystems.  Here are some of the libraries we've released this week to go along with it:
mirage-www (update): the live website now runs on the 1.0 tools.  Explanation of how to build it in various configurations is available here.alcotest (new): a lightweight and colourful test framework built over oUnit.  The interface is simpler to facilitate writing tests quickly, and it formats test results nicely.mirage-block-xen.1.0.0 (new): is the stable release of the Xen Blkfront drive…Read more...      We've had a lot of people trying out MirageOS since the 1.0 release last week, and so we've been steadily cutting point releases and new libraries to OPAM as they're done.
The most common build error by far has been people using outdated OPAM packages.  Do make sure that you have at least OPAM 1.1 installed, and that you've run opam update -u to get the latest package lists from the package repository.
MirageOS 1.0.3 improves
Xen configuration generation, cleans up HTTP support, and adds support for FAT
filesystems.  Here are some of the libraries we've released this week to go along with it:
mirage-www (update): the live website now runs on the 1.0 tools.  Explanation of how to build it in various configurations is available here.alcotest (new): a lightweight and colourful test framework built over oUnit.  The interface is simpler to facilitate writing tests quickly, and it formats test results nicely.mirage-block-xen.1.0.0 (new): is the stable release of the Xen Blkfront driver for block devices.  The library supports both frontend and backend operation, but only the frontend is plumbed through to Mirage for now (although the backend can be manually configured).mirage-block-unix.1.2.0 (update): fixed some concurrency bugs and added support for buffered I/O to improve performance.fat-filesystem.0.10.0 (update): copies with more sector sizes, uses buffered I/O on Unix, and adds a KV_RO key/value interface as well as a more complicated filesystem one.mirage-fs-unix.1.0.0 (update): implements the KV_RO signature as a passthrough to a Unix filesystem.  This is convenient during development to avoid recompile cycles while changing data.mirage-xen.1.0.0 (update): removed several distracting-but-harmless linker warnings about code bloat.cohttp.0.9.14 (update): supports Server-Side Events via better channel flushing, has a complete set of HTTP codes autogenerated from httpstatus.es and exposes a platform-independent Cohttp_lwt module.cow.0.8.1 (update): switch to the Omd library for Markdown parsing, which is significantly more compatible with other parsers.ezjsonm.0.2.0 (new): a combinator library to parse, select and manipulate JSON structures.ezxmlm.1.0.0 (new): a combinator library to parse, select and transform XML tags and attributes.mirage-http-xen and mirage-http-unix provide the HTTP drivers on top of Cohttp for MirageOS. Although they are very similar at the moment, they will diverge as the Unix variant gains options to use kernel sockets instead of only the network stack.

We're making great progress on moving our personal homepages over to MirageOS.  The first two introductory wiki posts are also now available:
Building a hello world example takes you through the basic steps to build a Unix and Xen binary.Building the MirageOS website lets you build this website with several variants, demonstrating the Unix passthrough filesystem, the OCaml FAT filesystem library, and how to attach a network stack to your application.

As always, please feel free to report any issues via the bug tracker and ask questions on the mailing list.

   Hide
        
      
                    by Anil Madhavapeddy at Dec 19, 2013 
      
      
    
  


       
                  MirageOS 1.0: not just a hallucination!
      (Mirage OS)
    
    
                                      First: read the overview and
technical background behind the project.
When we started hacking on MirageOS back in 2009, it started off looking like a
conventional OS, except written in OCaml.   The monolithic
repository contained all the
libraries and boot code, and exposed a big OS module for applications to use.
We used this to do several fun tutorials at conferences
such as ICFP/CUFP and get early feedback.
As development continued though, we started to understand what it is we were
building: a "library operating system".  As the number of libraries grew,
putting everything into one repository just wasn't scaling, and it made it hard
to work with third-party code.  We spent some time developing tools to make
Mirage fit into the broader OCaml ecosystem.
Three key things have emerged from this effort:
OPAM, a source-based package manager for
 OCaml. It supports multiple simultaneous compiler installations, flexible
 package constraints, and a Git-friendly development workflow. …Read more...      First: read the overview and
technical background behind the project.
When we started hacking on MirageOS back in 2009, it started off looking like a
conventional OS, except written in OCaml.   The monolithic
repository contained all the
libraries and boot code, and exposed a big OS module for applications to use.
We used this to do several fun tutorials at conferences
such as ICFP/CUFP and get early feedback.
As development continued though, we started to understand what it is we were
building: a "library operating system".  As the number of libraries grew,
putting everything into one repository just wasn't scaling, and it made it hard
to work with third-party code.  We spent some time developing tools to make
Mirage fit into the broader OCaml ecosystem.
Three key things have emerged from this effort:
OPAM, a source-based package manager for
 OCaml. It supports multiple simultaneous compiler installations, flexible
 package constraints, and a Git-friendly development workflow.  Since
 releasing 1.0 in March 2013 and 1.1 in October, the community has leapt
 in to contribute over 1800 packages in this short time.  All of the 
 Mirage libraries are now tracked using it, including the Xen libraries.The build system for embedded programming (such as the Xen target) is
 a difficult one to get right.  After several experiments, Mirage provides
 a single command-line tool that
 combines configuration directives (also written in OCaml) with OPAM to
 make building Xen unikernels as easy as Unix binaries.All of the Mirage-compatible libraries satisfy a set of module type
 signatures in a single file.
 This is where Mirage lives up to its name: we've gone from the early
 monolithic repository to a single, standalone interface file that
 describes the interfaces.  Of course, we also have libraries to go along
 with this signature, and they all live in the MirageOS GitHub organization.

With these components, I'm excited to announce that MirageOS 1.0 is finally ready
to see the light of day!  Since it consists of so many libraries, we've decided
not to have a "big bang" release where we dump fifty complex libraries on the
open-source community.  Instead, we're going to spend the month of December
writing a series of blog posts that explain how the core components work,
leading up to several use cases:
The development team have all decided to shift our personal homepages to be Mirage
 kernels running on Xen as a little Christmas present to ourselves, so we'll work through that step-by-step how to build 
 a dedicated unikernel and maintain and deploy it (spoiler: see this repo).  This will culminate in
 a webservice that our colleagues at Horizon have been
 building using Android apps and an HTTP backend.The XenServer crew at Citrix are using Mirage to build custom middlebox VMs
 such as block device caches.For teaching purposes, the Cambridge Computer Lab team want a JavaScript backend,
 so we'll explain how to port Mirage to this target (which is rather different
 from either Unix or Xen, and serves to illustrate the portability of our approach).

How to get involved

Bear with us while we update all the documentation and start the blog posts off
today (the final libraries for the 1.0 release are all being merged into OPAM
while I write this, and the usually excellent Travis continuous integration system is down due to a bug on their side).  I'll edit this post to contain links to the future posts
as they happen.
Since we're now also a proud Xen and Linux Foundation incubator project, our mailing
list is shifting to mirageos-devel@lists.xenproject.org, and we very much
welcome comments and feedback on our efforts over there.
The #mirage channel on FreeNode IRC is also growing increasingly popular, as
is simply reporting issues on the main Mirage GitHub repository.
Several people have also commented that they want to learn OCaml properly to
start using Mirage.  I've just co-published an O'Reilly book called
Real World OCaml that's available for free online
and also as hardcopy/ebook.  Our Cambridge colleague John Whittington has
also written an excellent introductory text, and
you can generally find more resources online.
Feel free to ask beginner OCaml questions on our mailing lists and we'll help
as best we can!

   Hide
        
      
                    by Anil Madhavapeddy at Dec 09, 2013 
      
      
    
  


       
                  Asynchronous Python vs OCaml
      (Thomas Leonard)
    
    
                                I’ve now migrated the asynchronous download logic in 0install from Python to OCaml + Lwt.
This post records my experiences using Lwt, plus some comparisons with Python’s coroutines.
As usual, the examples will be based on the real-world case of 0install, rather than on idealised text-book examples.



Table of Contents

  The problem
  Solutions          Callbacks
      Promises
      OCaml Lwt
      Python generators
    
  
  Examples          Following a recipe
      Downloading with libcurl
      Error handling in key look-ups
      Failing over to a mirror
    
  
  Switches
  Parallel tasks
  Conclusions


The problem

What happens when you download a program using 0install? To make this concrete, let’s look at the downloads that happen when you enter the command:

$ 0launch http://simamo.de/0install/armagetronad.xml


to make this happen:



If the software is cached, we run immediately. If not, we need to download some things first. The steps are (you don’t need to r…Read more...I’ve now migrated the asynchronous download logic in 0install from Python to OCaml + Lwt.
This post records my experiences using Lwt, plus some comparisons with Python’s coroutines.
As usual, the examples will be based on the real-world case of 0install, rather than on idealised text-book examples.



Table of Contents

  The problem
  Solutions          Callbacks
      Promises
      OCaml Lwt
      Python generators
    
  
  Examples          Following a recipe
      Downloading with libcurl
      Error handling in key look-ups
      Failing over to a mirror
    
  
  Switches
  Parallel tasks
  Conclusions


The problem

What happens when you download a program using 0install? To make this concrete, let’s look at the downloads that happen when you enter the command:

$ 0launch http://simamo.de/0install/armagetronad.xml


to make this happen:



If the software is cached, we run immediately. If not, we need to download some things first. The steps are (you don’t need to remember this!):

  Download http://simamo.de/0install/armagetronad.xml. This feed just points us at various sub-feeds that say where to get implementations for each platform.
  I’m on Linux (64 bit), so on my computer we next download (concurrently) the four suggested sub-feeds:
          http://simamo.de/0install/armagetronad-experimental-Linux-x86_64.xml
      http://simamo.de/0install/armagetronad-alpha-Linux-x86_64.xml
      http://simamo.de/0install/armagetronad-beta-Linux-x86_64.xml
      http://simamo.de/0install/armagetronad-stable-Linux-x86_64.xml
    
  
  Once we have these, we find a dependency on the library http://simamo.de/0install/armagetronad-libs-Linux-x86_64.xml, so we download that XML file too.
  Once all the XML downloads have finished, we select a version of Armagetron (0.2.8.3.2 in my case) and its (single) declared library (version 0.3-pre0.1570 here).
  We download these two tar.bz2 files (concurrently) and unpack them to the cache.
  Finally, we run (as described in previous posts).


That’s the overall process. It’s not totally trivial, but in fact some of the steps are complex in themselves. For example, to download a single feed (XML) file:

  We download the feed from the given URL.
  We check the GPG signature on the feed. If we don’t have the GPG key, we must download that next.
  Once we’ve checked that the signature is valid, we need to decide whether to trust it. We download information from the key information server.
  Depending on the user’s configuration and the key information server’s response, we may show a confirmation dialog to the user.


In addition:

  If the primary site (simamo.de) fails, we try the mirror site http://roscidus.com/0mirror/ instead.
  If the primary site is slow, we ask the mirror too. If the primary then succeeds, we cancel the mirror download. If the mirror succeeds first, we use the information we got from it to find more required downloads, but also continue waiting for the primary (which may have more up-to-date information).
  If the key information server is slow, we display the dialog to the user anyway, but update the display if the information arrives while the user is pondering.
  We never use more than 2 HTTP connections per site at the same time. Further requests are queued.


In other words, there’s quite a bit of logic here (and there’s still the archive downloads too…). How can we make sure all these operations happen at the right time and that errors are handled correctly?

Solutions

Note that the challenge here is not to use multiple CPUs in parallel to perform some calculation faster, but to schedule and manage multiple concurrent operations. The effects of concurrency will be visible (i.e. the behaviour of the code, such as whether we decide to contact the mirror server or not, depends on how quickly things happen). Therefore, some non-determinism is unavoidable. However, we want to minimise it.

Most languages provide some kind of low-level preemptive multi-threading support, e.g. Python’s threading.create, Haskell’s forkIO, OCaml’s Thread.create, Java’s java.lang.Thread and Go’s go. In these cases, all threads always run in parallel by default. If two threads access a shared or global variable without appropriate locking, the program will occasionally fail in ways that are hard to reproduce or diagnose.

Of course, these languages provide mutexes, channels, etc to make correct code possible, but this style is unsafe by default. For example, if a multi-threaded program uses some library from multiple threads, and the author of the library was only thinking about single-threaded use, then you likely have a subtle, hard-to-trigger bug.

Let’s consider a simplified example: we want to fetch information from the key information server, parse it, and confirm the key with the user (this server says things like “This key belongs to a registered Debian developer”).
Within each thread, we might do something like this:

1
2
3
4
5
def confirm_key(feed):
	data = download_key_info(feed.sig)
	info = parse_key_info(data)
	ok = confirm_with_gui(feed, info)
	if ok: ...


Probably this code will crash if two keys are downloaded close together, because the graphical toolkit library used to show the GUI isn’t thread-safe. But there could be similar issues with any code we call (is parse_key_info thread-safe, for example? What about the XML parser it uses? etc).

So how can we avoid these problems? Rust uses its linear types to prevent concurrent access to mutable state, which looks very useful. For other languages, we can use cooperative multi-threading.

The idea here is that instead of running threads in parallel by default and remembering to add locks wherever necessary, we run only one thread at a time, switching between threads only at explicitly marked points.

The two schemes have different failure modes. If you forget the locking in preemptive code, you get subtle bugs. If you forget to allow task switching in a cooperative system, the program may run slower (waiting when it could be getting on with something). For an application like 0install, cooperative makes far more sense. Just making downloads and GUI interaction alone concurrent is really all we need.

Callbacks

The simplest scheme to implement uses callbacks. You tell the system to start an operation, and give it a function to call on success:

1
2
3
4
5
6
7
8
9
def confirm_key(feed):
	download_key_info(feed.sig, key_info_downloaded)
def key_info_downloaded(data):
	info = parse_key_info(data)
	confirm_with_gui(feed, info, trust_confirmed)
def trust_confirmed(ok):
	if ok: ...


Here, we don’t know what other functions may be called in the time between us calling download_key_info and the key_info_downloaded callback, but while our code is executing we know that we have complete control. For example, it’s not a problem if parse_key_info here only supports single threading.

Callbacks have two major problems:

  They make the code messy and hard to read.
  They handle exceptions poorly.


Imagine that download_key_info has succeeded. It calls the key_info_downloaded callback. That calls parse_key_info, which throws an exception. The exception gets returned to download_key_info which can’t do anything useful with it. Probably, it gets logged and the program hangs, waiting for a call to trust_confirmed that will never happen.

Promises

Promises are a nice alternative to callbacks. When you start an operation, you get a promise for the result. A promise is a place-holder for a result that will arrive in the future. Without any special syntax, using promises might look something like this:

1
2
3
4
5
6
7
8
9
10
11
def confirm_key(feed):
	data_promise = download_key_info(feed.sig)
	return data_promise.when_fulfilled(key_info_downloaded)
def key_info_downloaded(data):
	info = parse_key_info(data)
	confirmation_promise = confirm_with_gui(feed, info)
	return confirmation_promise.when_fulfilled(trust_confirmed)
def trust_confirmed(ok):
	if ok: ...


The function promise.when_fulfilled(callback) immediately returns a new promise for the (future) result of the callback.

Internally, a promise initially contains a queue for callbacks. When the promise is (eventually) resolved to a value, the callbacks are all run and the queue is replaced by the value. Attempting to attach any further callbacks just runs them immediately on the value.

Promises have a number of advantages over callbacks. For example, you can store promises of results in lists, pass them to other functions, etc. One particular advantage is exception handling. Consider our previous example:

  confirm_key returns a promise for the result of key_info_downloaded.
  download_key_info downloads the data successfully, fulfilling data_promise.
  key_info_downloaded is called (it was attached to data_promise as a callback).
  parse_key_info throws an exception, which is caught by the promise system.
  This “breaks” the promise returned by confirm_key.
  Whoever was waiting for confirm_key gets notified of the exception.


Notice that instead of propagating uncaught exceptions backwards (to download_key_info), we propagate them forwards (to whoever is waiting for the result). The result is that, as in synchronous programming, an exception is not lost just because someone in the chain doesn’t handle it.

A natural next step is to introduce some simpler syntax for this…

OCaml Lwt

OCaml provides a couple of libraries for handling promises - Lwt and Jane Street’s Async. I’ve only looked at Lwt, although they seem fairly similar.

The terminology I introduced above I learnt from E (which also has sophisticated distributed promises). I find the E terms more natural, but here’s a conversion table:

            E term
      Lwt term
    
  
            Promise
      Thread
    
          Fulfilled promise
      Returned thread
    
          Broken promise
      Failed thread
    
          Unresolved promise
      Sleeping thread
    
          Resolver
      Waker
    
  


In particular, while a Lwt thread is still working to produce a result, the thread is said to be “sleeping”, which I find rather awkward. A resolver/waker is the object used by the maker of the promise to resolve it.

Anyway, switching to OCaml and using Lwt without the syntax extensions, we get this:

1
2
3
4
5
6
7
8
9
let confirm_key feed =
  let data_promise = download_key_info feed.signature in
  Lwt.bind data_promise (fun data ->
    let info = parse_key_info data in
    let confirmation_promise = confirm_with_gui feed info in
    Lwt.bind confirmation_promise (fun ok ->
      if ok then ...
    )
  )


Here, Lwt.bind promise callback is like our previous promise.when_fulfilled(callback). Again, confirm_key returns a promise (thread) for the final result.

To make things more convenient, you can enable the Lwt syntax extension. This provides thread-aware alternatives to several built-in OCaml keywords:

1
2
3
4
5
let confirm_key feed =
  lwt data = download_key_info feed.signature in
  let info = parse_key_info data in
  lwt ok = confirm_with_gui feed info in
  if ok then ...


As if by magic, our asynchronous code now reads like the original synchronous code! lwt is the new way to do Lwt.bind, by analogy with the ordinary let construct. We just have to remember that we give up control between evaluating the right-hand side of the assignment (getting a thread/promise for the data) and assigning the actual data on the left-hand side. For example, another function might change a global variable while we’re waiting for the promise to resolve.

The other short-cuts are try_lwt, for_lwt, while_lwt and match_lwt, which do what you’d expect. As a bonus, try_lwt also adds a finally construct and for_lwt adds iteration over sequences - both are missing for the core OCaml language.

There are plenty of functions for combining or creating threads in various ways, including:

  let thread, waker = Lwt.wait () explicitly creates a promise and a resolver for it.
  Lwt.return value evaluates to a returned thread, which is useful if something needs a thread type but you already have the value.
  Lwt.choose threads waits until one of the given threads is ready.
  Lwt.join threads returns a single thread that returns when all of the given threads have returned.
  Lwt_list.map_s fn items applies fn to each item, waiting for the resulting thread to resolve before doing the next item.
  Lwt_list.map_p fn items as above, but runs all the threads in parallel.


Python generators

Python has an unusual solution to the problem, using its generator functions.

A generator is any function which contains a yield. Running such a function gets you an iterator. Each time you ask for a value from the iterator, the generator runs until the next yield to produce the result. It is suspended until the next call. Generators were originally just an easy way to produce sequences, for example:

1
2
3
4
5
6
7
8
def count():
  x = 0
  while True:
    yield x
    x += 1
for x in count():
  print(x)


However, this ability to suspend and resume functions is obviously useful for cooperative multi-threading too and, like many other people, I used them to create a such a system (back in 2004). The version used in the Python version of 0install was designed for Python 2.3, but since then Python has added many useful new features so I’ll describe the recent Tulip/asyncio system rather than my own, even though I haven’t actually used it much.

The idea is that every time you need to wait, you yield the promise (“future” in Python terminology) you’re waiting for. When it’s ready, the scheduler will resume your generator function with the result:

1
2
3
4
5
def confirm_key(feed):
	data = yield from download_key_info(feed.sig)
	info = parse_key_info(data)
	ok = yield from confirm_with_gui(feed, info)
	if ok: ...


Examples

Both systems (OCaml Lwt and Python generators) work very well in general. Here are some (slightly simplified) examples from 0install.

Following a recipe

Some downloads require collecting files from several places (e.g. an upstream tarball and some files to patch it with). We want to download the files in parallel, but execute the steps (e.g. unpacking downloads into the target directory) in series. My solution is that each download is a thread that performs the download and then returns a lazy thunk that applies it:

1
2
3
4
5
6
7
8
9
10
  (* Start all the downloads in parallel. *)
  let downloads = steps |> List.map do_step in
  (* Now iterate over the steps in series. *)
  downloads |> Lwt_list.iter_s (fun unpack ->
    (* Wait for download *)
    lwt unpack = unpack in
    (* Apply download to directory *)
    Lazy.force unpack
  )


Note that we start unpacking as soon as possible; we only wait when the next thing to unpack isn’t downloaded yet.

This was my first attempt at a Python version with asyncio:

1
2
3
4
5
6
7
8
9
10
# Start all downloads in parallel
downloads = [do_step(step) for step in steps]
# Wait for all downloads to complete
tasks, _ = yield from asyncio.wait(downloads)
# Unpack each download in sequence
for task in tasks:
	unpack = task.result()
	yield from unpack()


An interesting difference is that OCaml threads, once started, continue to run by themselves even if no-one is waiting for the result. When the OCaml code is waiting for the first download to complete, the other downloads are still going on. But if we yield from just the first download in Python, only that download makes progress. In the code above, therefore, the Python waits for all downloads to complete before it starts unpacking.

You can fix this by wrapping the future with async:

1
2
3
4
5
6
7
8
9
# Start all downloads in parallel
downloads = [asyncio.async(do_step(step)) for step in steps]
# Now iterate over the steps in series.
for d in downloads:
	# Wait for download
	unpack = yield from d
	# Apply download to directory
	yield from unpack()


Downloading with libcurl

libcurl doesn’t provide Lwt support. However, it is thread-safe. We can therefore use the Lwt_preemptive module to run each download in a real operating system thread and get a promise for its completion. In addition, we use a Lwt_pool to keep up to two Curl connections per site (queuing further requests).

When it’s our turn to run, we also start a five second timer if the caller wanted to be notified if the download is slow. This is used when downloading the small XML metadata files so the mirror can be tried in parallel (for archives, we only try the mirror if the download actually fails).

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
let make_site max_per_site =
  let create_connection () = Lwt.return (Curl.init ()) in
  let pool = Lwt_pool.create max_per_site create_connection in
  object
    method schedule_download ?if_slow channel url =
      Lwt_pool.use pool (fun connection ->
	let timeout = if_slow |> pipe_some (fun if_slow ->
	  let timeout = Lwt_timeout.create 5 if_slow in
	  Lwt_timeout.start timeout;
	  Some timeout;
	) in
	let download () =
	  download_in_thread connection channel url in
	try_lwt
	  Lwt_preemptive.detach download ()
	finally
	  timeout |> if_some Lwt_timeout.stop;
	  Lwt.return ()
    end


The download_in_thread function also needs to send progress notifications to back to Lwt, which it does using Lwt_preemptive.run_in_main.

Update: note that recent versions of ocurl support Lwt directly.

Python provides the ThreadPoolExecutor, which combines pooling and preemptive threading. This makes it a bit harder to start the timer (which should happen cooperatively), so we need to use call_soon_threadsafe, which is like Lwt’s run_in_main. Python doesn’t seem to provide a way to manage the HTTP connections with the pool - I guess you have to do that manually.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
class Site:
    def __init__(self, max_per_site=2):
        self.pool = futures.ThreadPoolExecutor(max_per_site)
    def schedule_download(self, channel, url, if_slow = None):
        thread_ready = asyncio.Future()
        def timer():
            # Wait for an executor to be ready...
            yield from thread_ready
            # Wait until 5 seconds into the download
            yield from asyncio.sleep(5)
            # Notify that the download is slow
            if_slow()
        def run_in_thread():
            connection = ...
            if if_slow:
                loop.call_soon_threadsafe(thread_ready.set_result,
					  True)
            download_in_thread(connection, channel, url)
        download = loop.run_in_executor(self.pool, run_in_thread)
        t = asyncio.async(timer())
	try:
	    yield from download
	finally:
	    t.cancel()


Error handling in key look-ups

Lwt does have a gotcha for error handling. Consider this code:

1
2
3
4
5
6
7
8
9
let confirm_key feed =
  lwt data =
    try
      download_key_info feed.signature
    with Failure msg ->
      log_warning "Failed to download key info: %s" msg;
      Lwt.return []
  in
  let info = parse_key_info data in


If querying the key info server fails, we want to log the error but continue with the confirmation, just with an empty list of hints.

Something I really dislike is code that looks right, compiles without warnings, works when you test it, and then fails in the field. Unfortunately, this code does just that. Even if you unit-test the error case!

The bug occurs because we accidentally used try rather than try_lwt. download_key_info successfully returns a promise for the information, so the with clause isn’t triggered and we exit the try block. Then Lwt waits for the promise to resolve so it can set data.
When unit-testing, you’ll probably raise the test exception immediately and so the with block does get called.

By contrast, Python’s generators have no such problems:

1
2
3
4
5
6
7
8
9
def confirm_key(feed):
	try:
		data = yield from download_key_info(feed.sig)
	except Exception as ex:
		logging.warning("Failed to download key info: %s", ex)
		data = []
	info = parse_key_info(data)
	ok = yield from confirm_with_gui(feed, info)
	if ok: ...


The other Lwt constructs don’t have this problem because the type-system will detect the error (e.g. if you use match instead of match_lwt), but with try and try_lwt the type signatures are the same.

Failing over to a mirror

We start downloading each XML feed from its primary site, but trigger a timeout task if it takes too long.
The timeout starts a download from the mirror, which happens in parallel with the original download attempt.
We don’t want to start the timer immediately because the download might get queued due to the rate limiting code, so we just pass the if_slow trigger to the download system (see above).

Because we need to report intermediate results (e.g. we have downloaded a possibly-slightly-old version from the mirror), we return a pair of the new result and a promise for the next update (or None if this is the last). In a similar way, we return errors as a pair of the current error (e.g. “mirror failed”) and a promise for the other result.

Lwt.choose selects the result of the first task from a list to resolve. When choosing between the primary and the mirror however we ignore the result and test explicitly, because we need to know which one it was.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
let timeout_task, timeout_waker = Lwt.wait () in
let if_slow () = Lwt.wakeup timeout_waker `timeout in
let primary = do_primary_download ~if_slow feed in
(* Download just the upstream feed, unless it takes too long... *)
match_lwt Lwt.choose [primary; timeout_task] with
| `ok result ->
    `update (result, None) |> Lwt.return
| `problem msg ->
    let mirror = do_mirror_download () in
    `problem (msg, Some (wait_for_mirror mirror)) |> Lwt.return
| `timeout ->
    (* OK, maybe it's just being slow... *)
    log_info "Feed download from %s is taking a long time." feed_url;
    (* Start downloading from mirror... *)
    let mirror = do_mirror_download () in
    (* Wait for a result from either *)
    lwt _ = Lwt.choose [primary; mirror] in
    match Lwt.state primary with
    | Lwt.Fail msg -> raise msg
    | Lwt.Sleep ->
	(* The mirror finished first *)
	begin match_lwt mirror with
	| `ok result ->
	    log_info "Mirror succeeded, but will continue to wait for primary";
	    `update (result, Some (wait_for_primary primary)) |> Lwt.return
	| `problem msg ->
	    log_info "Mirror download failed: %s" msg;
	    wait_for_primary primary end
    | Lwt.Return v ->
	(* The primary returned first *)
	match v with
	| `ok result ->
	    Lwt.cancel mirror;
	    `update (result, None) > Lwt.return
	| `problem msg ->
	    `problem (msg, Some (wait_for_mirror mirror)) |> Lwt.return


I’m too lazy to translate this into modern Python, but I think it’s clear that a direct translation would be easy enough.

The main problem would be losing OCaml’s type checking, which ensures that we handle all the possible error conditions. I simplified the outcomes into just ok and problem above, but in the real code we also distinguish replay_attack, aborted_by_user and no_trusted_keys, and handle them differently. For example, a “replay attack” from the mirror (where the mirror gives us a version older than one we’ve already seen) is ignored, whereas it’s reported if it comes from the primary.

Switches

The Lwt_switch module provides a way to group a set of activities together so you can stop them all at once.
You create a switch and pass it to all the various setup functions you call.
When you’re done, call Lwt_switch.turn_off to kill everything.
For example, each download goes to a temporary file. To ensure they’re all deleted afterwards:

1
2
3
4
5
6
let switch = Lwt_switch.create () in
try_lwt
  let tmpfile = download ~switch url in
  ...
finally
  Lwt_switch.turn_off switch


It’s easy to attach whatever finalisation code you want to a switch, e.g.

1
2
3
4
5
6
7
let download ~switch url =
  let tmpfile = make_temp_file () in
  Lwt_switch.add_hook
    (Some switch)
    (fun () -> Unix.unlink tmpfile; Lwt.return ());
  ...
  Lwt.return tmpfile


Then it doesn’t matter whether we download successfully, raise an exception inside of download, raise an exception after calling download, etc; the file always gets deleted.

Perhaps this is bad API design and I shouldn’t rely on download’s caller to clean up the file if download fails, but it does seem convenient. To avoid mistakes, I used ~switch to force the caller to pass a switch instead of the more normal ?switch (where use of a switch is optional).

Parallel tasks

Regular OCaml lets you assign multiple variables at once using and, so that all the expressions are evaluated in a context where none of them is bound. For example, to switch the names of two variables:

1
2
3
let x = y
and y = x in
...


Lwt uses this syntax with lwt to create multiple tasks in parallel and then wait for all of them. For example, when we run a command we may want to collect the standard output and standard error separately but in parallel (if we did them in series, the process might get stuck trying to write its stderr while we were trying to read its stdout, if the Unix pipe gets full). With this syntax, we can get the two strings with just:

1
2
3
lwt stdout = Lwt_io.read child#stdout
and stderr = Lwt_io.read child#stderr in
...


Conclusions

0install needs to manage several fairly complex concurrent download activities, including error handling, timeouts and mirrors. Cooperative multi-threading allows us to support this easily with a low risk of race conditions.

Python and OCaml both provide powerful and easy-to-use cooperative threading support. I think Python’s generators are slightly easier to understand for beginners, but I find both quite easy to use. I find Lwt’s terminology a little confusing, but thinking of threads as promises seems to help. Both systems handle exceptions sensibly.

Comparing Python and OCaml code, they’re pretty similar. Both make it easy to start and manage cooperative threads, to interact with pools of preemptively threaded code (e.g. libcurl) and to handle errors.
Using OCaml variants for network errors rather than exceptions is useful; this ensures that all such errors are handled. If you rely on exceptions instead then things mostly work, but watch out for using try rather than try_lwt.

The old 0install Python code used a custom system built on top of Python’s generators, but Python’s new asyncio module provides a standardised replacement (asyncio will be added to the standard library in Python 3.4). Lwt has been around for a while and is already available from Linux distribution repositories.

Lwt also integrates with several other libraries, including GTK, OBus (D-BUS bindings) and React. Lwt seems very reliable. The only bug I found in Lwt so far was a pipe read failure on Windows, which they quickly fixed.
Hide
        
      
                    by Thomas Leonard at Nov 28, 2013 
      
      
    
  


       
                  Switching from Bootstrap to Zurb Foundation
      (Amir Chaudhry)
    
    
                                I’ve just updated my site’s HTML/CSS and moved from Twitter Bootstrap to 
Zurb Foundation.  This post captures my subjective notes on the 
migration.

My use of Bootstrap

When I originally set this site up, I didn’t know what frameworks existed or 
anything more than the basics of dealing with HTML (and barely any CSS).  I 
came across Twitter Bootstrap and immediately decided it would Solve All My 
Problems.  It really did.  Since then, I’ve gone through one ‘upgrade’ with 
Bootstrap (from 1.x to 2.x), after which I dutifully ignored all the fixes 
and improvements (note that Bootstrap was up to v2.3.2 while I was still 
using v2.0.2).  



For the most part, this was fine with me but for a while now, I’ve been 
meaning to make this site ‘responsive’ (read: not look like crap from a 
mobile).  Bootstrap v3 purports to be mobile-first so upgrading would likely 
give me what I’m after but v3 is not backwards compatible, 
meaning I’d have to rewrite parts of the H…Read more...I’ve just updated my site’s HTML/CSS and moved from Twitter Bootstrap to 
Zurb Foundation.  This post captures my subjective notes on the 
migration.

My use of Bootstrap

When I originally set this site up, I didn’t know what frameworks existed or 
anything more than the basics of dealing with HTML (and barely any CSS).  I 
came across Twitter Bootstrap and immediately decided it would Solve All My 
Problems.  It really did.  Since then, I’ve gone through one ‘upgrade’ with 
Bootstrap (from 1.x to 2.x), after which I dutifully ignored all the fixes 
and improvements (note that Bootstrap was up to v2.3.2 while I was still 
using v2.0.2).  



For the most part, this was fine with me but for a while now, I’ve been 
meaning to make this site ‘responsive’ (read: not look like crap from a 
mobile).  Bootstrap v3 purports to be mobile-first so upgrading would likely 
give me what I’m after but v3 is not backwards compatible, 
meaning I’d have to rewrite parts of the HTML.  Since this step was 
unavoidable, it led me to have another look at front-end frameworks, just to 
see if I was missing anything.  This was especially relevant since we’d 
just released the new OCaml.org 
website, itself built with Bootstrap v2.3.1 (we’d done the design/templating 
work long before v3 was released).  It would be useful to know what else is 
out there for any future work.

Around this time I discovered Zurb Foundation and also the numerous 
comparisons between them (note: Foundation seems to come out ahead in most 
of those).  A few days ago, the folks at Zurb released 
version 5, so I decided that now is the time to kick the 
tires.  For the last few days, I’ve been playing with the framework and in 
the end I decided to migrate my site over completely.  



Swapping out one framework for another

Over time, I’ve become moderately experienced with HTML/CSS and I can 
usually wrangle things to look the way I want, but my solutions aren’t 
necessarily elegant. I was initially concerned that I’d already munged 
things so much that changing anything would be a pain.  When I first put the 
styles for this site together, I had to spend quite a bit of time 
overwriting Bootstrap’s defaults so I was prepared for the same when using 
Foundation.  Turns out that I was fine.  I currently use Jekyll (and 
Jekyll Bootstrap) so I only had three template files and a couple of 
HTML pages to edit and because I’d kept most of my custom CSS in a separate 
file, it was literally a case of swapping out one framework for another and 
bug-fixing from there onwards.  There’s definitely a lesson here in using 
automation as much as possible.

Customising the styles was another area of concern but I was pleasantly 
surprised to find I needed less customisation than with Bootstrap.  This 
is likely because I didn’t have to override as many defaults (and probably 
because I’ve learned more about CSS since then).  The one thing I seemed to 
be missing was a way to deal with code sections, so I just took what 
Bootstrap had and copied it in.  At some point I should revisit this.

It did take me a while to get my head around Foundation’s grid but it was 
worth it in the end.  The idea is that you should design for small screens 
first and then adjust things for larger screens as necessary. There are 
several different default sizes which inherit their properties from the size 
below, unless you explicitly override them.  I initially screwed this up by 
explicitly defining the grid using the small-# classes, which obviously 
looks ridiculous on small screens.  I fixed it by swapping out small-# for 
medium-# everywhere in the HTML, after which everything looked reasonable. 
Items flowed sensibly into a default column for the small screens and looked 
acceptable for larger screens and perfectly fine on desktops.  I could do 
more styling of the mobile view but I’d already achieved most of what I was 
after.  

Fixing image galleries and embedded content

The only additional thing I used from Bootstrap was the Carousel. I’d 
written some custom helper scripts that would take some images and 
thumbnails from a specified folder and produce clickable thumbnails with a 
slider underneath.  Foundation provides Orbit, so I had to spend time 
rewriting my script to produce the necessary HTML.  This actually resulted 
in cleaner HTML and one of the features I wanted (the ability to link to a 
specific image) was available by default in Orbit.  At this point I also 
tried to make the output look better for the case where JavaScript is 
disabled (in essence, each image is just displayed as a list).  Below is an 
example of an image gallery, taken from a previous post, when I 
joined the computer lab.

  Note: The gallery needs JavaScript but I've tried to make it degrade gracefully. -Amir
      
      
    
      
    
      
    
  
      
    
    
    
    
    
    
  


Foundation also provides a component called Flex Video, which allows the 
browser to scale videos to the appropriate size.  This fix was as simple as 
going back through old posts and wrapping anything that was <iframe> in a 
<div class="flex-video">.  It really was that simple and all the Vimeo and 
YouTube items scaled perfectly.  Here’s an example of a video from an 
earlier post, where I gave a walkthrough of the ocaml.org site. 
Try changing the width of your browser window to see it scale.

  Video demo


Framework differences

Another of the main difference between the two frameworks is that Bootstrap 
uses LESS to manage its CSS whereas Foundation uses SASS.  Frankly, 
I’ve no experience with either of them so it makes little difference to me. 
It’s worth bearing in mind for anyone who’s workflow does involve 
pre-processing.  Also, Bootstrap is available under the 
Apache 2 License, while Foundation is released under 
the MIT license.

Summary

Overall, the transition was pretty painless and most of the time was spent 
getting familiar with the grid, hunting for docs/examples and trying to make 
the image gallery work the way I wanted.  I do think Bootstrap’s docs are 
better but Foundation’s aren’t bad.  

Although this isn’t meant to be a comparison, I much prefer Foundation to 
Bootstrap.  If you’re not sure which to use then I think the secret is in 
the names of the frameworks.  

  Bootstrap (for me) was a great way to ‘bootstrap’ a site quickly with 
lots of acceptable defaults – it was quick to get started but took some 
work to alter.  
  Foundation seems to provide a great ‘foundation’ on which to create more 
customised sites – it’s more flexible but needs more upfront thought.  


That’s pretty much how I’d recommend them to people now.

Hide
        
      
                    by Amir Chaudhry at Nov 26, 2013 
      
      
    
  


       
                  Announcing the new OCaml.org
      (Amir Chaudhry)
    
    
                                As some of you may have noticed, the new OCaml.org site is now live!  

The DNS may still be propagating so if http://ocaml.org hasn’t updated for you then try http://166.78.252.20.  This post is in two parts: the first is the announcement and the second is a call for content.

New OCaml.org website design!

The new site represents a major milestone in the continuing growth of the OCaml ecosystem. It’s the culmination of a lot of volunteer work over the last several months and I’d specifically like to thank Christophe, Ashish and Philippe for their dedication (the commit logs speak volumes).  



We began this journey just over 8 months ago with paper, pencils and a lot of ideas. This led to a comprehensive set of wireframes and walk-throughs of the site, which then developed into a collection of Photoshop mockups. In turn, these formed the basis for the html templates and style sheets, which we’ve adapted to fit our needs across the site.  

Alongside the design process, we a…Read more...As some of you may have noticed, the new OCaml.org site is now live!  

The DNS may still be propagating so if http://ocaml.org hasn’t updated for you then try http://166.78.252.20.  This post is in two parts: the first is the announcement and the second is a call for content.

New OCaml.org website design!

The new site represents a major milestone in the continuing growth of the OCaml ecosystem. It’s the culmination of a lot of volunteer work over the last several months and I’d specifically like to thank Christophe, Ashish and Philippe for their dedication (the commit logs speak volumes).  



We began this journey just over 8 months ago with paper, pencils and a lot of ideas. This led to a comprehensive set of wireframes and walk-throughs of the site, which then developed into a collection of Photoshop mockups. In turn, these formed the basis for the html templates and style sheets, which we’ve adapted to fit our needs across the site.  

Alongside the design process, we also considered the kind of structure and workflow we aspired to, both as maintainers and contributors.  This led us to develop completely new tools for Markdown and templating in OCaml, which are now available in OPAM for the benefit all.  

Working on all these things in parallel definitely had it challenges (which I’ll write about separately) but the result has been worth the effort.  



The journey is ongoing and we still have many more improvements we hope to make. The site you see today primarily improves upon the design, structure and workflows but in time, we also intend to incorporate more information on packages and documentation. With the new tooling, moving the website forward will become much easier and I hope that more members of the community become involved in the generation and curation of content.  This brings me to the second part of this post.

Call for content

We have lots of great content on the website but there are parts that could do with a refresh and gaps that could be filled.  As a community driven site, we need ongoing contributions to ensure that the site best reflects its members.  

For example, if you do commercial work on OCaml then maybe you’d like to add yourself to the support page? Perhaps there are tutorials you can help to complete, like 99 problems?  If you’re not sure where to begin, there are already a number of content issues you could contribute to.  

Although we’ve gone through a bug-hunt already, feedback on the site is still very welcome.  You can either create an issue on the tracker (preferred), or email the infrastructure list. 

It’s fantastic how far we’ve come and I look forward to the next phase!

Hide
        
      
                    by Amir Chaudhry at Nov 20, 2013 
      
      
    
  


       
                  Migration plan for the OCaml.org redesign
      (Amir Chaudhry)
    
    
                                We’re close to releasing the new design of ocaml.org but need help from the 
OCaml community to identify and fix bugs before we switch next week.

Ashish, Christophe, Philippe and I have been discussing how we should go 
about this and below is the plan for migration.  If anyone would like to 
discuss any of this, then the infrastructure list is the best 
place to do so.

      We’ve made a new branch on the main ocaml.org repository with 
the redesign.  This branch is a fork of the master and we’ve simply cleaned 
up and replayed our git commits there.
  
      We’ve built a live version of the new site, which is visible at 
http://preview.ocaml.org - this is rebuilt every few minutes 
from the branch mentioned above.  
  
      Over the course of one week, we ask the community to review the new site 
and report any bugs or problems on the issue tracker. We triage 
those bugs to identify any blockers and work on those first.  This is the 
phase we’ll be in from today.
 …Read more...We’re close to releasing the new design of ocaml.org but need help from the 
OCaml community to identify and fix bugs before we switch next week.

Ashish, Christophe, Philippe and I have been discussing how we should go 
about this and below is the plan for migration.  If anyone would like to 
discuss any of this, then the infrastructure list is the best 
place to do so.

      We’ve made a new branch on the main ocaml.org repository with 
the redesign.  This branch is a fork of the master and we’ve simply cleaned 
up and replayed our git commits there.
  
      We’ve built a live version of the new site, which is visible at 
http://preview.ocaml.org - this is rebuilt every few minutes 
from the branch mentioned above.  
  
      Over the course of one week, we ask the community to review the new site 
and report any bugs or problems on the issue tracker. We triage 
those bugs to identify any blockers and work on those first.  This is the 
phase we’ll be in from today.
  
      After one week (7 days), and after blocking bugs have been fixed, we 
merge the redesign branch into the master branch.  This would 
effectively present the new site to the world.  
  


During the above, we would not be able to accept any new pull requests on 
the master branch but would be happy to accept them on the new, redesign 
branch.  Hence, restricting the time frame to one week.  

Please note that the above is only intended to merge the design and 
toolchain for the new site.  Specifically, we’ve created new landing 
pages, have new style sheets and have restructured the site’s contents as 
well as made some new libraries (OMD and MPP). The new toolchain 
means people can write files in markdown, which makes contributing content a 
lot easier.  

Since the files are on GitHub, people don’t even need to clone the site 
locally to make simple edits (or even add new pages). Just click the ‘Edit 
this page’ link in the footer to be taken to the right file in the 
repository and GitHub’s editing and pull request features will allow you to 
make changes and submit updates, all from within your browser (see the 
GitHub Article for details).  

There is still work to be done on adding new features but the above changes 
are already a great improvement to the site and are ready to be reviewed by 
the OCaml community and merged.

Hide
        
      
                    by Amir Chaudhry at Nov 06, 2013 
      
      
    
  


       
                  Third OCaml compiler hacking session
      (Compiler Hacking)
    
    
                                It's time for the third Cambridge OCaml compiler-hacking session! This time we're going to be back in the Computer Lab, where the first session was held.

If you're planning to come along, it'd be helpful if you could indicate interest via Doodle and sign up to the mailing list to receive updates:

Where: Room FW11, Computer Laboratory, Madingley Road

When: 6pm, Wednesday 6th November

Who: anyone interested in improving OCaml. Knowledge of OCaml programming will obviously be helpful, but prior experience of working on OCaml internals isn't necessary.

What: fixing bugs, implementing new features, learning about OCaml internals

Wiki: https://github.com/ocamllabs/compiler-hacking/wiki

We're defining "compiler" pretty broadly, to include anything that's part of the standard distribution, which means at least the standard library, runtime, tools (ocamldep, ocamllex, ocamlyacc, etc.), camlp4, ocamlbuild, the documentation, and the compiler itself. We'll have suggestions for mini-project…Read more...It's time for the third Cambridge OCaml compiler-hacking session! This time we're going to be back in the Computer Lab, where the first session was held.

If you're planning to come along, it'd be helpful if you could indicate interest via Doodle and sign up to the mailing list to receive updates:

Where: Room FW11, Computer Laboratory, Madingley Road

When: 6pm, Wednesday 6th November

Who: anyone interested in improving OCaml. Knowledge of OCaml programming will obviously be helpful, but prior experience of working on OCaml internals isn't necessary.

What: fixing bugs, implementing new features, learning about OCaml internals

Wiki: https://github.com/ocamllabs/compiler-hacking/wiki

We're defining "compiler" pretty broadly, to include anything that's part of the standard distribution, which means at least the standard library, runtime, tools (ocamldep, ocamllex, ocamlyacc, etc.), camlp4, ocamlbuild, the documentation, and the compiler itself. We'll have suggestions for mini-projects for various levels of experience, but feel free to come along and work on whatever you fancy.

We'll also be ordering pizza, so if you want to be counted for food you should aim to arrive by 6.30pm.
Hide
        
      
                    by Compiler Hacking at Oct 30, 2013 
      
      
    
  


       
                  Review of the OCaml FPDays tutorial
      (Amir Chaudhry)
    
    
                                
Last Thursday a bunch of us from the OCaml Labs team gave an OCaml tutorial 
at the FPDays conference (an event for people interested in Functional 
Programming).  Jeremy and I led the session with Leo, David and 
Philippe helping everyone progress and dealing with questions.

It turned out to be by far the most popular session at the conference with 
over 20 people all wanting to get to grips with OCaml!  An excellent turnout 
and a great indicator of the interest that’s out there, especially when you 
offer a hands-on session to people.  This shouldn’t be a surprise as we’ve 
had good attendance for the general OCaml meetups I’ve run 
and also the compiler hacking sessions, which Jeremy and 
Leo have been building up (do sign up if you’re interested in either of 
those!).  We had a nice surprise for attendees, which were 
uncorrected proof copies of Real World OCaml and luckily, we had just 
enough to go around.

For the tutorial itself, Jeremy put together a nice sequen…Read more...
Last Thursday a bunch of us from the OCaml Labs team gave an OCaml tutorial 
at the FPDays conference (an event for people interested in Functional 
Programming).  Jeremy and I led the session with Leo, David and 
Philippe helping everyone progress and dealing with questions.

It turned out to be by far the most popular session at the conference with 
over 20 people all wanting to get to grips with OCaml!  An excellent turnout 
and a great indicator of the interest that’s out there, especially when you 
offer a hands-on session to people.  This shouldn’t be a surprise as we’ve 
had good attendance for the general OCaml meetups I’ve run 
and also the compiler hacking sessions, which Jeremy and 
Leo have been building up (do sign up if you’re interested in either of 
those!).  We had a nice surprise for attendees, which were 
uncorrected proof copies of Real World OCaml and luckily, we had just 
enough to go around.

For the tutorial itself, Jeremy put together a nice sequence of exercises 
and a skeleton repo (with helpful comments in the code) so that people 
could dive in quickly.  The event was set up to be really informal and the 
rough plan was as following:

      Installation/Intro - We checked that people had been able to follow the 
installation instructions, which we’d sent them in advance. 
We also handed out copies of the book and made sure folks were comfortable 
with OPAM.
  
      Hello world - A light intro to get people familiar with the OCaml 
syntax and installing packages with OPAM. This would also help people to get 
familiar with the toolchain, workflow and compilation.  
  
      Monty Hall browser game - Using js_of_ocaml, we wanted 
people to create and run the Monty Hall problem in their 
browser.  This would give people a taste of some real world interaction by 
having to deal with the DOM and interfaces.  If folks did well, they could 
add code to keep logs of the game results.
  
      Client-server game - The previous game was all in the browser (so could 
be examined by players) so here the task was to split it into a client and 
server, ensuring the two stay in sync.  This would demonstrate the 
re-usability of the OCaml code already written and give people a feel for 
client server interactions. If people wanted to do more, they could use 
ctypes and get better random numbers.  
  


We did manage to stick to the overall scheme as above and we think this is a 
great base from which to improve future tutorials.  It has the really nice 
benefit of having visual, interactive elements and the ability to run things 
both in the browser as well as on the server is a great way to show the 
versatility of OCaml.  js_of_ocaml is quite a mature tool and so it’s 
no surprise that it’s also used by companies such as Facebook (see the recent 
CUFP talk by Julien Verlaguet - skip to 19:00).  

We learned a lot from running this session so we’ve captured the good, the 
bad and the ugly below.  This is useful for anyone who’d like to run an 
OCaml tutorial in the future and also for us to be aware of the next 
time we do this.  I’ve incorporated the feedback from the attendees as well 
as our own thoughts.



Things we learnt

The Good

      Most people really did follow the install instructions beforehand. This 
made things so much easier on the day as we didn’t have to worry about 
compile times and people getting bored.  A few people had even got in touch 
with me the night before to sort out installation problems.  
  
      Many folks from OCaml Labs also came over to help people, which meant 
no-one was waiting longer than around 10 seconds before getting help.  
  
      We had a good plan of the things we wanted to cover but we were happy to 
be flexible and made it clear the aim was to get right into it.  Several 
folks told us that they really appreciated this loose (as opposed to rigid) 
structure.  
  
      We didn’t spend any time lecturing the room but instead got people right 
into the code.  Having enough of a skeleton to get something interesting 
working was a big plus in this regard. People did progress from the early 
examples to the later ones fairly well.
  
      We had a VM with the correct set up that we could log people into if they 
were having trouble locally.  Two people made use of this.
  
      Of course, It was great to have early proofs of the book and these were 
well-received.
  




The Bad

      In our excitement to get right into the exercises, we didn’t really give 
an overview of OCaml and its benefits.  A few minutes at the beginning would 
be enough and it’s important so that people can leave with a few sound-bites.
  
      Not everyone received my email about installation, and certainly not the 
late arrivals.  This meant some pain getting things downloaded and running 
especially due to the wifi (see ‘Ugly’ below).  
  
      A few of the people who had installed, didn’t complete the instructions 
fully but didn’t realise this until the morning of the session.  There was a good 
suggestion about having some kind of test to run that would check 
everything, so you’d know if there was something missing.
  
      We really should have had a cut-off where we told people to use VMs 
instead of fixing installation issues and 10-15 minutes would have been 
enough.  This would have been especially useful for the late-comers.
  
      We didn’t really keep a record of the problems folks were having so we 
can’t now go back and fix underlying issues.  To be fair, this would have 
been a little awkward to do ad-hoc but in hindsight, it’s a good thing to 
plan for.
  


The Ugly

  The only ugly part was the wifi.  It turned out that the room itself was a 
bit of a dead-spot and that wasn’t helped by 30ish devices trying to connect 
to one access point!  Having everyone grab packages at the same time in the 
morning probably didn’t help.  It was especially tricky as all our 
mitigation plans seemed to revolve around at least having local connectivity.
In any case, this problem only lasted for the morning session and was a 
little better by the afternoon.  I’d definitely recommend a backup plan in 
the case of complete wifi failure next time!  One such plan that Leo got 
started on was to put the repository and other information onto a flash 
drive that could be shared with people.  We didn’t need this in the end but 
it’ll be useful to have something like this prepared for next time.  If 
anyone fancies donating a bunch of flash drives, I’ll happily receive them!


Overall, it was a great session and everyone left happy, having completed 
most of the tutorial (and with a book!).  A few even continued at home 
afterwards and got in touch to let us know that they got 
everything working.
It was a great session and thanks to Mark, Jacqui and the rest of 
the FPDays crew for a great conference!



(Thanks to Jeremy, Leo, David and Philippe for contributions to this post)

Hide
        
      
                    by Amir Chaudhry at Oct 28, 2013 
      
      
    
  


       
                  FP Days OCaml Session
      (Amir Chaudhry)
    
    
                                On Thursday, along with Jeremy and 
Leo, I’ll be running an OCaml Hands-on Session at 
the FPDays conference. Below are some prep 
instructions for attendees.

Preparation for the session

If you’re starting from scratch, installation can take some time so it’s 
best to get as much done in advance as possible.  You’ll need OPAM (the 
package manager), OCaml 4.01 (available through OPAM) and a few libraries 
before Thursday.  If you have any issues, please contact Amir.

      OPAM: Follow the instructions for your platform at http://opam.ocaml.org/doc/Quick_Install.html. 
OPAM requires OCaml so hopefully the relevant dependencies will kick in and 
you’ll get OCaml too (most likely version 3.12).  You can get a cup of 
coffee while you wait. After installation, run opam init to initialise OPAM.
  
      OCaml 4.01: We actually need the latest version of OCaml but OPAM 
makes this easy.  Just run the following (and get more coffee):
  


$ opam update
$ opam switch 4.01.0
$…Read more...On Thursday, along with Jeremy and 
Leo, I’ll be running an OCaml Hands-on Session at 
the FPDays conference. Below are some prep 
instructions for attendees.

Preparation for the session

If you’re starting from scratch, installation can take some time so it’s 
best to get as much done in advance as possible.  You’ll need OPAM (the 
package manager), OCaml 4.01 (available through OPAM) and a few libraries 
before Thursday.  If you have any issues, please contact Amir.

      OPAM: Follow the instructions for your platform at http://opam.ocaml.org/doc/Quick_Install.html. 
OPAM requires OCaml so hopefully the relevant dependencies will kick in and 
you’ll get OCaml too (most likely version 3.12).  You can get a cup of 
coffee while you wait. After installation, run opam init to initialise OPAM.
  
      OCaml 4.01: We actually need the latest version of OCaml but OPAM 
makes this easy.  Just run the following (and get more coffee):
  


$ opam update
$ opam switch 4.01.0
$ eval `opam config env`

  Libraries: For the workshop you will need to check that you have the 
following installed: libffi, pcre and pkg-config.  This will depend on 
your platform so on a Mac with homebrew I would do 
brew install libffi pcre pkg-config or on Debian or Ubuntu 
apt-get install libffi-dev.  After this, two OCaml packages it’s worth 
installing in advance are core and js_of_ocaml so simply run:


$ opam install core js_of_ocaml

OPAM will take care of the dependencies and the rest we can get on the day!
Hide
        
      
                    by Amir Chaudhry at Oct 22, 2013 
      
      
    
  


       
                  OCaml tips
      (Thomas Leonard)
    
    
                                In today’s “thing’s I’ve learnt about OCaml” I look back at my first OCaml code, and think about how I’d write it differently now.



Table of Contents

  Removing ;;
  Warnings
  Exhaustive matching
  Handy operators
  Handling option types
  Conclusions


Removing ;;

Looking back at my code, the most obvious “this is beginner code” clue is the use of ;; everywhere. The OCaml tutorial gives a list of complicated rules for when to use ;;, but in fact it’s very simple:

  Never use top-level expressions in an OCaml program.
  Never use ;; (except when tracking down syntax errors).


If you want to run some code at startup (e.g. your “main” function), just put it inside a let () = ... block. That way you’ll also get a compile-time error if you miss an argument. I don’t know why OCaml even allows top-level expressions. e.g.

1
2
3
4
5
6
(* Bad - mistake goes undetected and you need ';;' *)
Printf.printf "Hello %s";;
(* Good - compiler spots missing argument *…Read more...In today’s “thing’s I’ve learnt about OCaml” I look back at my first OCaml code, and think about how I’d write it differently now.



Table of Contents

  Removing ;;
  Warnings
  Exhaustive matching
  Handy operators
  Handling option types
  Conclusions


Removing ;;

Looking back at my code, the most obvious “this is beginner code” clue is the use of ;; everywhere. The OCaml tutorial gives a list of complicated rules for when to use ;;, but in fact it’s very simple:

  Never use top-level expressions in an OCaml program.
  Never use ;; (except when tracking down syntax errors).


If you want to run some code at startup (e.g. your “main” function), just put it inside a let () = ... block. That way you’ll also get a compile-time error if you miss an argument. I don’t know why OCaml even allows top-level expressions. e.g.

1
2
3
4
5
6
(* Bad - mistake goes undetected and you need ';;' *)
Printf.printf "Hello %s";;
(* Good - compiler spots missing argument *)
let () =
  Printf.printf "Hello %s"


In a similar way, I was a bit over cautious about adding parenthesis around expressions. For example, I had Str.regexp ("...") and match (...) with. They’re not needed in most cases.

Warnings

Always compile with warnings on. I don’t know why this isn’t the default. Use -w A to enable all warnings.

I actually use -w A-4, which disables the warning when you use a default match case. Default match cases should be avoided when possible, but if you’ve gone to the trouble of adding one then you probably needed it.

Exhaustive matching

One of the great strengths of OCaml (which I missed at first) is that it always makes you handle every possible case. Providing a catch-all case defeats this check. In my initial code, I needed to process a list of bindings. First, all the environment bindings, then all the executable ones. I made a do_env_binding function which applied environment bindings and ignored all others:

1
2
3
let do_env_binding env impls = function
| EnvironmentBinding {var_name; mode; source} -> ...
| _ -> ()


I did the same for executable bindings. Then I applied them all like this:

1
2
3
4
5
(* Do <environment> bindings *)
List.iter (do_env_binding env impls) bindings;
(* Do <executable-in-*> bindings *)
List.iter (do_exec_binding config env impls) bindings;


I now think this is bad style, because if a new binding type is added no compiler warning will appear. It’s better to have the functions accept only the single kind of binding they process. Then the code that calls them separates out the two types of binding. If a new type is added later, the code will issue a warning about an unmatched case:

1
2
3
4
5
6
7
8
let do_env_binding env impls {var_name; mode; source} = ...
bindings |> List.iter (function
  | EnvironmentBinding b -> do_env_binding env impls b
  | ExecutableBinding b -> Queue.add b exec_bindings
);
exec_bindings |> Queue.iter (do_exec_binding config env imps)


Handy operators

The recently released OCaml 4.01 adds two new built-in operators, @@ and |>. They’re very simple, and you can define them yourself on older versions like this:

1
2
let (@@) fn x = fn x
let (|>) x fn = fn x


They both simply call a function with an argument. For example print @@ "Hello" is the same as print "Hello". However, they are very low precedence, which means you can use them to avoid parenthesis. For example, these two lines are equivalent (we load a file, parse it as XML, parse the resulting document as a 0install selections document and then execute the selections):

1
2
execute (parse_selections (parse_xml (load_file path)))
execute @@ parse_selections @@ parse_xml @@ load_file path


The advantage here is that when you read an (, you have to scan along the rest of the line counting brackets to find the matching one. When you see @@, you know that the rest of the expression is a single argument to the previous function.

The pipe operator |> is similar, but the function and argument go the other way around. These lines are equivalent:

1
2
execute @@ parse_selections @@ parse_xml @@ load_file path
load_file path |> parse_xml |> parse_selections |> execute


Intuitively, the result of each segment of the pipeline becomes the last argument to the next segment.

At first, I couldn’t see any reason for preferring one or the other, so I decided to use just @@ initially (which was most familiar, being the same as Haskell’s $ operator). That was a mistake. |> is the more useful of the two.

In the original post, I complained that you had to write loops backwards, giving the loop body first and then the list to be looped-over. With |>, that problem is solved:

1
2
3
items |> List.iter (fun item ->
  Printf.printf "Item: %s\n" item
)


Using the pipe operator eliminates the mismatch between the desire to make the function the last argument and OCaml’s common (but not universal) convention of putting the data structure last. It can also make things look more object-oriented, by putting the object first. Consider this code for setting an attribute on an XML element:

1
set_attribute a b c


Which is the element, and which are the name and value? Written this way, it’s hopefully obvious that c is the element:

1
c |> set_attribute a b


Sequences become clearer. For example, consider adding two items to a collection in order:

1
2
3
4
5
6
7
8
  (* Using () *)
  let items = Collection.add "two"
    (Collection.add "one" items)
  (* Using |> *)
  let items = items
  |> Collection.add "one"
  |> Collection.add "two"


I was even considering changing the order of the arguments to my starts_with function to make it work with pipe. Currently, we have:

1
if starts_with a b then ...


But does it check that a starts with b or the other way around? They’re both strings, so type checking won’t catch errors either. Reversing the arguments and using pipe, it would be clear:

1
if a |> starts_with b then ...


However, extlib’s version uses the original order, so I decided not to change it. Also, I used it in a lot of places and I couldn’t find a semantic patching tool to change them all automatically (like Go’s gofmt -r or C’s Coccinelle - which, interestingly, is written in OCaml).

Handling option types

I noted the lack of a null coalescing operator in my original code. I’ve now made some helpers for handling option types (I don’t know if OCaml programmers have standard names for these). I find them neater than using match statements.

The first I named |?. It’s used to get the value out of an option, or generate some default if it’s missing. It’s defined like this:

1
2
3
4
let (|?) maybe default =
  match maybe with
  | Some v -> v
  | None -> Lazy.force default


Using OCaml’s built-in lazy syntax makes this a bit nicer than having to define an anonymous function each time you use it. It’s used like this:

1
2
3
4
5
6
7
8
9
10
11
12
13
14
(* Use config.dir, or $HOME if it's not set *)
let dir =
  match config.dir with
  | None -> Sys.getenv "HOME"
  | Some dir -> dir in
(* Using |? *)
let dir = config.dir |? lazy (Sys.getenv "HOME") in
(* Guess the MIME type if it's not set on the element *)
let mime_type = mime_type |? lazy (Archive.type_from_url url) in
(* Abort if not set *)
let item = lookup name |? lazy (raise_safe "Item '%s' not found" name)


The only slight issue I have is that if you forget the lazy when raising an exception then you don’t get a compile-time error. It just throws the exception in all cases. However, you should spot this problem quickly when you test it.

Another common task is to execute some code with the option’s value only if it’s set. I defined if_some for this. It takes a function to call with the value, but partial application means you usually don’t need to define one explicitly. For example, to stop a timer if you have one:

1
2
3
4
5
6
7
8
(* Normal method *)
let () =
  match timeout with
  | None -> ()
  | Some timeout -> Lwt_timeout.stop timeout in
(* Using if_some *)
timeout |> if_some Lwt_timeout.stop;


Finally, there’s a pipe_some, which is the same except that it maps None -> None rather than None -> ().

Conclusions

After spending a few months writing OCaml, my coding style hasn’t actually changed much since my first attempts right after reading the tutorials. I’m not sure whether this is good or bad. Like Python, there is a one-obvious-way-to-do-it feeling to OCaml, unlike Haskell and Perl, which somehow seem to encourage clever-but-incomprehensible solutions. When I’ve read other people’s OCaml code (e.g. Lwt), I haven’t found anything new or hard to read.

The main changes have been cosmetic: the removal of ;;, fewer brackets, and the |> operator to make the code tidier, plus some common helper functions. I’m also finding more ways to make the type system do more of the work: e.g. avoiding catch-all match cases and using Polymorphic Variants.

The most useful functions I’ve added (some borrowed from other people) are:

  |? for handling None values (see above)
  if_some and pipe_some (see above)
  finally_do to work around the lack of a try...finally syntax in OCaml
  filter_map (apply a function to each item in a list, filtering out any None replies)
  starts_with (as in Python)
  abspath and realpath (to resolve pathnames; translated from the Python standard library code)


If anyone else wants my realpath, it’s in Support.Utils.

What other useful tips or utilities do people have?
Hide
        
      
                    by Thomas Leonard at Oct 13, 2013 
      
      
    
  


       
                  FPDays 2013 Real World OCaml tutorial in Cambridge
      (Anil Madhavapeddy)
    
    
                                
Yaron Minsky and I have been running OCaml tutorials for a few years at
ICFP and CUFP, but haven’t
really spread out into the wider conference circuit.  Now that Real World
OCaml is almost finished, the scene is set for
doing much more.  The first such tutorial is being help at FPDays
2013 on October 24th in the lovely Murray
Edwards College in Cambridge.  Check out the Lanyrd
page for ticket information, and the OCaml
session page for more information.

The basic layout of the tutorial is to get started with the guided tour of the
book, and then work through building a
distributed message broker.
This gets you familiar with the Core
standard library, the Async event-driven
I/O library, and all the strongly-typed RPC plumbing that goes in between.
We’re hoping to have physical preprints of the book available for free to
attendees, so do sign up fast if you wish to attend.

As a bonus, the Cambridge FPDays session will feature Jeremy Yallop working through
the book and conducting t…Read more...
Yaron Minsky and I have been running OCaml tutorials for a few years at
ICFP and CUFP, but haven’t
really spread out into the wider conference circuit.  Now that Real World
OCaml is almost finished, the scene is set for
doing much more.  The first such tutorial is being help at FPDays
2013 on October 24th in the lovely Murray
Edwards College in Cambridge.  Check out the Lanyrd
page for ticket information, and the OCaml
session page for more information.

The basic layout of the tutorial is to get started with the guided tour of the
book, and then work through building a
distributed message broker.
This gets you familiar with the Core
standard library, the Async event-driven
I/O library, and all the strongly-typed RPC plumbing that goes in between.
We’re hoping to have physical preprints of the book available for free to
attendees, so do sign up fast if you wish to attend.

As a bonus, the Cambridge FPDays session will feature Jeremy Yallop working through
the book and conducting the tutorial: he has an incredible depth of knowledge
about the innards of OCaml’s type system, and so advanced users will also find
a good home in this tutorial to throw questions at him too!  For those of you
interested in other programming languages, there are also excellent-looking
sessions on Erlang, F# and Scala, and Phil Wadler is giving a keynote speech.
I’m most excited about Sam Aaron’s session
on live coding and music though.  You have to hear it to believe it…
Hide
        
      
                    by Anil Madhavapeddy at Oct 08, 2013 
      
      
    
  


       
                  Using Travis for secure deployments with SSH
      (Anil Madhavapeddy)
    
    
                                
In my previous post
on Travis, I explained how it can be used to easily test OCaml packages on
GitHub without having to host any infrastructure yourself.

The next step I wanted to investigate was how to use Travis to trigger service deployments
after a successful build.  One nice feature that Travis has is support for
encrypted environment variables. The basic workflow is that you encrypt
key/value pairs using a public key that they publish per GitHub repository.
Once registered, this is made available as a decrypted environment variable
within the Travis worker.  You can use this to transmit API keys or other
authentication data that you need to commit to the travis.yml file, but
obviously can’t leave on a public repository for the world to see.

The small hitch with this whole scheme is that there’s a very small limit
of about 90 bytes or so for the size of each individual environment variable
that’s exported, and so you can’t just stash an SSH private key in there.
Instead…Read more...
In my previous post
on Travis, I explained how it can be used to easily test OCaml packages on
GitHub without having to host any infrastructure yourself.

The next step I wanted to investigate was how to use Travis to trigger service deployments
after a successful build.  One nice feature that Travis has is support for
encrypted environment variables. The basic workflow is that you encrypt
key/value pairs using a public key that they publish per GitHub repository.
Once registered, this is made available as a decrypted environment variable
within the Travis worker.  You can use this to transmit API keys or other
authentication data that you need to commit to the travis.yml file, but
obviously can’t leave on a public repository for the world to see.

The small hitch with this whole scheme is that there’s a very small limit
of about 90 bytes or so for the size of each individual environment variable
that’s exported, and so you can’t just stash an SSH private key in there.
Instead, it needs to be Base64 encoded, split it up into multiple environment
variables of the right size, and then reassembled within the Travis VM.  Rather
than deal with importable shell scripts between MacOS X and Linux, I created
a small travis-senv command-line binary to make this easier.

To use it, just opam install travis-senv and follow the instructions on the
README at the homepage.  Here’s the
fragment of shell script that pushes the build output to another GitHub
repository:

if [ "$DEPLOY" = "1" ]; then
  # get the secure key out for deployment
  opam install travis-senv
  mkdir -p ~/.ssh
  SSH_DEPLOY_KEY=~/.ssh/id_dsa
  travis-senv decrypt > $SSH_DEPLOY_KEY
  chmod 600 $SSH_DEPLOY_KEY
  echo "Host mirdeploy github.com" >> ~/.ssh/config
  echo "   Hostname github.com" >> ~/.ssh/config
  echo "   StrictHostKeyChecking no" >> ~/.ssh/config
  echo "   CheckHostIP no" >> ~/.ssh/config
  echo "   UserKnownHostsFile=/dev/null" >> ~/.ssh/config
  git config --global user.email "travis@openmirage.org"
  git config --global user.name "Travis the Build Bot"
  git clone git@mirdeploy:mirage/mirage-www-deployment
  cd mirage-www-deployment
  mkdir -p xen/$TRAVIS_COMMIT
  cp ../src/mir-www.xen ../src/mir-www.map ../src/www.conf xen/$TRAVIS_COMMIT
  bzip2 -9 xen/$TRAVIS_COMMIT/mir-www.xen
  git pull --rebase
  git add xen/$TRAVIS_COMMIT
  git commit -m "adding $TRAVIS_COMMIT"
  git push
fi

I’ve been using this to automate the construction of the Mirage Xen unikernel
homepage.  Every time there’s a push to the
mirage-www, the Travis
scripts now
retrieve an SSH deployment key using travis-senv, and push the results of the
build to the
mirage-www-deployment
repository that stores the build output.  This repository is polled by the
hosting machines we have to look for new kernels and rotate the website (but
more on this later – I’m just integrating the EC2 and Rackspace scripts to
remove this step entirely next!)
Hide
        
      
                    by Anil Madhavapeddy at Oct 06, 2013 
      
      
    
  


       
                  Intellisense for OCaml with Vim and Merlin
      (Anil Madhavapeddy)
    
    
                                
I’m finalizing the installation instructions for Real World OCaml and
finally got around to configuring the awesome Merlin editor tool.  I pretty much never configure
my editor, so here are my notes on getting Merlin working with Vim completely from scratch on
MacOS X (I’m on 10.9DP7, but this should work with earlier versions of MacOS X too).

First, install some basic plugin tools by following the Synastic installation instructions.

Install vim-pathogen:

mkdir -p ~/.vim/autoload ~/.vim/bundle; \
curl -so ~/.vim/autoload/pathogen.vim \
https://raw.github.com/tpope/vim-pathogen/master/autoload/pathogen.vim

Install vim-sensible for useful defaults:

cd ~/.vim/bundle
git clone git://github.com/tpope/vim-sensible.git

Install Syntastic:

cd ~/.vim/bundle
git clone https://github.com/scrooloose/syntastic.git

Follow the Merlin/vim from scratch directions:

opam install merlin

Finally, add this this in your ~/.vimrc to turn everything on.

execute pathogen#infect()
let s:ocamlmerli…Read more...
I’m finalizing the installation instructions for Real World OCaml and
finally got around to configuring the awesome Merlin editor tool.  I pretty much never configure
my editor, so here are my notes on getting Merlin working with Vim completely from scratch on
MacOS X (I’m on 10.9DP7, but this should work with earlier versions of MacOS X too).

First, install some basic plugin tools by following the Synastic installation instructions.

Install vim-pathogen:

mkdir -p ~/.vim/autoload ~/.vim/bundle; \
curl -so ~/.vim/autoload/pathogen.vim \
https://raw.github.com/tpope/vim-pathogen/master/autoload/pathogen.vim

Install vim-sensible for useful defaults:

cd ~/.vim/bundle
git clone git://github.com/tpope/vim-sensible.git

Install Syntastic:

cd ~/.vim/bundle
git clone https://github.com/scrooloose/syntastic.git

Follow the Merlin/vim from scratch directions:

opam install merlin

Finally, add this this in your ~/.vimrc to turn everything on.

execute pathogen#infect()
let s:ocamlmerlin=substitute(system('opam config var share'),'\n$','','''') .  "/ocamlmerlin"
execute "set rtp+=".s:ocamlmerlin."/vim"
execute "set rtp+=".s:ocamlmerlin."/vimbufsync"

Trying it out

Read the excellent instructions at the Merlin for Vim from scratch wiki page.  A few immediately useful things I did are below.

Displaying types: You can get the local type of something by typing in \t anywhere.  This is the <LocalLeader> for anyone who has customized their setup (it is mapped to backslash by default).

Tab completing functions: You can tab-complete module signatures by pressing Ctrl-X-O (make sure you keep Ctrl pressed the whole time).  This activates vim’s OmniComplete mode.  To get this to work with your external projects, create a .merlin file.  I use this for ocaml-dns for example.

PKG lwt cstruct
S lib
S lib_test
S lwt
B _build/lib
B _build/lib_test
B _build/lwt

Also, don’t forget to pass the -bin-annot function to the compiler to
generate the typed-AST cmt files.  You can add true: bin_annot to your
_tags file if you’re using OCamlbuild (in OCaml 4.01.0 or higher, and
it’ll be silently ignored on previous versions so you don’t need to worry
about breaking older OCaml compilers).

Hide
        
      
                    by Anil Madhavapeddy at Oct 03, 2013 
      
      
    
  


       
                  Liveblogging the first Human Data Interaction workshop
      (SRG Syslog)
    
    
                                I'm at the Open Data institute, with Richard Mortier, Jon Crowcroft, Amir Chaudhry and Hamed Haddadi , live-blogging a daylong workshop about our emerging Human-Data Interaction research initiative.  The room is packed with notable researchers from all over the UK, so this promises to be an exciting day!

Mort opens up with an introduction to the format of the workshop and lays out the problem space:
Visualisation and sense making: how do we make sense of such complex, technical systems, and crucially, what is the process of translating the information to
Transparency and audit: is essential to give a feedback loop to users
Privacy and control: ...
Analytics and commerce: there is clearly an ecosystem forming around personal data (c.f. several startups specifically around this
Data to knowledge: what are the new business models around

Attendee introductions
Danielle (missed her last name, is from Switzerland): did a PhD on anomaly detection and then worked at Swiss banks on securi…Read more...I'm at the Open Data institute, with Richard Mortier, Jon Crowcroft, Amir Chaudhry and Hamed Haddadi , live-blogging a daylong workshop about our emerging Human-Data Interaction research initiative.  The room is packed with notable researchers from all over the UK, so this promises to be an exciting day!

Mort opens up with an introduction to the format of the workshop and lays out the problem space:
Visualisation and sense making: how do we make sense of such complex, technical systems, and crucially, what is the process of translating the information to
Transparency and audit: is essential to give a feedback loop to users
Privacy and control: ...
Analytics and commerce: there is clearly an ecosystem forming around personal data (c.f. several startups specifically around this
Data to knowledge: what are the new business models around

Attendee introductions
Danielle (missed her last name, is from Switzerland): did a PhD on anomaly detection and then worked at Swiss banks on security monitoring. The key challenges in the area are around the nature of the questions we need to ask about all the data that we have access to.
Steve Brewer: coordinator of the IT as a Utility network which is funding all this.  Interested in the next steps: both immediate and the overall vision and direction it's all going.  Concrete actions emphasised.
Ian Brown: Oxford University and cybersecurity. Interested broadly in these issues and also has a degree in psychology and behavioural psychology.  The challenge is how to balance the "engineering" (this isnt fundamental computer science) and understand why there is so little takeup of this.
Amir Chaudhry: working on a toolstack for the Internet of Things.  When we have tweeting toasters, how do we make it all useful to people. It's not the raw data, but the insights we present back to people to make it useful.
Elizabeth Churchill: was at Yahoo, now at eBay Research.  Right now the challenge I'm facing is personalisation, and is trying to understand why the models they have of people are rubbush (this is why so much advertising isnt very useful).  The reasoning processes and data quality is important.  We are making assertions about people based on data they have gathered for another purpose, and the ethical and business issues are important. Has been pushing on : what the data source, what is the data quality, and are these assertions based off data thats appropriate, and how can we design new algorithms"
Jon Crowcroft: from the Computer Lab in Cambridge and does anything to do with communication systems, and leads the Horizon project with Mort and PAWS which is working on wifi access in disadvantaged areas. Also working on Nymote.  Is interested in new human/data rights vs t&cs.  Rights and duties are encoded in terms and conditions (badly) -- see Latier's latest book about this, and see how poor Amazon's recommendations are.  We're interested in building a technology where you own your own data but business practises all gel together.  We had a workshop at how the EU is pushing the right to be forgotten, so how can we ensure that data can be removed including all references.  People go "its too difficult", but this isn't true -- takedowns work, and why is it that only big corporations can afford to take down stuff.  The right to oblivion ("not be there in the first place") and data shouldn't be a public good but people should have the right to be in a paid avoidance zones. (See Shockwave Rider by Brunner, 30 years old, loads of great technical ideas, and Future Shock is a good read too).  Can we shift regulatory positions and have
Thomas Dedis: runs Fab Lab in Barcelona.  Cofounder of Smart Citizen, crowdfunded environment sensing platform based on a piece of hardware based on Arduino.  Allows people to capture data in their own homes, and push it to an online platform.  Allows people to push it in a 'conscious' way? Go to smartcitizen.me to see the data platform.  Intended to grow into other places, but capturing air pollution, humidity, temperature and other environment data.  Main thing is that you own your own data.  Much cheaper than the competition too and crowdfunding.
Martin Dittus (@dekstop) is a PhD student at UCL and works on cities.io. Thousands of people mapping the planet in incredibly detailed ways and it works with both commercial and non-commercial stuff. What makes these systems work, what are the processes to coordinate things, questions of data quality and how to assert stuff over it?  Used to be part of the last.fm data team which is about personal data gathering and detailed profiling that users themselves put up.
Christos Efstratiou: as of September is a lecturer at University of Kent (and is still a visiting research at Cambridge).  Works on sensing, more broadly that has to do with people and sensors in the environment and embedded sensing in the environment.  Privacy is a huge issue and is his key challenge.  This isn't sensing in the old style like Active Badge -- back then, people werent aware of the issues and nowadays, people are more aware of privacy.  So the challenge is the evolving user perceptions and how our system design works.  Anecdote: at a recent wedding he was at, there was a request from the  bride/groom to note post any public photos to Twitter/Facebook. We've lost control over our public personas.
Julie Freeman: an actual resident in this building and the art associate for the ODI space!  Is a PhD student at QMW and is interested in echoing other people.  Interested in physical manifestations of digital data, and how we can "step away from the screen".  Broad interests in transformation of digital data.
Barbara Grimpe: from Oxford and is a sociologist in an interdisciplinary EU project on "Governance for Responsible Innovation".  They have a network notion of responsibility which takes into account that a lot of data production and data use takes place in networks that posses strong ties between people.  She started two cases studies in two area: telecare technologies for elderly people (relevant due to the EU Horizon 2020 societal challenge in aging and wellbeing due to the demographic change in western societies, and this brings the ethical issue around the use of personal data at scale); there is difficulty of informed consent due to dementia also. The other case study is completely different and is about financial market technology and the data is highly commercially sensitive, so understanding how transparency can be balanced against financial control and the need for genuine market secrets to facilitate trade.
Wifak Gueddana is a postdoc at information systems group at LSE.  Did her PhD on open source communities and how open source can work for NGOs and grass roots.  Working on a research project for online platforms -- how to use computational analytics to collect and process data and deal with the qualitative and subjective issues.
Hamed Haddadi: lecturer in QMUL, asking how we can compute over mobile data and has worked on advertising and privacy aware system systems.  Linking the temporal nature of the data and understanding how much of your data is exposed (amusing anecdote about wearing shorts)
Muki Haklay: professor of GIScience and the Extreme Citizen Science group. He's interested in participatory citizen science and cyberscience.  Interested in GeoWeb and mobiile GeoWeb technologies and understanding how tools for voluntary participation work (open street map, participatory geoweb).  "How can you give control to people that are not technical" and how do we build protocols for cultural sensitivity?  Working on Open Street Map, while its easy for techies to contribute and feel happy, it might be an issue from a privacy perspective without GPS fuzzing or pseudonyms (you can say "I know where you live" to every OSM user). (discussion about most data being rubbish and is a psychology question about whether this depresses people!)
Jenny Harding: from the Ordnance Survey who control most of the UK's mapping data and is a principal scientist in the research team team on working on how people interact with geography in business, public service and leisure.  Moving beyond just GPS into the connective function about what's going on in places, and the connection between different objects and places.  What is the purpose for needing all this connected information, and how can the Ordnance Survey better serve its users with such usable connected data?  She commissions internal and external research and this includes PhDs and postdocs.  Challenge for this workshop: how personal data relates to location and the different levels of granularity at which such relationships can be made -- beyond GPS, there is address data in (e.g.) supermarket loyalty cards, and other data at a postcode level, and different types of geographies all have connections.  Understanding provenance of data is really really important .
Pat Healey: Professor of Human Interaction and head of cognitive science research group and workson health issues.  Not here yet so introduced by Hamed.
Tristan Henderson; lecturer in compsci at St Andrews in Scotland, did his PhD on first person shooter games and runs a widely used mobile data set called CRAWDAD. They archive and share it and so work a lot on redistribution of data. His undergrad was economics so his interest is on behavioural aspects and usability issues too (Tristan has recently joined the HCI group and the ethics committee at St Andrews).  How can we get researchers to further engage with the ethics process and to refine the notions of informed consent in electronic terms.  Challenges: what is acceptable HDI and are we conducting it in an acceptable way (q from Mort: how broad? a: everything).  And is HDI unique enough that we might need another term.
Laura James: from OKF (not here yet)
Helene Lambrix: visiting LSE and usually at Paris-Dauphine University in France and is hoping to finish her PhD this year! Interested in corporate reputation and social media.
Neal Lathia: researcher at Cambridge University and did his PhD on online recommender systems. Noticed a disparity between data services online and the offline word so started working on recommender systems for urban systems (banging head against TFL data).  At Cambridge, started working with psychologists and leads EmotionSense (how satisfied are you with your life, as well as ongoing smartphone sensor data) -- its really popular.  Challenges: language issue around how we present issue and motivate people around using that data (how does using EmotionSense affect their behavior)
Panos from Brunel: interested in cybersecurity and intelligence from media data mining and cloud based work. Commodification process of digital economy data and what is the legal frameowork surrounding this.  What are the personas for data to apply frameworks and data mining techniques to it?  Challenges: regulatory system using big personal data.
Eva from University of Nottingham and has just submitted PhD and waiting viva. Background is political science and internationl relations, and is interested in informed consent and how we sustain consent rather than just secure it as a one-off. The challenges: the human bit and how we communicate the complexity of systems and how people can make meaningful decisions. If you want people to be engaged with process then we need data to be more social and human.
Ursula Martin: professor in a Russel Group university in Mile End road. Is a mathematician and is researching the production of mathematics, and how it happens in the first place. Producing maths is a slow, painstaking thing, and is wondering how the rate of production can keep up with our needs. When interacting with an outfit getting her data, she's not just an isolate, but is actually part of a large complex system.
Richard Mortier "mort": From Nottingham and is the dude running this workshop so has introed before!
Nora Ni Loideain: doing a PhD in the european data protection and this requires the mandatory retention of data by telecomms provider. References recent US events cf Snowden and her PhD is on privacy oversight and the guards (or lack thereof) in current frameworks.  Challenges: how can we build these safeguards and what is the nature of informed consent with these.
Ernima Ochu: based in Manchester. Sometimes an activist, sometimes a writer, sometimes an artist. Background in neuroscience!  The social life of data and what happens around it, and how people get around based on it.
Yvonne Rogers and is from team UCL and is director of UCLIC (Interaction Center) and also an Intel-funded institute at UCL where they work on connected data.  Given lots of data from sources, interested in how people can engage with it and how to visualise . (Hamed: "Human City Interaction is the next thing!")
Geetanjali Sampemane; background in Computer Science, and works at Google on Infrastructure Security and Privacy group.  Challenge is how to help people calibrate the benefits and risks for appropriate tradeoffs.  How can humans make an informed choice, and this isn't based on informed choice.  Giving people buttons and options isn't the most useful way to approach this, and we need a mental model similar to how we judge risks in the physical world.  In the online world, noone understands how dangerous it is to reuse passwords. Security people have made is a little worse by telling people to use complicated passwords, but brute force isn't the big problem right now, it's the connectivity of services.
Cerys Willoughby: Southhampton and looking at the usability of notebooks and wondering how to make the interfaces usable without being a technological guru. (missed rest due to reading cool comic she projected about her work. Sorry!)
Eiko Yoneki: from Cambridge, and she works on digital epidemiology and real world mobility data collection in Africa (e.g. EipPhone).  She analyses network structure to understand graphs and connectivity.  Also works on CDNs and builds self-adaptive CDNs, and works on graph-specific data parallel algorithms.
Jonathan Cave: game theorist (Yale, Cambridge, Stanford) and worked in a lot of government/academia/research areas. Works on economics and IoT (festival of things and the boundaries of humanity is coming up soon in Cambridge on 29th October).  Fascinating anecdote about price of sick animals
George Danezis: formerly MSR now UCL, and is interested in technical security aspects of how to glue together distributed mobile devices and not leave our personal data unreadable.  Has done work on privacy friendly statistics and how we can process and analyse it as an aggregate data set. Has worked in the context of smart metering in Europe.
Breakout sessions
We then had breakout sessions to brainstorm the challenges in this area (lots of post it notes and arguments), and Amir Chaudhry has summarised the results of the 5 presentations here:
Disambiguating data. For example from the home and organisations. This isn't a new problem but becomes more important the more data collection occurs using different sources.  Who are the principles in terms of onwership and provenance?  How do we deal with communities/groups  and data control?
Why not try crowd sourcing mechanisms for people to use so that they can improve the use of sites like Ryanair (who deliberately obfuscate things).  Changing mindset from consumer perspective to a producer perspective.  i.e humans make data and perhaps can provide this to others for economic benefits.
We have data and different notions of data quality.  It's not always the case that the most scientific data is the best, depending on how it's used.  We have Collectivist notions of data culture: e.g this conversation right now in the room isn't just individual, it's all of us so we can't ascribe it to individuals. Then there are Network notions, based on transactions that use a reductionist view to decide what they're worth.  Can think of these on a continuum and the research challenge is how well do different ends of above scale in producing data and making value.  Interesting question is if people opt out (right to forget or right to oblivion), then the data that is left is flawed.
We need to examine current assumptions and values around data.  What is the unit of analysis? Must be clearer on this. Where and when we look at what data also matters as well as global aggregation.  Sometimes also want to look at trajectories and data flows and how this changes over time.  How do we interact with this data.  Do users interact with data directly or with something that sits in front of it?  There's an interaction between data science and the creative process of presenting information in a certain way.  This depends on ultimate goal being served, for example e.g beavioural changes or just increased engagement.
There are big challenges in integration of groups who want to construct humans. Groups like social sciences (think about risk), Psychological science (reputation), Data sciences (Epistomology and Ontology), Design science (Interfaces and interactions). What is the new meaning of ownership and liability? e.g who owns this collection of posters and the ideas that have come out? [Hamed and Mort clarify that it's all theirs!]  What happens if there are negative consequences as a result of using poor data?
Business models are also important in order to go from studies to practice. Are there new social structures we could make to help this?  For example, we have venture capital that takes risks but what about social capital to spread risk to create new businesses e.g kickstarter and the like.
What does informed consent mean? Current system puts onus on user to understand all the contractual conditions before deciding.  Perhaps there's a social-network method of crowd-sourcing opinions on ToS or providing some kind of health rating?  Perhaps data protection agencies could certify terms or maybe the EFF or non-profits can provide some kind of rating system (c.f Moody's etc?). For example from the home and organisations. This isn't a new problem but becomes more important the more data collection occurs using different sources.  Who are the principles in terms of onwership and provenance?  How do we deal with communities/groups  and data control?
Why not try crowd sourcing mechanisms for people to use so that they can improve the use of sites like Ryanair (who deliberately obfuscate things).  Changing mindset from consumer perspective to a producer perspective.  i.e humans make data and perhaps can provide this to others for economic benefits.
We have data and different notions of data quality.  It's not always the case that the most scientific data is the best, depending on how it's used.  We have Collectivist notions of data culture: e.g this conversation right now in the room isn't just individual, it's all of us so we can't ascribe it to individuals. Then there are Network notions, based on transactions that use a reductionist view to decide what they're worth.  Can think of these on a continuum and the research challenge is how well do different ends of above scale in producing data and making value.  Interesting question is if people opt out (right to forget or right to oblivion), then the data that is left is flawed.
We need to examine current assumptions and values around data.  What is the unit of analysis? Must be clearer on this. Where and when we look at what data also matters as well as global aggregation.  Sometimes also want to look at trajectories and data flows and how this changes over time.  How do we interact with this data.  Do users interact with data directly or with something that sits in front of it?  There's an interaction between data science and the creative process of presenting information in a certain way.  This depends on ultimate goal being served, for example e.g beavioural changes or just increased engagement.
There are big challenges in integration of groups who want to construct humans. Groups like social sciences (think about risk), Psychological science (reputation), Data sciences (Epistomology and Ontology), Design science (Interfaces and interactions).  What is the new meaning of ownership and liability? e.g who owns this collection of posters and the ideas that have come out? [Hamed and Mort clarify that it's all theirs!]  What happens if there are negative consequences as a result of using poor data? Business models are also important in order to go from studies to practice. Are there new social structures we could make to help this?  For example, we have venture capital that takes risks but what about social capital to spread risk to create new businesses e.g kickstarter and the like.
What does informed consent mean? Current system puts onus on user to understand all the contractual conditions before deciding.  Perhaps there's a social-network method of crowd-sourcing opinions on ToS or providing some kind of health rating?  Perhaps data protection agencies could certify terms or maybe the EFF or non-profits can provide some kind of rating system (c.f Moody's etc?)
Next steps
Ian Brown took notes on our breakout session on business models for privacy:
Collectives/cooperatives sharing data through PDSes
How to incentivise membership? Dividends, social benefit (e.g. medical research)
what currently exists where data controller has strong incentive not to leak/breach data e.g. Boots for brand loyalty, Apple/Facebook? So long as customer can switch effectively (portability, erasure)
Power tool sharing at village level. Hang off existing structures e.g. local councils.
New forms of micro-markets e.g. physical gatherings? Alternatives to currencies. Kickstarter? Distribution reduces risk of centralised architectures.
What do syndicalist-anarchist models of data management look like?
Current uses of data are optimising existing business practices. But what totally new practices could be enabled? Human-facing efficiencies?

What are types of biz models? Startups, personal profit, NGO, medical research, banks. Balanced investment portfolio.
Hide
        
      
                    by Anil Madhavapeddy at Oct 02, 2013 
      
      
    
  


       
                  Test your OCaml packages in minutes using Travis CI
      (Anil Madhavapeddy)
    
    
                                
A few months ago, Mike Lin posted instructions of how to bootstrap an OCaml testing environment within the Travis continuous integration tool.  I finally got around to integrating his prototype scripts properly using the latest OCaml and OPAM versions during my travels last week to ICFP.  It’s been an extraordinarily quick and pleasant experience to add the (free!) Travis test runs to my OCaml programs on GitHub, so here’s how you can do it too.  Dave Scott and I have used this for about 15 of our own projects already, and I’m switching the whole of the Mirage repo infrastructure over to it this week.

(edit: I’ve done a followup post about integrating Travis with SSH to make secure deployments easier.)

Getting started

Getting my first Travis build working with one of my OCaml projects took about 2 minutes in total:

First, log into Travis and sign in via Twitter.  Click on the Accounts button on the top-right and you should see a list of the all the GitHub repositories that…Read more...
A few months ago, Mike Lin posted instructions of how to bootstrap an OCaml testing environment within the Travis continuous integration tool.  I finally got around to integrating his prototype scripts properly using the latest OCaml and OPAM versions during my travels last week to ICFP.  It’s been an extraordinarily quick and pleasant experience to add the (free!) Travis test runs to my OCaml programs on GitHub, so here’s how you can do it too.  Dave Scott and I have used this for about 15 of our own projects already, and I’m switching the whole of the Mirage repo infrastructure over to it this week.

(edit: I’ve done a followup post about integrating Travis with SSH to make secure deployments easier.)

Getting started

Getting my first Travis build working with one of my OCaml projects took about 2 minutes in total:

First, log into Travis and sign in via Twitter.  Click on the Accounts button on the top-right and you should see a list of the all the GitHub repositories that you have access to.  Just click the On switch for the one you want to start testing.  Nothing will actually happen until the next code push or pull request goes to that repository.  Behind the scenes, the On button that you clicked use the GitHub APIs to turn on the Travis post-commit hook for your repository.

Create a .travis.yml file in your main repository (see below or this gist).  Travis doesn’t have native support for OCaml, but it isn’t really needed since we can just use C and write our own shell scripts.  The env variables define a matrix of the different combinations of OCaml and OPAM that we want to test.  Just remove variations that you don’t care about to avoid wasting Travis’ CPU time (open source projects are supported on a fair-use basis by them).
Here’s the .travis.yml that I used for my ocaml-uri library: 

language: c
script: bash -ex .travis-ci.sh
env:
  - OCAML_VERSION=4.01.0 OPAM_VERSION=1.0.0
  - OCAML_VERSION=4.01.0 OPAM_VERSION=1.1.0
  - OCAML_VERSION=4.00.1 OPAM_VERSION=1.0.0
  - OCAML_VERSION=4.00.1 OPAM_VERSION=1.1.0
  - OCAML_VERSION=3.12.1 OPAM_VERSION=1.0.0
  - OCAML_VERSION=3.12.1 OPAM_VERSION=1.1.0

Now you just need the .travis-ci.sh shell to run the actual tests.  Travis provides an Ubuntu Precise/i386 VM that is destroyed after every test run, so we need to initialize it with the OCaml and OPAM binary packages.  Since you often want to test different versions of all of these, I created a series of stable Ubuntu PPAs that have OCaml 3.12,4.0,4.1 and OPAM 1.0 and the (currently beta) 1.1 package manager.  You can find them all on my Launchpad page, but the below script takes care of it all for you.

# Edit this for your own project dependencies
OPAM_DEPENDS="ocamlfind ounit re"
	 
case "$OCAML_VERSION,$OPAM_VERSION" in
3.12.1,1.0.0) ppa=avsm/ocaml312+opam10 ;;
3.12.1,1.1.0) ppa=avsm/ocaml312+opam11 ;;
4.00.1,1.0.0) ppa=avsm/ocaml40+opam10 ;;
4.00.1,1.1.0) ppa=avsm/ocaml40+opam11 ;;
4.01.0,1.0.0) ppa=avsm/ocaml41+opam10 ;;
4.01.0,1.1.0) ppa=avsm/ocaml41+opam11 ;;
*) echo Unknown $OCAML_VERSION,$OPAM_VERSION; exit 1 ;;
esac
	 
echo "yes" | sudo add-apt-repository ppa:$ppa
sudo apt-get update -qq
sudo apt-get install -qq ocaml ocaml-native-compilers camlp4-extra opam
export OPAMYES=1
opam init 
opam install ${OPAM_DEPENDS}
eval `opam config env`
make
make test

Now just do a push to your repository (a commit adding the Travis files above will do), and you will soon see the Travis web interface update.  For example, here’s the output of ocaml-uri that the above example files are for.  Of course, you should tweak the scripts to run the tests that your own project needs.  Let me know if you make any useful modifications too, by forking the gist or e-mailing me.

Testing pull requests in OPAM

Travis isn’t just for code pushes though; as of a few months ago it can also test pull requests.  This is an incredibly useful feature for complex projects such as the OPAM repository that has lots of contributors.  You don’t need to do anything special to activate it: whenever someone issues a pull request, Travis will merge it locally and trigger the test runs just as if the code had been pushed directly.

I did do some special scripting to make this work with the OPAM package repository.  Ideally, every new package or metadata change in OPAM will attempt to rebuild just that package set (rebuilding the entire repository would far exceed the 50 minute testing budget that Travis imposes).  I put together a very hacked up shell script that greps the incoming diff and rebuilds the subset of packages.  This is now live, and you can see both successful and failed pull requests (once the request has been merged, there’s a tiny little green arrow beside the commit that was tested).

This is a very unconservative estimate test matrix since a package update will also affect the reverse transitive cone of packages that depend on it, but it does catch several common typos and incompatibilities (for example, packages that use OPAM 1.1-only features by mistake).  The longer term plan is to use the OCamlot command line tool to parse the pull request and compute an exact set of packages that need rebuilding.

Deployment


Travis has one last very cool feature up its sleeve.  When a project has successfully built, it can run a scripting hook in the VM, which can be used to trigger a further code push or service update.  If the service requires authentication, you can encrypt a secret in the travis.yml using their public key, and it will be available in the VM as an environment variable (but won’t be useful to anyone else looking at the code repository).

There are quite a few uses for these Travis deployment scripts: automating the rebuilding of the ocaml.org website infrastructure, rebuilding the central opam-doc cmt file archive, or even autoupdating Mirage microkernels to Amazon EC2 and Rackspace.

So how does this tie into the ongoing work on OCamlot?  Quite simply, it’s saved us a ton of frontend work, and lets us focus on the more interesting OCaml-specific problems.  Travis is also somewhat reactive (since it only runs in response to pushes or pull requests), and we still need to be able to run complete repository sweeps to look for more systematic breakage. It also doesn’t support any non-Ubuntu operating systems yet.  However, Travis takes the burden off us for handling the steadily increasing number of package updates, and can be used to launch further OCamlot jobs on the other architectures and distributions.  All in all, I’m very grateful to Mike for taking the trouble to blog about it back in March!

And a little bit of cheeky ARM hackery

I couldn’t resist one final little hack to see if I could actually do some ARM/OCaml testing using the Travis infrastructure.  I adapted Ian Campbell’s excellent guide to ARM cross-chroots, and ended up with a rough script that builds an unattended chroot and can run OCaml/OPAM installations there too.

This isn’t something I’m quite comfortable running on all of my repositories just yet though, since an OCaml cross-build using qemu took over 5 hours and was quite rightly terminated when running within Travis.  To mitigate this, I’ve built a custom apt-repository for ARMel packages to install a binary toolchain, and you can see the results of Lwt/ARM building successfully.  I’ll update on this as I get it working on more packages, as I’m quite interested in getting a solid Xen/ARM toolstack up and running for another exciting project over in OCaml Labs…
Hide
        
      
                    by Anil Madhavapeddy at Sep 30, 2013 
      
      
    
  


       
                  Experiences with OCaml objects
      (Thomas Leonard)
    
    
                                I’m now written 15,000 lines of OCaml while migrating 0install to the language. So here’s another “things I’ve learned” post…

The official objects tutorial offers a good introduction to using objects in OCaml, but it doesn’t explain a number of important issues. Chapter 3 of the OCaml manual does explain everything, but I had to read it a few times to get it.

The manual notes that:

  the relation between object, class and type in OCaml is very different from that in mainstream object-oriented languages like Java or C++, so that you should not assume that similar keywords mean the same thing.


Good advice. Coming from a Python/Java background, here are some surprising things about objects in OCaml:

  An object’s type is not the same as its class.
  A class A can inherit from B without being a subclass.
  A class A can be a subclass of B without inheriting from it.
  You don’t need to use classes to create objects.




I’m going to try explaining things in the …Read more...I’m now written 15,000 lines of OCaml while migrating 0install to the language. So here’s another “things I’ve learned” post…

The official objects tutorial offers a good introduction to using objects in OCaml, but it doesn’t explain a number of important issues. Chapter 3 of the OCaml manual does explain everything, but I had to read it a few times to get it.

The manual notes that:

  the relation between object, class and type in OCaml is very different from that in mainstream object-oriented languages like Java or C++, so that you should not assume that similar keywords mean the same thing.


Good advice. Coming from a Python/Java background, here are some surprising things about objects in OCaml:

  An object’s type is not the same as its class.
  A class A can inherit from B without being a subclass.
  A class A can be a subclass of B without inheriting from it.
  You don’t need to use classes to create objects.




I’m going to try explaining things in the opposite order to the official tutorial, starting with objects and adding classes later, as I think it’s clearer that way. Classes introduce a number of complications which are not present without them.

Table of Contents

  Creating objects
  Object types          A puzzle
    
  
  Creating many objects          Casting
    
  
  Classes          Using inheritance
      Problems with classes
    
  
  Conclusions


Creating objects

In Python, you create a single object by first defining a class and then creating an instance of it. In OCaml, you can just create an instance directly (Java can do this with anonymous classes).

For example, 0install code that interacts with the system (e.g. getting the current time, reading files, etc) does so by calling methods on a system object. For unit-testing, we pass mock system objects, while when running normally we pass a singleton object which interacts with the real system. We can define the singleton like this (simplified):

1
2
3
4
5
let real_system =
  object
    method time = Unix.time ()
    method exit code = exit code
  end


Note that the exit method is calling the built-in exit function, not recursively calling itself. Calling a method has to be explicit, as in Python.

To call a method, OCaml uses # rather than .:

1
Printf.printf "It is now %f\n" real_system#time;


Initially, I defined time as method time () = Unix.time (), but this isn’t necessary. Unlike for regular function definitions, the body of a method is evaluated each time it is called, even if it takes no arguments, not once when the object is created.

Object types

OCaml will automatically infer the type of real_system as:

< exit : int -> 'a; time : float >


(note: exit never returns, so it can be used anywhere, which is why it gets the generic return type 'a)

This is not a class (nor even a class type). It’s just a type.

Any object providing these two methods will be compatible with real_system. There is no need to declare that you implement the interface.

You also don’t need to declare the type when using the object. For example:

1
2
3
let exit_with_error system = system#exit 1
...
exit_with_error real_system


However, the automatic inference will often fail. In particular, if a method is defined with optional arguments then it will be incompatible:

1
2
3
4
5
6
7
8
9
10
let real_system =
  object
    method exit ?(code=0) msg =
      print_endline msg;
      exit code
  end
let exit_success system = system#exit "Success!"
exit_success real_system


Error: This expression has type < exit : ?code:int -> string -> 'a >
       but an expression was expected of type < exit : string -> 'b; .. >
       Types for method exit are incompatible


In a similar way, using labelled arguments will fail unless you use them in the same order everywhere. To avoid these problems, it seems best to define the type explicitly:

1
2
3
4
5
6
7
8
9
10
11
12
type system = <
  exit : 'a. ?code:int -> string -> 'a
>
let real_system =
  object (_self : system)
    method exit ?(code=0) msg =
      print_endline msg;
      exit code
  end
let exit_success (system:system) = system#exit "Success!"


As in Python, self is explicit. However, it’s attached to the object rather than to each method, and you can leave it out if you don’t need it. I added it here in order to constrain its type to system. I used _self rather than self to avoid the compiler warning about unused variables.

A puzzle

It seems to me that some object types can be inferred but not defined. Consider this interactive session:

1
2
3
4
5
# let x =
    object
      method len xs = List.length xs
    end;;
val x : < len : 'a list -> int > = <obj>


However, we can’t actually use the type it prints:

1
2
# type lengther = < len : 'a list -> int > ;;
Error: Unbound type parameter 'a


You can define this type:

1
# type lengther = < len : 'a. 'a list -> int > ;;


But that’s a different (and less useful) type:

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
# let y =
    object (_ : lengther)
      method len xs = List.length xs
    end ;;
val y : lengther = <obj>
# y#len [1;2;3];;
- : int = 3
# let test specimen = specimen#len [1;2;3];;
val test : < len : int list -> 'a; .. > -> 'a = <fun>
#  test x;;
- : int = 3
# test y;;
Error: This expression has type lengther
       but an expression was expected of type < len : int list -> 'a; .. >
       Types for method len are incompatible


I’m not sure what causes these problems. You can, however, use the cast operator (:>) to convert to the required type if it happens.

Update: x does have a type, but it’s polymorphic: 'a lengther. OCaml has cleverly noticed that I don’t actually store any 'a values in the object, so it allows this single object to handle multiple types. For most objects, this will not be the case (for example, a mutable stack object could be an int stack or a string stack, but not both). For details, see my later post Polymorphism for beginners.

Creating many objects

Usually, you’ll want to create many objects, sharing the same code. For example, when one 0install program depends on another, it may specify restrictions on the acceptable versions. Here’s how we make version_restriction objects to represent this (simplified):

1
2
3
4
5
6
let make_version_restriction expr =
  let test = Versions.parse_expr expr in
  object
    method meets_restriction impl = test impl.version
    method to_string = "version " ^ expr
  end


This is not a class. It’s just a function that creates objects.
It’s used like this:

1
2
let python2 = make_version_restriction "2..!3"
let python33_plus = make_version_restriction "3.3.."


Notice the test variable, which is like a private field in Java. It cannot be used from anywhere else, simply because it is not in scope. You can define functions here in the same way. OCaml does not allow accessing an object’s fields from outside (e.g. restriction.expr in Java or Python), but you can make a field readable by writing a trivial getter for it. e.g. to expose expr:

1
2
3
4
5
6
7
let make_version_restriction expr =
  let test = Versions.parse_expr expr in
  object
    method meets_restriction impl = test impl.version
    method to_string = "version " ^ expr
    method expr = expr
  end


Casting

You can cast to a compatible (more restricted) type using :>. e.g.

1
2
3
type printable = < to_string : string >
print (python2 :> printable)


However, OCaml does not store the type information at runtime, so you cannot cast in the other direction. That is, given a printable object, you cannot find out whether it really has a meets_restriction method. This doesn’t seem to be a problem, since the places where I wanted to check for several possibilities were better handled with variants.

Classes

OK, so we can create objects with public methods, constructors, internal functions and state, and define types (interfaces). So what are classes for? The key seems to be this: Classes are all about (implementation) inheritance. If you don’t need inheritance, then you don’t need classes.

Changing make_version_restriction to a class would look like this:

1
2
3
4
5
6
7
8
class version_restriction expr =
  let test = Versions.parse_expr expr in
  object
    method meets_restriction impl = test impl.version
    method to_string = "version " ^ expr
  end
let python2 = new version_restriction "2..!3"


We just changed the let to class and make_version_restriction to new version_restriction (in fact, there are some syntax restrictions when defining classes: a class body is a series of let declarations followed by an object, whereas a function body is an arbitrary expression).

When you define a class (e.g. version_restriction), OCaml automatically defines three other things:

  a class type (version_restriction)
  an object type (also called version_restriction), defining the public methods
  an object type for subtypes (#version_restriction)


The object type just defines the public methods provided by instances of the class. The class type also defines the API the class provides to its subclasses. Confusingly, OCaml calls this the “private” API (Java uses the term “protected” for this).

You can use method private to declare a method that is only available to subclasses, and val to declare fields (fields are always private). Methods can be declared as virtual if they must be defined in subclasses (this is like abstract in Java). A class with virtual methods must itself be virtual.

Using inheritance

To inherit from a class, use:

1
2
3
  object
    inherit superclass args as super
  end


Here’s an example from 0install: a distribution object provides access to the platform-specific package manager, allowing 0install to query the native package database for additional candidates. Each distribution subclasses the base class. Here’s my first (wrong) attempt to do this with classes (simplified):

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
class virtual distribution (distro_name:string) =
  object
    method match_name name = (distro_name = name)
    method virtual is_installed : string -> bool
  end
class virtual python_fallback_distribution distro_name : distribution =
  object (self)
    inherit distribution distro_name
    method private fallback_to_python = ...
    method is_installed = self#fallback_to_python
  end
class debian_distribution : python_fallback_distribution =
  let check_cache = ... in
  object
    inherit python_fallback_distribution "Debian" as super
    method is_installed package =
      try check_cache package
      with Not_found -> super#is_installed package
  end


The Python code in 0install maintains a cache of the dpkg database for quick access. The OCaml can query this cache, but can’t (currently) update it, so if the cache is out-of-date then it must fall back to the Python code.

This code doesn’t compile:

("super" in "super#is_installed package")
Error: This expression has no method is_installed


If you’re used to other languages, you may have assumed, like me, that class python_fallback_distribution : distribution means
“python_fallback_distribution extends distribution”. It doesn’t. It means that the class type of python_fallback_distribution is identical to that of distribution. Therefore, python_distribution can’t see the is_installed method, since it was virtual in distribution.

The solution here is simple: remove the : distribution bits.

In fact, we don’t need a class for debian_distribution at all: a simple object would do (we can still inherit, we just can’t let others inherit from us):

1
2
3
4
5
6
7
8
9
let debian_distribution =
  let check_cache _ = ... in
  object (_ : #distribution)
    inherit python_fallback_distribution "Debian" as super
    method! is_installed package =
      try check_cache package
      with Not_found -> super#is_installed package
  end


Notice that we declare the type of the object as #distribution, ensuring that this is a subtype of it. For a plain object (like this), we could also use just distribution, which would prevent us from adding any extra methods. When defining a class, you’d get an error if you did that, because restricting the type to distribution would prevent subclassing in some cases (e.g. adding additional methods). For some reason, if you don’t declare a type at all then it defaults to something strange that sometimes causes confusing errors at compile time.

Problems with classes

Using classes causes a few extra problems. For example, this object

1
2
3
4
5
6
let nat_classifier =
  object
    method classify x =
      if x = 0 then `zero
      else `positive
  end


has type

< classify : int -> [> `positive | `zero ] >


However, if you try to turn it into a class, you get:

Error: Some type variables are unbound in this type:
     class nat_classifier :
       object method classify : int -> [> `positive | `zero ] end
   The method classify has type int -> ([> `positive | `zero ] as 'a)
   where 'a is unbound


OCaml can see that this method only returns `positive` or `zero`, but that may be too restrictive for subclasses. e.g. an `int_classifier` subclass may wish to return `positive`, `negative` or `zero`. So you’ll need to declare the types explicitly in these cases.

Update: Sorry, the above is nonsense (as pointed out in the comments). You’ll get the same error if you just try to name the type:

# type t =  < classify : int -> [> `positive | `zero ] >;;
Error: A type variable is unbound in this type declaration.
In method classify: int -> ([> `positive | `zero ] as 'a)
the variable 'a is unbound


The type of the plain object is polymorphic (because it contains a >, which indicates a (hidden) type variable). This allows it to adapt in certain ways. For example: if you had some code that expected to be given the type [`positive | `negative | `zero] then our object would be compatible with that too (although it would never actually return negative, of course).

To fix it, we can either specify a closed (non-polymorphic) return type:

1
2
3
4
5
6
class nat_classifier =
  object
    method classify x : [`zero | `positive] =
      if x = 0 then `zero
      else `positive
  end


Or we can list the type variable explicitly (allowing it to remain polymorphic):

1
2
3
4
5
6
class ['a] nat_classifier =
  object
    method classify x : 'a =
      if x = 0 then `zero
      else `positive
  end


Another example:

1
2
3
4
let file =
  object
    method read_with fn = fn file_contents
  end


has type

 < read_with : (in_channel -> 'a) -> 'a >


(i.e. it passes the open file to the given callback function and returns whatever that returns)

But if you try to use a class, you’ll get:

Error: Some type variables are unbound in this type:
         class file : object method read_with : (in_channel -> 'a) -> 'a end
       The method read_with has type (in_channel -> 'a) -> 'a where 'a
       is unbound


Again, you need to give the type explicitly in this case. Here, we probably want to use “universal quantification” to make the class non-polymorphic:

1
2
3
4
5
class file =
  object
    method read_with : 'a. (in_channel -> 'a) -> 'a =
      fun fn -> fn file_contents
  end


Conclusions

The answer to Stack Overflow’s When should objects be used in OCaml? starts:

  As a general rule of thumb, don’t use objects.


Indeed, the OCaml standard library doesn’t appear to use objects at all.

However, they can be quite useful. In 0install, we use them to abstract over different kinds of restriction (version restrictions, OS restrictions, distribution restrictions), different platform package managers (Arch, Debian, OS X, Windows, etc), and to control access to the system, using real_system, dryrun_system (which wraps a system, forwarding read operations but just logging writes, for --dry-run mode), and fake_system for unit-testing.

The main things to remember are that:

      You often need to declare types explicitly, as the automatic type inference often can’t infer the type, or infers an incompatible type.
  
      Classes and class types are about inheritance (the API exposed to subclasses), while object types are about the public API.
  


There are still some things I’m not sure about:

      Is there any disadvantage to using plain objects rather than classes (when inheritance isn’t needed)? Is it considered good style to use classes everywhere, as the tutorial does?
  
      When declaring argument types, whether to use (system:system) (I need a system object) or (system:#system) (the type of objects from subclasses of system). In general, I don’t understand why we need separate types for these concepts.
  

Hide
        
      
                    by Thomas Leonard at Sep 28, 2013 
      
      
    
  


       
                  Feedback requested on the OCaml.org redesign
      (Amir Chaudhry)
    
    
                                There is a work-in-progress site at 
ocaml-redesign.github.io, where we’ve 
been developing both the tools and design for the new ocaml.org pages.  This 
allows us to test our tools and fix issues before we consider merging 
changes upstream.

There is a more detailed post coming about all the design work to date and 
the workflow we’re using, but in the meantime, feedback on the following 
areas would be most welcome.  Please leave feedback in the form of issues on 
the ocaml.org sandbox repo.  You can also raise points on the 
infrastructure mailing list.

      OCaml Logo - There was some feedback on the last iteration of the 
logo, especially regarding the font, so there are now several options to 
consider.  Please look at the images on the 
ocaml.org GitHub wiki and then leave your feedback on 
issue #16 on the sandbox repo.
  
      Site design - Please do give feedback on the design and any glitches 
you notice. Text on each of the new landing pages is still an initial …Read more...There is a work-in-progress site at 
ocaml-redesign.github.io, where we’ve 
been developing both the tools and design for the new ocaml.org pages.  This 
allows us to test our tools and fix issues before we consider merging 
changes upstream.

There is a more detailed post coming about all the design work to date and 
the workflow we’re using, but in the meantime, feedback on the following 
areas would be most welcome.  Please leave feedback in the form of issues on 
the ocaml.org sandbox repo.  You can also raise points on the 
infrastructure mailing list.

      OCaml Logo - There was some feedback on the last iteration of the 
logo, especially regarding the font, so there are now several options to 
consider.  Please look at the images on the 
ocaml.org GitHub wiki and then leave your feedback on 
issue #16 on the sandbox repo.
  
      Site design - Please do give feedback on the design and any glitches 
you notice. Text on each of the new landing pages is still an initial draft 
so comments and improvements there are also welcome (specifically: Home 
Page, Learn, Documentation, Platform, Community). There are already a few 
known issues, so do 
add your comments to those threads first.  
  


Hide
        
      
                    by Amir Chaudhry at Sep 24, 2013 
      
      
    
  


       
                  Liveblogging OCaml Workshop 2013
      (SRG Syslog)
    
    
                                I'm here bright and early at the OCaml 2013 workshop at ICFP with Heidi Howard, Leo White, Jeremy Yallop and David Sheets!  There's a packed session today with lots of us presenting, so we'll swap over editing this post as the day progresses.  I also need to pop out to attend a panel on the future of Haskell so I'll be missing the bindings session, sadly!

Accessing and using weather data in OCaml
Hez Carty from MDA Information Systems on doing large-scale weather data processing.
He started with OCaml in graduate school and was looking for something better than Python, whereas they are building systems that need to run for many years with high stability.  He's not from a formal CS background but from the physical sciences, and found that the static typing made it easy to explore the language features.  The native code output is also important, and he finds predictability very important in day-to-day engineering.  The simple FFI also made a big difference.
They do weather data ret…Read more...I'm here bright and early at the OCaml 2013 workshop at ICFP with Heidi Howard, Leo White, Jeremy Yallop and David Sheets!  There's a packed session today with lots of us presenting, so we'll swap over editing this post as the day progresses.  I also need to pop out to attend a panel on the future of Haskell so I'll be missing the bindings session, sadly!

Accessing and using weather data in OCaml
Hez Carty from MDA Information Systems on doing large-scale weather data processing.
He started with OCaml in graduate school and was looking for something better than Python, whereas they are building systems that need to run for many years with high stability.  He's not from a formal CS background but from the physical sciences, and found that the static typing made it easy to explore the language features.  The native code output is also important, and he finds predictability very important in day-to-day engineering.  The simple FFI also made a big difference.
They do weather data retrieval and use HTTP/zeromq as a message bus to do language interop, and a workflow management tool to handle dependency graphs.  They take weather models frmo all these sources, and do various analyses (principal component analysis, probability models for precipitation via Monte Carlo simulations with a first order Markov chains).  More generally, there is a bunch of processing for climatology and data extraction and insertion.
A NAM forecast plots the geo data of the USA, and overlays semitransparent layers (e.g. clouds) that are stacked up to get a fake satellite view of the earth, and then there are simulated radar returns where the model (e.g.) rains.  In addition to the structured formats, there is raw streaming data from sensor stations that send back raw binary data such as precipitation events, which is written as a small OCaml library to parses them into more structured formats.
The big data formats are HDF4 and GRIB, which are binary data formats. There are mature C libraries to parse them and lots of existing analysis tools (e.g. wgrib which can extract lat/longs from a big file).  However, the tools arent great for doing larger scale analysis, as shelling out all the time for thousands of points is not ideal -- therefore they bind to OCaml to do the analysis in-house.
The GRIB bindings use int, float and float arrays and bindings are entirely hand-written <i>(I'm looking forward to the ctypes talk! --anil)</i>.  They expose only an OCaml friendly interface that is safe for callers.  The speaker now shows several nice graphs from CloudSat tracks from earth orbit, and how they take intersecting profiles to deduce things about what multiple satellites are seeing.
They're mixing lots of different tools and satellite sources -- derived information from teleconnections (air surface pressure and how it varies in terms of oscillations between the north and south Atlantic).  All the analysis is written in OCaml, and he had to work with several scientists who didn't have access to their initial source code in many cases, so the experience of working with his fresh system was very positive.
Challenges:
standard issues when interfacing with C (type mismatches between C and OCaml in particular, and the question of sharing vs copying).  The worst failure mode is silent failure with bad data, which leads to bad science!
"Getting used to the functional way of life" can tough, as is knowing when to balance elegance with the speed benefits of not copying hundreds of megabytes of data around can be tough.
Development environments other than Emacs and Vim are poorly supported.

The overall project has been very successful, and the bindings for HDF4/GRIB API bindings have been used to process 1TB+/day for over 5 years now.  It's based on Bigarray and mmap, and the FFI makes getting up and running quickly.
He finds OPAM with local repositories fantastic, and used GODI and odb/oasis-db before that.
Q: how did he do the cool plots?
A: plplot bindings on github
Q: Anil: did you move from Fortran
Q: Nate Foster: terabytes of data makes me wonder about GC?
A: going through and folding over data generates huge amount of garbage and slows things down considerably. If the project is one-off, he tweaks the GC or manually call the GC more often. Really depends on length of code.
Q: The data processed is huge, so what was the parallel computation approach?
A: The task distribution system doesn't need a lot of parallelism, as he can work on one file at a time or chunk the files manually and aggregate the results.  He's looking forward to doing some semi-shared memory parallelism.
Q: Pierre Chambart: Is the numerical code in OCaml or C?
A: LAPACK in OCaml, but the Monte Carlo is pure OCaml and interpolation is in pure OCaml too.  Unless speed is absolutely required, it's written in OCaml.
Frenetic Network Controller
Arjun Guha about their multiyear project to program software define networks. When he started on this, he thought that networks are just a series of tubes! The reality is very different though, as there are numerous intermediate boxes (firewalls, routers, wireless auth). The real problem is that each of these intermediate boxes runs specialized, vendor-specific software and are configured independently via some proprietary CLI. They're difficult to configure and difficult to statically reason about.
SDN helps with this by introducing standardized programmable that can deploy in-network features, and a logically centralized controller (a big beefy server) that enables reasoning about whole-network behaviour. SDN is a hot topic right now (lots of commercial startups, 200+ people at HotSDN, generally very busy).
There are several Openflow controllers out there: Java (Floodlight), Python (Pox), C++ (Nox) and Haskell (Nettle), but Arjun introduces a novel OCaml programmable switch!
Openflow works via a controller "faulting" in new packets that are routed to the controller when unknown packets show up on the wire. The source code consists of a generic Openflow packet parser (its a binary protocol), and a generic packet parser (e.g. TCP/IP) and a routing engine.
First library: ocaml-packet that deals with deserializing TCP/IP/ARP/ICMP/Ethernet/802.1Q, and only depends on cstruct by Anil et al. (shows an example of a C struct for IPv4), and the nice thing that they get network byte order for free (thanks to Pierre Chambarts ocplib-endian of course).
ocaml-openflow uses serialization for OpenFlow 1.0 and 1.3, and is based on a (cleaned up) fork of the mirage-openflow library, using ideas from Nettle about to structure the code. OpenFlow 1.3 is less complete currently but is still quite usable.
The runtime systems for OpenFlow 1.0 and 1.3 listens for TCP connections from switches, and does the OpenFlow handles and keeps connections to switches alive.
The source code uses Lwt and so exposes the monad to external users, e.g.:
type t
val connect : Lwt_unix.file_descr -> t option Lwt.t
They have a tutorial for non-OCaml hackers, and found teaching OCaml, Openflow AND Lwt was too much for new users. Therefore they built a new controller called Ox that hides much of the details of Lwt threading. It still uses them internally (Lwt, ocaml-openflow) but is inspired by Ox.
Therefore they can build simple openflow tutorials using Ox, and advanced controller such as Frenetic can be built directly using the low level libraries. Good for beginners and weathered OCaml hackers alike!
The problem with the controller seeing every bit of data is that it gets flooded by packets if it doesnt insert rules into the switches itself. Every switch has a flow table that has priorities, patterns and actions, and tries to take action without bothering the controller. At start of day, the controller does see everything, but eventually flows settle down. The problem they are solving is avoiding having two copies of the program on both the controller and the switch, and figuring out how to deal with race conditions across the two. The simple, logical structure of the OCaml program gets torn apart if they try to write these two parallel logic rules by hand.
They deal with this in the Frenetic network controller. This is a DSL for programming OpenFlow networks, and it has boolean predicates, several policy composition operations and compiles to OF flow rules. It implements ideas from several papers (ICFP/POPL/SIGCOMM/PLDI) and is fully open source on GitHub. (frenetic-lang/frenetic).
They made a transition from Haskell to OCaml for a few reasons. They specified several parts of the Core compiler and runtime in Coq, and found Haskell/Coq extraction very painful (specifically, they had to abandon Coq functors, which had helped them discover key algebraic properties for their work on Kleene algebras). They found it easier for their collaborators to use OCaml than Haskell in general too.
Lwt programming is mind boggling for beginners although the syntax extension does help, and the lack of multicore is a future problem as the controller needs scale up. Overall, they find that hacks at any layer are possible now and lots of projects have spun up!
Q: multicore is a common criticism, but could you give an example of how you would actually solve this is you had shared memory vs a fork model?A: There are several networking algorithms in the literature, and its a barrier for consumers. Nate notes that they could write one by hand but there's a time issue there and would rather have it automagically.
PHP Program analysis at Facebook
Pffff is all about deadcode removal, test coverage, checking for undefined function and use of undeclared variables, and syntactical grep rules. They also do taint analysis via abstract interpretation, type driven flow control and eventually (via Monoidics) a separation logic model (they use OCaml too).
This talk is about static analysis of a huge codebase (5m lines+ of PHP code is scary!). Since he had a lot of big monitors, he built the tools to do analysis over huge codebases. He demonstrates this on the OCaml compiler itself (camlp4 is huge! boo!!! -anil). They can also zoom in in realtime to each box and eventually see the content of the file itself!
The goal is NOT to build an editor, but to analyse and explore large code bases. It opens and editor if required any time. The idea is to transform semantic program analysis into a visual formt that's more easily digestible. Just like in Google Maps where you see the big important roads and only the details when you zoom in. He uses this tool all day on his own codebase too, and gradually realised that when you want to learn a new codebase, you want to start to learn the software architecture first, and then want to figure out the dependency graph before seeing details.
He then shows an interactive grid map that explains the dependencies of whole subdirectories of the OCaml compiler, and drill down to understand what they're for.
Audience q: did you consider a flare graph? a: others have done edge graphs and graphviz etc, and none of these scale to quite the heights that this particular grid done. The ordering of the grid is also the transitive order of dependencies, so it starts with the list of simple dependencies (stdlib) and ends with the toplevel (depends on everything). You can spot for example the OCaml dev rule that things in stdlib are only used by the compiler.
With good software, the top right of the grid is full, which is forbidden by OCaml (no cyclic module deps). With other software, the situation is far worse (audience laughter).
There are many other tools (CodeQuery, stags, sgrep/spatch and scheck).  There are plugins for PHP, OCaml (using the cmt options) and Java (using Joust), C/C++ (using clang) getting all the semantic information from varied source code.  There is a plan to move it to the web (from a GTK interface) by using Ocsigen.
 
Conclusion: CodeMap is a scalable semantic source code vis/searcher for huge codebases, and codeGraph visualizes dependencies.   Future work is to minimize backward deps.
All on Github at github.com/facebook/pfff.git
Q: can you insert other metadata such as Linux perf output?
A: yes! (I think)
Extension points: an alternative to camlp4
Leo White talking about alternatives to camlp4. Why? Camlp4 is very complex and it comes due to its support for an extensible grammar. Did a wg-camlp4 working group to figure out how to evolve common uses towards a more easily parsed standard, to reduce the day-to-day dependency on camlp4. After 400 emails, a consensum emerged.
sexplib as an example: the old version was
type t = {
  float: int default(42);
  bar: float;
} with sexp
The annotated version is:
type t = {
  foo: int; [@default 42]
  bar: float
} [@@sexp]
This has annotation data that is passed straight through to a transformer, with no extensions to the grammar. Thus the sexplib transformer can understand with @default 42 means, and editors can easily hide or fold annotations without having to understand their details precisely.
The compiler ignores any AST extension fragments that remain after all the transformers are done.
sedlex
[%lexer
  match foo with
  (Range ('a','z') | Range(1000,1500)), 65 -> Foo
  | Star (Range('a','z')) -> Bar
]
These are genuine AST nodes and so if these aren't handled by the transformer, an error will result. Another example is
properly foreign syntax, via COW (Caml on the Web)
let world = "world" in
let html =  [%html {|<h1>Hello $str:world$!</h1>
|}]
There's also a shortcut convention for the syntax, such as:
match%lexer foo with
 {|
There's a shorthand form to avoid have lots of brackets due to deeply nested lets.
[%lwt let x = f () in
[%lwt let y = g () in
These have all been implemented by Alain Frisch and merged onto trunk (in OCaml 4.02.0dev). Thoughts and feedback are very welcome on the wg-camlp4 list
Q: how does this affect the quality of error messages?A: Leo -- do you mean error messages from the extension itself?Q: what if there is an error in the extension?A: The 42 that was in the default has an AST fragment and so the transformer will transplant the location. A sensible transformer will therefore be nonlossy.
Q: can you chain transforms?A: yes, thats why we are going with an AST fragment rather than a string, so chaining is easier. If youve got some macro expansion transformer and you want to use it with the sedlex example, they are executed in the other they are given on the command line. The next transformer will just see the resulting AST fragment, and therefore should compose between than camlp4 extensions have in the past.
Q: do you have ideas about hygiene as Scheme does?A: Not particularly in this case, although Leo has some ideas for future systems.
Q: Ashish Agarwal -- do existing extensions need to be reimplementedA: Good thing about all these existing extensions is that they have an existing parser, so you can take your existing implementation in camlp4 and mechanically translate into an extension_points version!
Q: Yaron: is anyone thinking about good libraries (hygiene etc)A: Alain Frisch has merged in ppxtools into OPAM stable (that depends on 4.02.0+)
Q: Will this improve performance over camlp4A: Parsing is an expensive process and this is still doing marshalling. The cost is often the case that we are shelling out to this other process, so we still need to look into improving performance in other ways.
GPGPU programming with SPOC
M Bourgoin presents his PhD work on SPOC.
GPGPU is odd hardware: several mutiprocessors, dedicated memory, connected via PCI-E and transferred between a GPU and host using DMA. Currently hardware has 300-2000 cores vs 4-16 CPU cores (wow). However this demands a very specific progamming model to take advantage of all this hardware.
Shows an example of a vector addition in openCL
__kernel vac_add(global const double *c, global const double *a, global double *b, int N)
int nIndex = get_global_id(0);
if (nIndex >= N)
  return;
c[nIndex] = a[nIndex] + b[nIndex]
To actually use this though, you need a massive program to set up the kernel and execute it. It does the transforms, initialization, scheduling and so forth.
OCaml and GPGPU complement each other, since we have sequential high-level OCaml and highly parallel and low-level GPGPU. So the idea with SPOC is to use OCaml to develop high level abstractions in the GPGPU and program it from OCaml.
SPOC is a stream processing engine in OCaml that emits Cuda but lets it be more hardware independent.
let dev = Devices.init ()
let n = 100000
let v1 = Vector.create Vector.float64 in
let v2 = Vector.create Vector.float64 in
let v3 = Vector.create Vector.float64 in

let k = vector_add (v1,v2,v3,n)
let block = {blockX = 1024; blockY=1; blockZ=1}
let grid={gridx=(n+1024-1)/1024; gridY=1; gridZ=1}

let main()=
random_fill v1;
random_fill v2;
Kernel.run k  (block,grid) dev.(0);
#printf of result
SPOC takes care of transferring the kernel to the GPU and back and handling the scheduling and exchange of data. Vectors are very similar to Bigarrays in that they are external memory.
We want: simple to express, predictable performance and extensible libraries, and for current high performance libraries to be usable more safely.
Two solutions: interop with Cuda/OpenCL kernel (better perf and more compatible but less safe). There is also a DSL in OCaml called Sarek, which talk focusses on.
Sarek vector addition has an ML like syntax, and the language features monomorphic, imperative, toplevel and statically typed checked (with type inference). Its statically compiled to OCaml code and dynamic compilation to Cuda and OpenCL.
SPOC+Sarek achieves 80% of hand-tuned Fortran performance and SPOC+external kernels is on par with Fortran (93%). It's type safe, 30% code reduction, memory managed by the GC and there are fewer memory transfers. This is ready for the real world and has great performance! Available on OPAM.
Great performance, portable, great for GPU and for multicore CPU and nice playground for future abstractions.Who can benefit?
OCaml programmers -> better performanceHPC programmers -> much more flexible and extensible than current state of the art.
Future work: Sarek needs custom types, function decls, recursions, exceptions, and to build parallel skeletons using SPOC and Sarek.
Why not use Sarek for multicore? Well, here's the results!
ParMap: data parallel and similar to OCaml mapfold
OC4MC: posix threads, compatible with current OCaml code
SPOC : GPGPU kernels on a CPU, mainly data parallel and needs OpenCL.

Simple program to calculate power series and matrix multiplication.
Power: OCaml 11s14, Parmap 3s30 SPOC <1sMatmul: OCaml 85s OC4MC 28s and SPOC 6.2s
Significant improvements using SPOC -- 11x for power series and 12x for SPOC. Running on a quad core intel i7.
SPOC is available via OPAM and compatible with x86_64 on Linux and MacOS X, and speaker would love feedback.
Q: Anil - how hard is OpenCL to installA: easy if you know your hardware model and to the vendor website. Harder with multiple GPUs though.
A: if you want to target CPUs, you need to target OpenCL and not CUDA.
High level performance optimisations for OCaml
Pierre Chambart at OCamlPro (funded by Jane Street). OCaml is fast, but not an optimizing compiler (it has predictable performance, good generated code, but what can we do to make it go faster?).
A small modification to high level code shouldnt influence the low level bits.
let f x = 
  let cmp = x > 3 in
  if cmp then A else B

let g x =
  if x > 3 then A else B
The second bit of code is faster than the first here since 'g' is spotted by the compiler peephole optimizer.
let g x = 
  let f v = x + v in
  f 3
Abstract code should be compiled less abstractly. Above, we want to inline f, but a closure is still allocated per call unnecessarily. But we also dont want the compiler to be too smart and lose predictability!
Currently compiler pipeline has a parsetree - typed tree - lambda (untyped) - clambda (lambda+closures) - cmm (simple C like) and then mach (instruction graph similar to llvm) and then lin (assembly like).
Details: typed tree to lambda (construct elimination) - lambda (high level simplication such as reference elimination and pattern matching generation) - lambda to clambda (inlining, constrant propagation and book keeping in an intricate recursive loop) - clambda to cmm (unboxing, lots of peep hole checks for boxing/unboxing and tagging/untagging) - cmm to mach (instruction selection and some more peephole) - mach (allocation fusion, register allocation and scheduling)
Where do we do high level optimizations then? For example, where do we do inlining (since it allows other transformations to apply). typedtree is too complicate, lambda (want inlining, simpler with closures), clambda (is too difficult to improve, and Pierre did try) and cmm (good for local optimization but not global) and mach is architecture specific.
They are adding a new intermediate form between lambda and clambda known as flambda. This needs a highlevel view of values, simple manipulations, explicit closures and explicit value dependencies. lambda to flambda introduces closures, and flambda to clambda is mainly book keeping (and preparing cross-module information). The magic for optimization will be in flambda-flambda passes, and not in the flambda to clambda layers.
Shows several examples of inlining, simplication and dead code elimination and why its significantly easier in the flambda form for simple microexamples. Also shows lambda lifting!
Change the performance model. Now its WYSIWYG, but we want some sort of understandable compile-time evaluation. For example, consider a map function as a metaprogramming function that you want to execute at compile time without all the intermediate closure allocation.
Add build_test to your OPAM to make it easier to test the compiler, and Obj-using code is very likely to break if you do certain mutations behind the compiler, or dead code elimination might happen if you use a value behind the compiler's back.
Q: SimonPJ: why is it easier to do closure conversion first, as it's a lot easier in GHC to do the opposite. There are now two ways to get a value in scope doing it the other way.A: In Haskell you have everything in your closure, since you get the information from the closure.Q: Think of lambda calculus with letA: We do flow analysis to get around this!Q: This is quite complex. Is it whole program analysis?A: Restricted to a single compilation unit, and a few other cases, not cross-module yet.
Q: Yoann: if the prediction is "its always faster" then why do you care about predictability?
Replacing the format string hack with GADTs
Benoit Vaugon talks about how to improve the typing of Format strings in OCaml by using GADT encodings. He stats with showing basic examples of the Printf module, and advanced examples that include the "%a" callback, date scanning and hex dumping.
The OCaml type checker has an inferred type that encodes the details of the format string in a "format6" type that has lots of type parameters that describe the format string contents.During type checking, the literal string is parsed and there is manual inference of the format6 type parameters. At runtime, the format strings are represented by the string from the source code.
During printing functions, it parses the format and count parameters and accumulates parameters. It then extracts and patches subformats, and finally calls the C printf function. Scanning is more complex but broadly similar.
There are quite a few problems with this. There are 5 (!) format string parsers (two in Printf, two in Scanf, one in the type checker) and there are lots of little incompatibilities as a result. For example
Printf.printf "%1.1s" "hello"
Results in Invalid_argument exception despite the static type checking, with a useless error message.
There is a weakness in the type checker too, for example
Printf.sprintf "%s.+f" 3.14
results in the format string being printed.
You can also segfault from there, such as
Format.printf "@%d%s" 42 "hello"
will segfault.
Speed is also a concern, as parsing the format at runtime is slow, and reparsing is required by another slow C printing function. Lots of memory allocation is required.
The new implementation sorts all this out by using GADTs to represent the formats. The format6 type is now concrete and not magic predefined. Formats now also become statically allowed and not multiply passed around dynamically.
The new implementation (which he shows) is a fairly epic GADT but quite regular when broken down into pattern clauses and examined with respect to the old implementation.
Issues: evaluation order changes slightly, as for printing functions the parameters are accumulated before printing. String_of_format is implemented by "%identity" and in the new implementation, we need to change the representation (either regenerate the string from the GADT or implement formats by a tuple).
There's now only one format parser (down from 4-5) for the standard library and OCaml type checker
type ('b,'c,'e,'f) fmt_ebb = Fmt_EBB:
 ('a,'b,'c,'d,'e,'f) CamlinternalFormatBasics.fmt ->
 ('b,'c,'e,'f) fmt_ebb
val fmt_ebb_of_string : string -> ('b,'c,'e,'f) fmt_ebb
val type_format: ('x,'b,'c,'t,'u,'v) format6 ->
   ('a,'b,'c,'d,'e,'f) fmtty -> ('a,'b,'c,'d,'e,'f) format6
(this looks verbose but isnt really exposed outside of the compiler.
Another issue is the "%(..%r..%)" construction, since we need to include a proof term of the number of "%r" occurrences. The encoding for this is quite complex.
Performance of the new system is significantly better across the board. A hello world goes from (230ns to 55ns) timing and (732 to 24 bytes allocated) for example.
Q: Oleg did it already in Haskell (ha ha -- editor). He notes that six parameters arent needed, as the GADT is the interpreter over the paramters.A: yes can remove them if we split the Printf and Scanf format, and there is also a format4 which removes several parameters. We also have a problem with error messages since all the type parameters make things really complicated.Q: Oleg: the work of interpreting the stream is pushed into the GADT, but if you write an interpreter over the GADT then we have simpler external types....discussion moves into offline discussion.
Dynamic types in OCaml
Gregoire Henry wants to do dynamic types in OCaml.
Structural type introspection: generic I/O primitives, type-safe (unlike Marshal) and typed (not written in Camlp4) -- these are polytypic functions.
Normal type introspection, for dynamic values (a k/v store or a DSL with dynamic typing), or extensible polytypic functions for abstract types.
This would also give us a common type representation for (e.g.) Eliom services that will be converted to Javascript, or for FFI libraries, and to let the debugger explore the heap with exact typing information.

Probably isnt a single representation that fits all these use cases: We need to preserve abstraction when desired, but also to break abstraction when desired (e.g. by the debugger or a generic printer, but not by accident!), and it shouldn't impose a big performance hit.
Gregoire gives a description of a polytypic printing function
let rec print (type t) (ty: t ty) (v:t) =
  match head ty with
  |Int -> print_int v
  | String -> print_string v
  | List ty -> print_list (print ty) v
  | Sum desc ->
  let (name, args) = sum_get desc v in
  print_string name 
  ...
We define a predefined type "t ty" and a syntax for type expressions "(type val t) of type "t ty". We then introduce implicit type arguments which are optional arguments instantiated at call-site with the dynamic representation of the expected type.
val print : ?t:(typ val 'a) -> 'a -> string
let print ?(type val t) (v:t) = ...

type t = R of int * int | ...
let x = R (1,2)
let () = print x (* implicit arg is (type val t) *)
This is experimental syntax, but is shows how we can keep the representation abstract but the printer can still operate over it since the implicit argument passes the printer around.
The main problem is now how we mix polytypic function and abstraction, without always printing "abstract" instead of the real underlying structure. One solution is an extensible polytypic function.
module M: sig
 type t
 val x: t
end = struct
  type t = R of int * int
  let x = R(22,7) let () = register_printer (external type val t) (fun x -> ... )

end
let () = print x
We register a printer internally in the module, without exposing its internal type externally, but still let it be printed. The internal and external types are different though. We do need to figure out a canonical name for types defined outside the current compilation unit (its absolute path).
By default though, abstraction should consistently introduce new nominal types, but we're not sure how to reference all the of the external names of a given type within its initial compilation unit (open problem). A pragmatic approach to solving this is the manual or semi-automatic creation of runtime "type names".
In conclusion, they have a GADT for structural introspection and a RTT representation with global names.  There is also a type-constructor index association table.  Implicit type arguments (lightweight syntax for calling polytypic function, and explicit type parameters for polymorphic functions).
Q: what's the gain of putting this into the compiler?
A: you may miss language corners, such as firstclass modules or functors, as your "userspace" OCaml code has to pass a type witness into a functor or firstclass module.  Compiler support makes this passing much easier.  The other big benefit is clear error messages from this compiler integration in the long term.  It's not currently easy for beginners, and this is very important to get right if this becomes a supported feature.
On variation, injectivity and abstraction
Jacques Garrigue speaks about an interesting bug in the type checker that needs some language changes. There was PR#5985 which was a serious type checker bug that lost injectivity.
module F(S: sig type 'a s end) = struct
  include S
  type _ t = T : 'a -> 'a s t
end
module M = F (struct type 'a s = int end)
let M.T x = M.T 3 in x
- : 'a = <poly> the type is lost!
After expanding "s", the definition of "M.t" is actually "type _t = T : 'a -> int t" but "'a" is not marked as an existential.
In order to protect against this unsoundness, all variables in type declarations must be bound, either by appearing inside the type parameters or be existentially bound (i.e. only present in GADTs). Inside type parameters, these variables must be injective.
OCaml doesnt do injectivity directly, since it relies of variance inference (since we have subtyping). The variance of a parameter is either explicit for abstract and private types or constrained parameters, or its inferred from its occurrences otherwise.
Constrained type parameters have been present since the very start.
type 'a t = T of 'b constraint 'a = 'b list
Rules for checking variance in this case become much more complicated, since constrained parameters variance must now be explicit The variance of type variables inside constrained parameters must also be weaker or equal the version inside the body of the definition.
type +'a t = T of 'b constraint 'a = 'b list (* 'b covariant *)
type 'a r = 'a -> int                        (* contravariant *)
type +'a t = T of 'b constraint 'a = 'b r    (* Fails *)
(* 'b is contravariant in parameters but covariant inside *)
In OCaml though, the variance of a parameter is allowed to be weakened through abstraction.
module M : sig
  type +'a u 
end = struct 
  type 'a u = int
end
This is correct for the types, but becomes incorrect when used for type parameters.
module F(X: sig type 'a r end) = struct
  type +'a t = T ob 'b constraint 'a = 'b X.r
end
module N = F (struct type 'a r = 'a -> int end)
Even when we assume that r is invariant, 'b is inferred as invariant from the parameter of t, which overrides the covariance of the body. Meanwhile in the module N, the variance is now wrong.
How do we fix this? We need to refine the definition of variance, as traditional variance subumption defines a lower bound on the variance, but we need to add some upper bound information too, to be sure that parameters cannot have a strong variance.
(technical explanation of how to compose variances omitted here)
In OCaml 4.01, full variance inference is done using 7 flags (the last is a special case required for principality). Some uses will no longer type check so programs may be rejected for safety.
In the future, Jeremy Yallop, Leo White and Jacques would like to add injectivity annotations for abstract types.
type #'a s
type _ t = T : 'a -> 'a s t
Add new types for isomorphic abbreviations, as Haskell does
module M : sig type #'a t val f : int -> ['pos] t end = struct
  type 'a t = new int
  let f x = (abs x : int :> 'a t)
end
This is similar to private, but subtyping will work in both directions -- this is useful for efficiency, encoding runtime types and allows coercions to be delayed until the signature rather than in the both (in the example above, the coercion in "f" could be omitted) due to the signature.
Another problem is that one cannot prove the uniqueness of abstract types, and we dont know whether an abstract type is contractive (so even if you use rectypes, the following functor can never be defined)
module Fixpoint (M: sig type 'a t end) = struct
  type fix = fix M.t 
end
Error: the type abbreviation fix is cyclic
One also never knows whether an abstract type may be a float, so the float array optimization may be mismatched.
OCamlot: lots of OCaml testing!
(will post David's slides on the OCaml Labs website since it's one of our talks. Will edit this post when that happens).
Merlin: editor support for OCaml
Merlin is a sophisticated editor assistant for OCaml that provides autocompletion in vim and emacs. They leap straight into an interactive demo that's awesome: autocompletion, module introspection when writing a functor, and so on.
Unlike other tools, this doesn't rely on a CMT file, so code doesn't need to compile (except for source browsing). Your project can have type errors when using Merlin!
It integrates with the ecosystem and has direct support for Findlib. There is no direct support for camlp4 however, with specific support for some extensions. If you want a new shiny extension then talk directly to them
There is build system support for running either omake or Jenga in polling mode (for iterative recompilation). Omake rules can inform Merlin to get a consistent state of the project!
A typical session: Parsing is done in two stages and the OCaml parser has been modified to be much more resilient to errors. The usual typing rules are applied, but passed onto a more "relaxed" typers in case of errors to give some support for incremental parsing.
Coming soon is documentation (ha!) as well as retrieval for ocamldoc comments to further augment hints, as well as even more error recovery heuristics. Some of the patches will also be submitted upstream to improve error handling (by accumulating them instead of printing them directly).
It's available on OPAM under an MIT license via "opam install merlin" and thanks to Jane Street for letting them work on it during an internship there, and all the bug reporters!
Q: Have you considered running Merlin on a remote system and speak the same JSON protocol?A: They haven't, but it really shouldn't be a problem.
Q: Julien V: do we have to build first?A: No you dont have to build firstQ: What happens when you change branches in a projectA: For project-wide information you need to compile and rebuild.
Q: in the second pass of type checking, what does "relaxed" meanA: When typing fails for some branch of the AST, in the simple case we introduce a fresh type variable and attempt to do unification with the result of this. This will never be sound, but lets it make progress and give SOME feedback to the user.
Q: is this a new type checker?A: No its the original type checker with augmented error handling paths to do more unification.
Q: is the relaxed checking really worthwhile? It wasnt a lot of use in gcc since you get a lot of followon errors that are meaningless.A: lots of discussion, opinions split, it can be disabled!
Understanding memory behaviour of programs
Cagdas B doing a PhD on memory profiling. He wants to study the behaviour of OCaml programs and work on decreasing memory footprint and generally reduce GC pressure on real OCaml programs.
Shows Why3 graph (see photograph) and they're running nice finegrained analysis over it on real world analyses. How do they generate these graphs?
$ opam switch 4.00.1+ocp-memprof
$ opam install why3
$ OCAMLRUNPARAM=m why3replayer.opt -C why3.conf p9_16
Patched compiler and then the OCAMLRUNPARAM will generate a lot of sampled snapshots of the OCaml heap. There's no need to change the code or the compilation options. Then install analysis tools
$ opam install ocp-memprof
$ ocp-memprof -loc -sizes PID
The final step analyses all the snapshots. But what is a snapshot? It's a compressed version of the OCaml heap, with location ids, a graph with pointers, and has saved globals (toplevel modules). We obtain the snapshots by computing a linear scan of all the heap values.
The OCAMLRUNPARAM=m forces the program to generate a snapshot after every major GC, or request the program to generate a snapshot via a HUP signal, or via a new "dump_heap" function that's been added to the GC module.
It works by reserving some space inside the standard OCaml value block, and so there is minimal runtime performance impact except when dumping out snapshots after a GC. There's no space overhead since the header size isnt changed. However, it's only available on 64-bit platforms and reduces block size to 64GB. Ran experiment with OPAM and location identifiers are limited.
There's one tool based on the snapshot identifiers and the "memprof" tool gets all identifiers from the heap and the corresponding location, and then looks up the right types from a specific location using cmt files (the typed AST).
Plan for this work is to improve the current framework (aggregate information by type and location) and to recover more types, and build more tools around it!
Q: Roshan: one common usecase is when grabbing locations, its boring to see Array.create and it would be nice to see a backtrace of the execution to see where the allocation happened.A: this is a problem indeed (discussion about the ongoing Mark Shinwell patches too, and previous talk about backtrace analysis from last years workshop).
Benchmarking in Core
Roshan James about microbenchmarking in JS Core. Precise measurementis essential for performance critical code. We want to measure the execution cost of functions that are relatively cheap. Functions with a short execution time that run millions of times in a tight loop.
The essence of benchmarking is to run a time loop (like Time.now) a lot of times. We need to figure out how to compute a batch size to account for the time it takes to run the actual timing measurement. Similar to Criterion for Haskell, where they loop Time.now to figure out how long it takes to compute the time itself, and then run the benchmark function, and compute a full estimate. This works ok, but it requires density plots to let you visualise the form of the error (bimodal, or something else, for example).
There is a lot of noise in microbenchmarking, such as delayed costs due to GC, and variance in execution times is affected by batch sizes. So if we vary the batch size, the variance changes quite a bit since the GC is running in different intervals during the batches. The GC noise has a periodicity though!
They treat microbenchmarking as a linear regression, and then fit execution time to a batch size. They vary the batch size geometrically to get a better linear fit (shows a very linear graph). Theres now no need to estimate the clock and other constant errors, since they are accounted for by the y-intercept which shows the constant costs.
Other costs can also be predicted in the same way, such as estimate memory allocations and promotions using batch size too. This also allows measuring a goodness of fit by using R2 and bootstrapping techniques.
Example use of Core_bech is really easy:just
open Core.Std
open Core_bench.Std
let t1 = Bench.Test.create ~name:"id" (fun () -> ())
Is the identity run for example, and just put something more complex in there. Output is an Ascii_table with lots of columns showing interesting results.
Some functions have a strange execution time, and the observation isnt a great fit. Between runs, they also directly query the GC and directly query how many times some things have happened (minor collections and so on), and include these along with the batch size in the predictor. With multiple predictors, they are really close to the observed results indeed.
(lots of interesting maths about how they perform runtime cost decomposition)
Its available via "opam install core_bench" and they want to expose more predictors by measuring the effect of live words on performance, and counters for major collection work per minor gc. Accuracy of results is interesting as least-squares is susceptible to outliers so they can incorporate the knowledge that the measurement is heavy-tailed.
Hide
        
      
                    by Anil Madhavapeddy at Sep 24, 2013 
      
      
    
  


       
                  Liveblogging CUFP 2013
      (SRG Syslog)
    
    
                                I'm here at the Commercial Users of Functional Programming workshop at ICFP 2013) with Heidi Howard, David Sheets and Leo White.

Marius Eriksen and Michael Sperber are co-chairing this year. Functional programming is a hot topic these days, and this year's program reflects the maturing nature of the program. Record number of submissions, and we could easily have made this a multi-day program.  Videos will be available online after the event.
Keynote is from Dave Thomas on what we can learn from the "language wars of the past".  His talk will cover the business, social and technical ends of building Smalltalk and his "objaholic" experiences with pushing language technology into businesses, where they never had any intention of changing in this regard.  This worked over the years because it made a material difference to these businesses.  Dave still uses the K programming a lot at work.


Dave is responsible for the Eclipse IDE and the IBM Java virtual machines, and has delivered …Read more...I'm here at the Commercial Users of Functional Programming workshop at ICFP 2013) with Heidi Howard, David Sheets and Leo White.

Marius Eriksen and Michael Sperber are co-chairing this year. Functional programming is a hot topic these days, and this year's program reflects the maturing nature of the program. Record number of submissions, and we could easily have made this a multi-day program.  Videos will be available online after the event.
Keynote is from Dave Thomas on what we can learn from the "language wars of the past".  His talk will cover the business, social and technical ends of building Smalltalk and his "objaholic" experiences with pushing language technology into businesses, where they never had any intention of changing in this regard.  This worked over the years because it made a material difference to these businesses.  Dave still uses the K programming a lot at work.


Dave is responsible for the Eclipse IDE and the IBM Java virtual machines, and has delivered software on everything from mainframes to wristwatches.
Rush in the 70s with Symbolics and the amazingly cool hardware, followed by the rush to logic programming that led to his grants disappearing. Midway through the 80s, they discovered we wouldnt get parallelism for free with all this magic. After that came the era of machine learning and knowledge ontology crusades, but they are very specialised bits of tech. There remain sectors where ontologies are useful. Nowadays its the multicore crusades, and FP is emerging as useful.
Every road leads to some form of FP, because the principles underlying it apply. Dave does vector programming, Haskell does this with array transformation.
Dave was talking to Simon PJ outside and claimed that "FP has won!", at which point Simon replied "really?" (laughter). OO techniques can also lead to a lot of tangled code: witness the evolution of Smalltalk to the current generation of languages.
FP technical challenges: what happens when you have a lot of functions, and a group of consenting adults are doing it together. This is called software engineering, or at worst, IT!
Lots of good work in the community on approaching large scale programming. Scala is a bit too complex for Dave (and audience rumbles in agreement or disagreement?). Java and JNI should just die. Erlang and actors are successful to decouple software. But do we really need to hide all our queues, and do we really need to hide our queues? Static analysis of queues are desired.
Want to go fast? Computers like rectangles, all the way down! Array operations and vector processing is the way to go in the long-term.
To go to industry, we need end-to-end toolchains. If you use MS Project, then you probably shouldn't introduce any new tools.
The first commercial customer for Smalltalk in the 1980s was Techtronix, who wanted to embed reusable tech in their oscilloscopes. If you're building a serious product like this, it needed language interop between C++ (the logic sensor) and the Smalltalk. Respect the language cultures among components (anyone remember s-records?). ROM had to be replaced with Flash because noone can write code that doesn't need upgrading (groans from the audience).
With the right tools, programmers will be able to go as quickly as Smalltalk, or at worse Java. We need the work of Simon [PJ] and colleagues on Haskell. Scala is so complex will become the Java (which is the Cobol of tomorrow).
When shipping products, you need to track all the versions of language components in the toolchain. Too many FP toolchains are monolithic and make it hard to track components in the field. JVMs are so slow because they don't have a good sharing model (DLLs).
What Dave would like is version-to-version transformation in languages. Y2K was a problem not because IBM hadn't fixed it in their OS, but because customers had modified their local installations and were pinned 6-10 versions behind! The same will happen for existing FPs that fragment into multiple implementations, so will they descend into multiple version hell or will standards emerge from the community?
Also, lets not leave the data behind. Can we alias data from existing legacy sources into our new languages, and not aim for the perfect representations? Most of our programs are dealing with problems with existing data, not code.
Onto deployment issues. Erlang works very well for the problem domain that it's designed for, but more complex domains exist like OSGI. Hot codeswap and live updates are often viewed as "boring problems" but are vital to real world deployments. Space usage must be paid attention to: how long can a given JVM stay up without leaking? Serialization is important to persist data across versions. If we're going to use "esoteric control structures" like CallCC, tailcalls or tags, the JVM isn't going to get us there.
These days, if you can't stay in your local cache, you will lose several orders of magnitude performance... you'll need directly connected CPU (or SSD!) memory, and static initalization, and language interop (eagerly vs laziness). Functional data structures and immutable collections are still immature, but need more work to convince doubting Thomas' about their scalability in real applications.
Hardware support for these are important. Intel Haswell has hardware transaction support (hence can be used by Azul JVMs etc) but we need more for functional langauge performance. There have been $2bn put into speeding up the JVM to make it as fast as C++. Google has a technology called Pinnacle which uses software write barriers that use VMs that execute directly, with a register model and not a stack model (which is easy to put together but hard to make fast). The new generation of language VMs will support direct execution and support modern functional programming languages.
"We all love types" -- Dave on objects, but not variables ("its easy to be cooperative with that definition!"). Pluggable types by Gilad are really cute, because they move us towards a world where particular features (Dave likes behavioural types) can be woven in. Perhaps can be done in a dependently typed world. Another good candidate are approximate types and dimensional types. When all these are put together though, it all needs to fit together with sound compositional semantics.
When using an interesting PL, Dave wants warnings to be emitted when the compiler makes space/time tradeoffs so that the programmer can decide between elegance and performance (also mentioned that Haskell programmers do this for .
How does language evolution work? Arthur Whitney maintains K, and he ensures that every version of K is incompatible with the previous one. This is a blessing (!) since it avoids a kitchen sink language and ensures a high quality set of libraries. Quite often there's a tussle between languages and libraries because there's not enough coevolution. For example, are closures in Java really useful for the tradeoff in the complexity of the VM? Perhaps we should be looking towards a collection of small correct languages instead of a few partly right and largely wrong languages such as Ada or Java (the Java hate is strong here -- editor).
Those of us that have programmed in vector languages (going back to APL) are largely write once read once languages because the code is so hard to read. One trick Dave does with K is to annotate comments inside expressions and use cold-folding without a point-free style to understand vector codeflow better. Also seeing the environment and current name bindings is extremely useful for teaching new languages.
The real problem is: comprehensions, folds, monads -- WTF? Yet to see anyone from the monad community give a talk to understand monads. It's typically "you dont need monads" and then the word is used multiple times in the talk. If it's not important, why mention it so often? Quite often, the FP experience makes normal programmers feel stupid because they don't break through this terminology barrier. We as a community need to help people break through the barrier of not being formally trained in mathematics for whatever reason ("category theory for breakfast?") and *be productive* in FPs.
Smalltalk was like this in the early days. People see a blank window and aren't sure what to do next. Objects irritated people when they came out, but FP scares people. This is a danger as we go through and wow people with what we can do, but make it look like magic. People change their habits are different rates and we need to understand that. With Haskell, it was odd that he had to get the program "right" (or right/wrong) before it work, as opposed to not working.
Bill Buxton (UI researcher friend) pointed out that people can't much get beyond programming a VCR. 4GLs were pretty much their limit. Dave's done a lot of work on evangelizing Smalltalk, but feels it was still too hard. People just didn't understand how to separate behavior, as the best practices on how to build objects were foreign to most of them. This all settled on some mix of SQL and some glue code, which doesn't require learning a lot of new technologies.
Given there are many different types of programmers, do they all really need the full power of FP? Some subset of features is more useful, with one way to do things that doesn't change from release to release. For example, a simple dictionary keeps changing from Hashmap to concurrent Hashmap to something else, but ultimately is still a dictionary. How do we separate the reasoning over performance and structure and not scare people with both at the same time. Knuth started doing this decades ago with literate code, and we *still* dont have a decent way of writing documentation and code in the same place.
We are still missing a pure FP book that is built in the same style and comprehension as SICP. We need thinks that you look the code and represent things more densely (for example, why don't we use Unicode in the same style as APL symbols to increase the succinctness?). They are working on J programs that move glyphs around on the iPad as an example of this 4GL density.
We need to take all these ideas and *talk more* about metaphors in FP. Where's the "Scala, F#, Clojure, the good parts" and a frank exposition about the antipatterns? Don't lie about where the dragons are, but colour code them appropriately to not surprise programmers.
We need to understand how to design with types. In the vector languages, this is called "shape shifting" as they don't have types, but similar concept. It's also hard to get the community people to talk about shape shifting, and as a result the art is being lost in the new generation of languages. For FP, can the Yodas in the audience help their audience to "trust the types" ?
Q: In the top 20 languages, every language has a free-or-open-source runtime. This will make a huge difference as it lets people experiment.
A: It's not been a barrier in experimentation for Dave, but it has been a barrier for commercial use. Dave would not attribute a particular significance to open source as it's never been a real problem for him (using trial versions and so forth). Open source is a mixed sword, as it tends to product impoverished language teams that don't have enough resources to move forward. The really good virtual machine implementors are commercially polished (c.f. the $2bn behind the JVM performance from earlier). Clojure is a great example where they appealed to the community to pay on their own behalf to donate for a production version. To build a serious production PL, you need a lot of government research money or a major commercial sponsor.
Q: Marius asks about mobile and embedded and why FP hasn't cracked it.
A: they are often late to the game, and it's often not easy to target the runtimes with a high performance runtime. You have to be willing to understand the interface.
Q: Phil Walder says the scariest thing is that if you become a success, then Smalltalk takes the form of C++
A: (or Java!). Being able to maintain the independence of the language creators vs a major player is crucial to conceptual integrity. (Phil) Tools that will migrate between version X and Y and a sustainable ecosystem. It's important to ensure that the creators get paid to maintain -- even the JVM only has 10-20 people in the core team and a vast ecosystem.
OCaml at Facebook via the Hack language
Julien Verlauget is talking about how Facebook is adopting OCaml.
Why PHP is good: fast installation times, lots of libraries, easy to learn, and scales in a parallel way (multiple instances).
However, Facebook doesn't care about any of these features. What Facebook wants is a fast feedback loop between writing code and having it appear on screen. There's no good way to write a unit test for an experience, and instead a fast feedback loop between code and a live site is how people still work at Facebook today.
Performance at the scale of Facebook really matters. 1% can be a huge bump, but PHP is really hard to optimize due to all the dynamic features in the language. Aside from runtime performance, the other concern is development time for engineers, who need to avoid security problems and general bugs in an incredibly large codebase.
The first attempt at this was to scale the runtime via HipHop. The first attempt was to compile PHP into an interpreter (via sandboxes) and then compile production code to C++. Unfortunately this lead to a huge divergence in performance characteristics between the interpreter and compiled code which led to confusion in production.
The current one is HHVM which is a combined model via JIT runtime that patches code at runtime and helps merge these models.
HHVM is still a PHP platform, and a new model called Hack provides a *new programming language* that targets HHVM (which is essentially a PHP runtime).
Hack is a statically typed language for HHVM. It's compatible with PHP and interoperates with no overhead and has the same runtime representation as PHP. It has evolved from PHP, and so if you know PHP, you know Hack. It's designed for incremental adoption via introducing gradual typing (c.f. keynote talk about not scaring programmers not used to types).
function foo(): void {
  $x = 'hello';
  return $x;
}
This returns a readable error message in the style of a statically typed language, since the inference algorithm keeps a witness about every value's type and shows counter examples.
The Hack type system must be annotated with class members, function parameters and return types. Everything else is inferred to minimize programmer burden.
Hack types include:
Nullable such as
?int
mainly because of the huge code base that had no pattern matching, so null propagation is important
Tuples
Closures
Collections such as
Vector<int>
that have generics too
Constraints
Type Aliasing
Extensible Records

Demo of a simple mailbox demo follows. HH has a command line mode that lets you syntax highlight which parts of the source code are dynamically checked, by outputing the code with red if it's potentially unsafe. Awesome!
class Mailbox {
  private $data;
  public function send($data) {
    $this->data = $data
  }
  public function fetch() {
    return $this->data;
  }
}
(very PHP like but now he annotates it with specific types)
class Mailbox {
  private string $data;
  public function send($data) : void {
    $this->data = $data
  }
  public function fetch() : string {
    return $this->data;
  }
}
As a programmer, I now want to know if this has all been statically checked (since the previous checking mode showed several red sections which were potentially unsafe). He runs the compiler with a strict mode that rejects the source if there is any unsafe bits.
He then demonstrates how to propagate null types through the code, and then introduces several subtle errors that are all caught.
An audience members asks what happens if you send both an int and a string to a mailbox, and Julien demonstrates a "mixed" type that covers both (is this some sort of row polymorphism? -- editor).
The response time of the type checker is close to instantaneous (for the developer loop). Even with a thousand files, the type checkers gets back in a second or less with an error message.
To get the tooling right, Hack uses js_of_ocaml to compile the entire OCaml type checker to Javascript and then run in a complete web-based IDE (demonstrates a full cloud9-like IDE they use internally at FB). To work at scale, they created a server that runs persistently in the background and type checks all the files and responds from a memory cache.
At scale, the the autocomplete requires very lowlatency in the webbased editor, and users also use version control (e.g. switching between branches). The server also has to use a reasonable amount of RAM and have a reasonable initalization time. Fundamentally it has to be stable as developers have to depend on their tools.
A vast amount of code is written in OCaml for the tools. Good choice because it is ideal for symbolic computation, excellent performance and can be compiled to Javascript. Good C interop is really good. The main challenge is the lack of multicore, although Facebook works around it with their multiprocess architecture.
Architecture: the master process has multiple processes which uses shared memory to avoid having to go through the master. The underlying buffers are lock free (similar to our Xen shared memory ring! -- editor).
Advice on OCaml at scale:
IPC -- use lockfree structures and think about sockets/pipes instead of just TCP
GC small heap and shared memory is compacted by master
OCaml makes you think hard about shared objects, which is a good thing.

Q: is it open source??A: soon, this is the first time Facebook has talked about it publically.
OpenX and Erlang ads
What is OpenX: (i) An opensource PHP ad server (ii) OpenX is a global company with an ad service featuring a big ad exchange written in Erlang. This talk is about the transition from PHP to Erlang.

1998-2007. it was a conventional PHP management system and MySQL. The company started using it as an EC2-based SaaS ad server and serving up clients successfully.
In 2008, they wanted to mix in an ad exchange, to add liquidity to marketplaces that have empty ads. At the same time, they moved offices to Pasadena and looked at new tech. Switched to Hadoop (Java) and Postgres (for some reason). Poor technical choices were made, such as writing to the database for every request (sometimes multiple times!).
In 2009 as the customers increased, there was a big push to scaling things up for performance (this is when the speaker joined the company). Switched Postgres to Cassandra to move from a relational database to a more NoSQL approach that had the appropriate consistency properties. At this time, the architecture was also decomposed to give them headroom to scale for a while, particularly by adding HTTP load balancers and so on.
They wanted to add even more low latency stuff, such as spot bids for immediate bids on empty slots. Anthony decides that the model was a good fit for Erlang, since one request needed to fan out to multiple backend services with a soft-realtime limit to return a response to the user. It took a few months to prototype an Erlang based solution which worked very well.
After this first experience with Erlang and evangelising it, and pointing it out to his coworkers, they came up with another good problem: analyzing the frequency of how often someone looks at an ad. This required a new database that would hold bespoke data efficiently, and also integrated Riak from Basho to interface with the database to run the query engine. This now holds on the order of billions of keys.
In 2007, they ran into problems with Cassandra due to making an incompatible upgrade which would have required OpenX to shut down their business to upgrade, which was unacceptable. Since they had a good experience with Riak core, they developed a new system to move away from Cassandra and add more Erlang for management stack.
In 2011, they decided to split their UI layer into a more Web 2.0-like UI API. The API continues to be implemented in PHP (with a Javascript UI clientside), but the Erlang evolution continued with a gateway service and marketplace service that are both pure Erlang. The basic problem is ad selection, and the logic for this is written as a DSL implemented in both Erlang and Java to enable code reuse across the tools.
However, Cassandra was still sticking around, and they wanted to move away from it. They implemented a data service layer to abstract away from the database. From the early days, they made a bad (but fast) choice to have clients directly call Cassandra via Thrift, as it was easy. They moved away to a more abstract model to get away from Thrift, and so could change clients. The new system could call both Cassandra and Riak to help evolve with both services running simultaneously and provide a seamless migration path to the new service (in stark contrast to the Cassandra upgrade story from earlier).
They also decided that the PHP use for the API was a bad idea, and so switched to Python. They engaged Erlang Solutions to write them an API router since their core team was busy (an example of external consultants coming in and being useful -- editor). The current system involved RabbitMQ coordination, and Riak's replication logic to move out to the colos for publication.
Currently they have 14-15 services in Erlang 7-8 in Java, and a blend of Python/PHP.
How did a PHP shop end up with so much Erlang? Well, they laid off all the PHP developers when they moved offices (lots of audience laughter).
More seriously, the architectural choices are important:
Cloud based (generic hardware, automated bootstrap, deployment, package oriented development and fault tolerance).
Service based (loosely coupled, single purpose components, pools of components, polyglot so multiple languages/
Cross language tools: Thrift (RPC between components), protobuf (RTB and Riak) and Lwes (logging and monitoring) which was open sourced by Yahoo after being originally written at Goto.com. A system called Framework that provides templates for code layout, builds RPMs/debs and integrates with autotools. It enforces versioning and reproducibility across languages.
Finally, evangelism is really important. If possible fix the game via architectural choices to set the scene for change. Find a project to showcase the technology, and be responsible for the project's success. Make sure to share work, and remember that tools are really important to make it easy for others to use.

Really important is figuring out build toolchain -- it can be so arcane. They did spend time on integrating with Erlang (erlrc inegrates with packaging system, erlstart to manage lifecycle, erlnode for ?).
Hiring: train people rather than trying to hire knowledgeable. Operations have unfamiliar systems to manage, and developers have new concepts and patterns.
How did Erlang help OpenX developers? The language fit the service model very well. The let-it-crash philosphy and supervision trees led to fewer real service outages.
OpenX is doing really well as a business, too: 250+ billion monthly transactions, 12+ billion daily bids, thousands of machines in 5 colos, 300 employees and $150m+ revenue in 2012.
Q: Marius: with a service based solution, how do you library them up?A: Their services aren't hugely fine grained and so usually a module or two (not a lot of state)
Q: Anil: is framework usable from languages? RPM integration seems killerA: It's still esoteric and internals not docced, but would welcome a chat!
BAE systems on Redesigning the Computer for Security
Tom Hawkins speaking on DARPA funded project.
SAFE is a codesign of:a new application langauge (breezE)a new systems programming language (Tempest)A new OSA new processorand security at every level for defense in depth.
Emphasising hardware enforced security since dynamic checking is too much overhead in software, for example for fine-grained information flow control. Hardware IFC handles the most general attack model (scripting down to machine code injection).
Hardware overview: every word of data contains a tuple of (group 5 bits, tag 59 bits and payload 64 bit). The Atomic Group Unit checks atom typs (e.g. instructions, data pointers and streams). The Fat Pointer Unit (confusingly, FPU) checks pointer operations, and tag management unit (TMU).
The label model can be used for efficient hardware typing, and more recently they are using labels for Control Flow Integrity (e.g. to prevent return oriented attacks). Register file is 32 elements deep, memory is accessed through load/store, and every bit of data in the system is a well-formed atom (and so has tag/groups associated with it).
FPU deals with buffer range checks (e.g. underflow or overflows for a buffer).
The TMU is where the magic is. It combines all the operations from the various operations being worked on (instructions, memory) and the TMU acts as a "database" checking that the operation is consistent with local policy, and the machine can continue execution or raise a security exception.
At day 1, they had an outline for an ISA but nothing else. TIARA project was a baseline (Howie Shrobe, Andrw DeHon, Tom Knight) and they had no languages, no hardware and no toolchain.
They started by sketching out an assembly language and then building an ISA simulator. They simulated simple assembly programs, and HW researchers started coding in Bluespec (a Haskell-like FP that Lennart keynoted about at CUFP 2011 in Japan).
The PL researchers started designing Breeze. Plan was to "steal" Andrew Meyers work on JIF, and "Breeze should be done in a couple of months"!. It didn't quite work out that short (laughter).
SAFE assembly is as low level as expected, but it adds some new properties:
frames manage fat pointer bounds
Atomic group declarations on data mark critical sections
Tags on data can be marked (e.g. bootloader private)
Secure closures is code/data with the authority to run code over the data (they call it gates)

Assembly is tedious and so they started with macros. The Breeze interpreter is now running and there was pressure to start building the compiler. The OS guys wanted a language to build the higher level components.
Solution: a SAFE assembly DSL embedded in Haskell that uses Haskell as the macro language and becomes the library for the future Breeze compiler.
Breeze Language: version 7 -- just the datatypes for Booleans was hard and took 4-5 weeks ("this IFC stuff is kind of tricky"). The convenience and modularity of lexical authority passing and one-principal per module is extremely inconvenient!
SAFE Assembly in Haskell looks like a typical EDSL. A monad captures program descriptions, and macros set up data tags and also for better control flow (such as a while loop or ifelse conditionals in the asssembly).
At Year 1.5, the EDSL is still assembly language, no matter how many macros there are. Manual register allocation, calling conventions and data structures. Meanwhile the Breeze compiler was inching off the ground, but there is an awkward transition from high level CPS IR to assembly. There needs to be an intermediate form somewhere. However, the SAFE EDSL in Haskell works great in the compiler backend plugged in seamlessly with the Breeze code generator, which was nice.
Breeze Language is now on Version 12 and continues to answer questions. "What do we do on an access violation?". Simply stopping the machine isn't enough, since "What if I maliciously send you data that you can't access?". "Simple I'll just check the label before I attempt to read it?". "But what if the label itself is private?". This was the big "aha" moment in IFC, since private labels end up being a poison pill in IFC.
At Year 2.0, the Breeze compiler is going through a major overhaul since it needs improvements (the middle IR is improved but not enough). They temporarily shelve the compiler and they realize it wont come to the rescue of the OS. They really needed a higher low-level language than just assembly.
Breeze language v23 has a solution to poison pills: make all labels public! To label data you must specify the label in advance (brackets), but public labels are not compatible with lexical authority passing so they had to relax their original vision a little.
Year 2.5: Tempest is started as the systems PL for SAFE. Imperative with register allocation and optimizations and control of assembly with inlining and user specified calling conventions. It uses the SAFE EDSL to generate assembly (that was a successful DSL) and nicely fills the Breeze IR gap in the compiler for when that is resurrected.
Breeze language 34: delayed exceptions with not-a-value values (Nav).
The Tempest EDSL with inline assembly has a SAFE assembly sublanguage. The SAFE flow isnow the Breeze, Templest and Safe Assembly layers, with EDSLs of each gluing the compiler pipeline together. There is a SAFE ISA simulator and debugger, along with a Bluespec simulator to save on the FPGA debugging cycles. The TMU is an elaborate piece of hardware (an exotic CAM and cache structure) that a lot of the effort has gone into.
Lessons learnt:
designing a higher order IFC language is very hard even with some of the most brilliant PL researchers in the world. Optimal number of researchers is 2-7 rather than any more :) Ben Pierce and Greg Morrisett arguing over very hard problems was a lot of fun in a hard problem space.
Day 1 should have started with Tempest, not assembly. Hard to be productive with assembly code, and tempest is the right level of runtime/processor codesign. Level of indirection provides insulation from a changing ISA.
EDSLs are great for bootstrapping a language and make excellent backend libraries. However, engineers must be comfortable with the host language and not all the engineers were comfortable with Haskell when they started. EDSLs are hard to debug (line number information apparently hard to get out of error messages vs a concrete syntax).
Concrete syntax is more relevnt for some languages than others (e.g. Tempest vs SAFE assembly). The best transition point to a concrete syntax is after developer pressure for modular programming. Once language has modularity, then the switch can be made without disruption.
Would a DSL have helped hardware design? Forever debugging ISS and FPGA instead of raw Bluespec. A DSL describing ISA semantics would have kept is synchronized by generating Bluespec, ISS and SAFE EDSL and Coq documentation.

This has been a huge project spanning a vast number of layers. It has produces a large number of interesting papers including one at ICFP this week on testing non-interference quickly by using QuickCheck to generate programs that don't violate non-interference properties.
FRP at Netflix by Jafar Hussain
Netflix used to have tight coupling between the middle tier and the UI teams. One-sized fits all messages and led to inefficient call patterns.
The plan was to decouple this by giving UI developers the ability to decouple components (or "force!").
Two developer personas at Netflix: Cloud and UI people. Netflix deals with a third of all US broadband traffic, so they need to think at scale but also to run on really tiny cheap devices that eke out all performance. How do they turn UI developers into effective cloud developers who think at scale?
Gave them UI comforts: Groovy, OO API and a Reactive API. Most UI developers (or indeed any developers) can't be trusted with locks. This means that reactive programming isn't enough, which leads to "how do we make parallel programs safe for developers"
Enter Erik Meijer, the legend himself! He asked "Whats the difference between a database query and a mouse drag event?". Database would use functional code and a mouse drag might be imperative. Erik points out that they are both collections.
New JS closure syntax in ES6 is fantastic to make them easier to use. If you imagine a query for well-rated movies, then you need to map over video lists. He then shows a mouse drag event, but points out that the structure of the code is very simpler. Instead of a "filter" operator, he uses "takeUntil" which stops the operation after some predicate has been satisfied.
The iterator pattern is similar to the observer, except for the bidirectional signalling. Iterators can throw next/hasNext/Throwable, but Observers dont have a similar completion semantic. What happens when we add these to the Observer? We end up with "onNext" and "onCompleted" and "onError", so now the Observer has comparable semantics to Iterator, so the only thing that changes is the direction of the errors.
Some platforms also have an Iterable. There is now also an Observable to which you can pass an Observer, in the same vein as passing an iterator to a Java Iterable. We also add a Disposable to both, so that when the IO stream has terminated, it can be stopped immediately without having to junk the rest of the data.
Erik and his team at MSR demonstrated that these patterns are dual, so that Iterable and Observable are dual! These are the Reactive Extensions libraries that is a combinator library for the Observable type. It's open source and has been ported to a number of other languages than C# (including Java).
Observable Monad is a Vector version of the Continuation monad. Null propagation semantics of the Maybe monad and Error propgation semantics of the Either monad. Produced and consumed with side-efects and acts as a fault line between pure and impure code. It can be composed functionally and cancellation semantics are clean. It can be synchronous or asynchronous which is a common mistake (to assume that Iterable is async -- either can also be sync simply by calling the functions immediately).
If you Map over Observable, then
(rapid transcript)
var map = (observable, func) =>
  { forEach: observer => 
    {
    var subscription =
     observable.forEach
       onNext : item => oserver.onNext(func(item))
       onError : error => observer.onError (error)
       onCompleted () => observer.onCompleted()
We can use this to come up with a single reactive type for cloud and UI developers! Jafar demonstrates example of social notifications in the middle tier, but using Observable to filter notifications from a composition of the socialService and messageService. Both underlying services are returning observables, and are non-blocking and completely parallel. Both can execute synchronously or asynchronously and decide which knobs to turn for their particular scheduling needs. The programmer doesn't need to care about the details.
Another example is search autocomplete, which is surprisingly hard. Need to decide how often to send requests (throttle them to be not per-key) and also deal with concurrent responses from keypresses coming in out of order.
var res = 
  keyPresses.throttle(20).flatMap(
    search => 
      getSearchResults(search).takeUntil(keypresses));

res.forEach(resultSet => listBox.setItems(resultSet)
This code fragment does all this composably and handles all that logic. It's throttled, ordered correctly and terminates until the keypresses stop (with respect to the throttling policy.
The full tier is a UI (huge amount of mutable state), dropping in to a distributed functional style, and heading into the data tier. Recall this is at huge scale.
Wins: Rx is open sourced from MS and ported to Java (Observable) and generally better tooled now.Evangelism comes up again since you can't assume the best technical solution will win. Practice public speaking and focussing on soft skills! Speaker got better in a short amount of time at distilling the essence of the benefits, and explaining to the target audience why something matters (e.g. to a UI developer vs a cloud developer as he explained earlier).
Challenges: Training/Hiring. Netflix didn't fire all their UI developers as the previous speaker did (audience laughter). Instead, they leveraged their skills at eking out mobile performance, and taught them alternatives: functional programming, vector programming and reactive programming. Be available to help out people (developers work weird hours!) and interactive training exercises really help too. Also help them understand when to apply each technqique for a particular programming task.
Some of the Javascript programmers had some intuition for these techniques from things like Promises. It wasn't quite the right answer, as sometimes you need vector versions for performance instead (since it degenerates to a scalar version by passing a vector of size 1). Netflix dont bother teaching scalar, only vector.
Although the programmers are used to mapreduce, the embedded version of this is a lot harder for them to grasp -- it's just a programming style and not just magic in Hadoop. Hiring process adapts to spot the competent functional programmer.
bind/flatMap/concatMap/mapcat/mapMany are all monadic versions with subtle semantics and so they use interactive training exercises to train developers in their runtime semantics.
Challenges: performance needs some chunking for lowend devices (vector a manually chunked scalar monad) and is best applied to less chatty event streams. It worked for search on lowend devices since it takes so damn long to type anything in, so a less chatty UI stream (audience laughter). "We are screwed if the hardware UIs improve".
Type-unsafe flatMap is easier to understand but needs to be used carefully, but is useful on lowend deviecs.
http://github.com/Reactive-Extensions/RxJS has all the source, so give it a try!
Q: How do Databinding in Rx?A: No templates, but many other tiers before the view layer are bound together. We introduce an Observable with both pure and impure code (an Observable type with a value property). However they learnt how to do this directly since Observable's lazy default is the wrong for data binding. Implementing a different type that is strict and publishes the result directly.
Medical Device Automation using message passing concurrency in Scheme
What does a molecular diagnostic device do and what does Scheme have to do with all this?
Detects the presence of specific strands of DNA/RNA in sample and then does necleic acid extractions and purification. Sample are excited using lasers as they undergo PCR which results in spectrographs.Device contains 19 boards with temp, motor control with many parts that move simultaneously at very high precisions. (author shows demos of the motors running and playing music from the system!)
Client is written in C# and WPF and is thin/stateless. Sever is written in Scheme and Erlang with OTP embedded. Supervision structure to isolate hardware failures. A DSL provides the glue logic.
Scheme is exceptionally clear and simple syntax, and they love the hygeinic syntactic abstractions for their DSL and the first-class functions and continuations are key. Arbitrary precision arithmetic is important for result calculations.
The Erlang embedding in Scheme is how they interop. They embed records by referring to structured data by field name instead of field index. This allows for copying a record and changing particular fields.
There's no pattern matching built into Scheme and enables matching on all the usual datatypes (strings, lists, vectors and records). It also allows matching the value against a local variable and/or binding values to local variables in the pattern itself, which is new in Scheme (but quite ML-like? -- editor)
Processes are on-shot continuations with software timer interrupts. The Gen-server patterns provide a functional way of serializing messages and mutating state. The init/handle-call/handle-cast/handle-info and terminate are all encoded, and so it's all done except code-change (the FDA wouldnt appreciate code changing on the fly!)
The event manager is responsible for event logging and subscriptions, and supervisors monitor processes and execute a restart or clean shutdown semantic (with enough logging for a field debug cycle).
How it works in the instrument server: "let is crash" throughout and a notable lack of defensive programming due to the actor model. They manage some big hardware which fails all the time and this architecture directly addresses that reality.
Easy assay is executed via a DSL which defines the pipeline in a confirm mode to estimate running time, suggest missing resources and warn about a lack of timeslots. Once satisfied this is all taken care of, the queuing is done via a Tetris-style binpacking heuristic. Each action has its own process for fault tolerance. The DSL exposes an option to ignore action failures which is important for test runs and debugging (also during hardware development).
Some examples of device failures are very bad: Therac 25 was a RTOS that gave people fatal radiation doses. It was tracked down to bad use of mutable state and integer overflow (taken care of in their system with FP and arbitrary precision math). Ariane 5 was due to interger overflow due to conversion precision, which also doesn't happen in their Scheme system which has non-lossy conversion. And finally the space shuttle simulator crashed due to memory unsafety (a restart was unclean), which cannot happen in Scheme. The supervision hierarchy in Erlang/Scheme can handle crashes with a stable strategy at any layer.
Why a DSL? The assay developers need to write their own recipes such as "which recipes for this instrument, and in which order?". The scientists shouldn't need software engineers to help them work, so the DSL sorts that out. Mechanical engineers need to check reliability and timing/variability on the hardware, which they can do by configuring DSL parameters.
There are a few bits to the DSL: an extraction/purification language (tip loading, pipetting, moving magnets) and the protocol language (aspirating, dispensing, etc) and finally the motor language.
Shows an example of the PCR language, which is a complex recipe involving baking, thermal cycling in loops. The DSL follows the logic flow of the English language version of the recipe very closely (with more brackets).
Wrote a web server that supports HTTP requests and JSON/jQuery and an sexpression-based HTML code generator. Can render pages with Scheme logic and render to webbrowsers. Use by engineers for debugging and prototyping without having to touch the real UI, and non-engineers use it to map data sources from instruments into JSON (which can be fed into validation tools, for example to send to the FDA for regulation).
Lessons learnt:
Message passing is great but not a silver bullet. Gen servers can deadlock between each other despite being deadlock free internally. Timeouts in genservers and supervisor logging makes this possible to catch at a higher level, which is crucial!
Not much on the web about a design pattern for the supervisor structure, so they had to pioneer it (didnt really want to pioneer it though)
Automated unit test frameworks also built by them by hand
Hiring remains hard into this unique environment.
Existing quality metrics like bug density are problematic, since Scheme is terse and so looks artificially worse than more verbose languages!
FDA has more scrutiny into this system since this is different from the usual C/C++/C# codebases and also the large number of open source tools such as Sqlite/Emacs/git. Unique to this regulated problem space of course.

Message passing concurrency makes reasoning about semantics of concurrent process easier, ass well as debugging problems via fault isolation. Let-it-crash leads to shorter, less defensive code with fewer branch points and test cases. Strong typing and runtime checks provide cognizant failure instead of silent corruption (crucial to avoid in this medical space).
Mixing languages has been nice, since they can pick the good bits of Scheme (macros, arbitrary precision math) and Erlang (OTP). DSLs provide the glue here, and rapid prototyping for non-expert software developers.
Gilt and Scala microengines
Clothing group, and they need to customize the website with per-user targetting. The customer facing stuff is the tip of the iceberg with a bunch of backoffice features that need to be plumbed through. The traffic patterns are all about flash traffic that spike massively during sales, and if they go down in these times they lose lots of money.
Gilt is about six years old, and it was built as a Rails app in the beginning that was quite large. It had unclear ownership, complex dependencies, lengthy test cycles, and unexpected performance impacts from small changes. Rails encourages to "throw everything into one directory -- a thousands models in one directory" which is really unscalable as the number of engineers grew. Some bad production issues came out of this problem.
Then they started to transition to microservices by transitioning a few core critical pieces into microserviecs such as inventory management. The number of services grew to over 300 services in just 4 years (each HTTP microprotocols), all built using Scala. They developed an architecture to support this sort of growth of microservices.
Cornerstones of infrastructure (goal==cool stuff for customers!)
Build system: SBT is a "simple build tool" which then turned into "Scala build tool" due to its complex mental model. Configuring everything correctly can be tricky if you have a number of dependencies. Gilt wanted to abstract developers from this so they could focus on writing appliction code. They solved it via a large SBT plugin that takes care of dependency management that provides a standardized configuration for everything.A complete build specification is shown which is very succinct: it just has a combinator for adding dependencies. Behind the scenes, the plugin provides a huge amount of support (testing, rpms, dependency, continuous builds, and many more)
Configuration is handled by a Zookeeper cluster and can be overridden with local files or system properties. It's mapped into strongly-typed classes in Scala and validated via Hibernate so that configuration must match up with service expectations.
Testing is the most challenging part of a microservice architecture. Functional tests are the gold standard, but hard for developers to run 300 services on their local laptops. Unit tests can run locally, but there's no mocking service for these (and overmocking results in useless testing that in non representative). They use the Cake pattern for dependency injection, which enables type-safe and scalable composition. It uses Scala's self-types and multiple trait inheritance. Things like object repositories or configuration providers can be expressed in this way. The pattern lets them mixin different sources and weave in real/mocked sources.UI Testing is handled via Selenium and is built on ScalaTest. Automated browser testing with reusable components works well, and leverages Scala's type system well to reject invalid tests/code/configuration.
Continuous Delivery lets them deliver 20 to 30 service versions every day using the aforementioned structures

Some cool stuff: on their website, there are badges on their website showing inventory for each item. The inventory badges update in realtime as you look at the page, which really motivates people to buy as they spot inventory dropping down. Shoppers get excited and feel like they're in a Black Friday sale and are engaged in the shopping activity! Building this stuff was quite easy to do using event-driven Play application and Akka actors.
They build on this for "freefall sales". Essentially a Dutch auction and is also web sockets and Play based. It initiates a sale that lasts for a fixed time with the price dropping regularly. The highlight is a $16000 purse that leads to audience laughter.
Q: how to deal with production side of microservices such as service discovery or log aggregation?A: no service discovery as Zookeeper tracks where everything is (which ports something is on). Log aggregation isnt great at the moment, so they're working on centralized collection atm.
Q: what's the greatest win of FP here? Is type safety really important or something like that?A: in the context of the company development and the technical stack, one nice thing about Scala is that it combines the "good parts" of Ruby (concise, expressive) with type safety and performance (as they had in Java). Scala provided a balance between Java and Ruby by giving them type safety, performance and conciseness, which is a big win in terms of expressivity and reuse of components.
Q: how do you handle versioning of microservices?A: each service maintains back compat in its API until they ensure all their clients are upgraded. The URI path has the version number, so service providers bump the version number for new clients and keep the old version working. They then migrate all the clients of the service as quickly as is practical.
PalletOpts and infrastructure automation
(I missed a lot of this talk due to a lunch chat spilling over. I'll fill it in later --editor)
A Clojure DSL that runs shell scripts and handles very complex infrastructure automation.
Q: Yaron: How do you handle error propagation through scripts (the equiv of set -e in sh)A: Each combinator propagates errors and there are different run strategies that handle errors differently depending on needs.
Streaming data at Twitter with Summingbird
Speaker is @sritchie (his slides). 
Summingbird is how to do streaming Map Reduce at the scale they have at Twitter (scale is "staggering"). It has been open sourced since Sep 3rd and a bunch of internal systems at Twitter run on it. Mainly three people: @posco @sritchie and Ashu -- small teams.
Vision:
"Write your logic once": Twitter has a number of 1000+ host Hadoop clusters and a lot of their data is done via k/v processing with Hadoop. Going streaming had to be done evolutionary by letting it be done either on Hadoop or on some new streaming service. Programmers experimenting with data shouldn't have to make systems decisions early on in the process. Twitters scale: 200m+ active users, 500M tweets/day and scale is pervasive.
Solve systems problems once so that systems that are scaling stay fixed.
Make non-trivial realtime compute as accessible as Scalding.

This leads onto Summingbird: a declarative map/reduce streaming DSL. It's a realtime platform that runs on Storm, and a batch platform that runs on Hadoop. The datastructures that come out of the DSL can be run on either platform (so you can choose between batch/streaming very late in the process).
One of the big concepts that Summingbird embraces is the idea of "associative plus" operations -- the monoid.The 4 S' of Summingbird ("you know you're onto something when they all have something in common!"):
Source[+T]
Store[-K,V]
Sink[-T]
Service[-K,+V]
Source is where data comes from, Store is a k/v store that is being aggregated into. Sink is a dump as processing items, and Service permits k/v lookups during a job.
Store: what values are allowed? What constraints can we place on a MR job such that any valid value can make progress.
trait Monoid[V] {
  def zero: V
  def plus(l:V, r:V):V
}
"less scary than a monad!". If you can take any two values and smash them together, you have a monoid -- e.g. integers. Also Sets have a monoid (concat) and lists (concat) and maps (recursively pull in the monoid on the value). Others include: HyperLogLog, CMS, ExponentialMA, BloomFilter, Moments,MinHash,Topk. All you need to do is find a datastructure with a monoid and you can use Summingbird. There is an open source library called Algebird (birds birds birds!).
It took some convincing (the words like "monoid" still scared people at Twitter, sadly).
Monoids are amazing due to Associativity. If you know that the values you are reducing are associative, then it's easy to parallelise since they can be built "in any order".
Even though realtime is important at Twitter, noones wanted to dip in due to the difficulty of getting started. Summingbird can pull in data from an existing static Hadoop cluster and then union it with a realtime feed. If you give the logic to Summingbird, it will run the logic on Hadoop (consuming discrete batches of data) and the client will keep track of how far it's gone. The realtime comes from partial aggregations over fixed time buckets, and is merged together in the client using the monoid to aggregate them together to look like a proper realtime feed.
One cool feature: when you visit a tweet, you want the reverse feed of things that have embedded the tweet. The MR graph for this comes from: when you see an impression, find the key of the tweet and emit a tuple of the tweetid and Map[URL,Long]. Since Maps have a monoid, this can be run in parallel, and it will contain a list of who has viewed it and from where. The Map has a Long since popular tweets can be embedded in millions of websites and so they use an "accountment sketch" which is an approximate data structure to deal with scale there. The Summingbird layer which the speaker shows on stage filters events, and generates KV pairs and emits events.
Twitter advertising is also built on Summingbird. Various campaigns can be built by building a backend using a Monoid that expresses the needs, and then the majority of the work is on the UI work in the frontend (where it should be -- remember, solve systems problems once is part of the vision).
Future plans:
Akka, Spark, Tez platforms
Pluggable graph optimizations since various planners do improved filter push
Metadata publishing with HCatalog -- in OSS, this means better tooling for EC2 and other cloud platforms.
And of course, more tutorials

All work happens on Github so contributors are welcome.
Summingbird is working well for the majority of realtime apps at Twitter. It's all about the Monoid! Data scientists who arent familiar with systems can deploy realtime systems, and 90%+ of the code is reused across the realtime and batch systems.
Q: Did Hadoop need any changes to cope with the interfacing with Summingbird.A: Scalding (written at Twitter) handles much of the interfacing with Hadoop. Hadoop is really the classical k/v store now. Real problem is figuring out which operations can work on both the batch and realtime and only admitting realtime.
Q: Yaron: associativity is one nice thing about Monoid, but what about commutativity is also important. Are there examples of non-commutative datastructuresA: Good examples. It should be baked into the algebra (non-commutativity). This helps with data skew in particular. An important non-commutative application is Twitter itself! When you want to build the List monoid, the key is userid,time and the value is the list of tweets over that timeline (so ordering matters here). It's not good to get a non-deterministic order when building up these lists in parallel, so that's a good example of when associativity and commutativity are both important.
Functional Probabilistic Programming
Avi from Charles River, introducing the Figaro language.
Problem: Suppose you have some information ("suppose Brian ate pizza last night") and you want to answer some questions based on this ("is Brian a student or a programmer?") and keep track of the uncertainty in the answers.
Solution: Create a joint probability distribution over the variables, assert the evidence and use probabilistic inference to get the answer back.
One common way to do this generative models. Probabilistic models in which variables are generated in order. Later values can depend on previous variables with a binding. This is a v popular approach with several implementations available over the years.
Developing a new model requires figuring out a representation, an inference algorithm and a learning algorithm. All of these are significant challenges with each of them still being considered paper-worthy.
The dominant paradigm is functional probabilistic programming. An expression describes a computation that produces a value.
let student = true in
let programmer = student in
let pizza = student && programer in 
(student, programmer, pizza)

let student = flip 0.7 in
let programmer = if student flip(0.2) else flip(0.1) in
let pizza = if student && programmer flip(0.9) else flip(0.3)
(student, programmer, pizza)
How are we supposed to understand this kind of program. Sampling semantics: run this program many times, each with a sample outcome. In each run, each outcome has some probability of being generated. The program defines a probability distribution over all the outcomes.
This is a powerful approach since we are taking a Turing powerful language and probabilistic primitives are added, and this naturally expresses a wide range of models.
Structured variable elimination, Markov chain Monte Carlo, importance sampling and so forth can all be built using functional probabilistic programming. PPLs aim to "democratize" model building -- one shouldn't need extensive training in ML or AI to build such models, or use them effectively.
IBAL (2000-2007) is the first practical PPL, implemented in OCaml but not embedded in OCaml. Exact inference using structured variable elimination and later had intelligence important sampling. However, it was hard to integrate it with applications and data (in IBAL the data had to be hardcoded into the data, which wasnt practical). It also lacked continuous variables. Therefore the speaker built Figaro to work around the limitations.
Figaro (2009-present) is a EDSL in Scala and allows distributions over any data type. It has a highly expressive constraint system that allows it to express non-generative models. There is an extensible library of inference algorithms that contains many of the popular probabilistic inference algorithms (eg. variable elimination).
The goals of Figaro are to implement a PPL in a widely used library: Scala is the interop language as Java compat is a prime goal. Figaro provides various good abstractions such as OO that come out of Scala (not sure, he skipped slide rapidly here? -- editor)
Figaro has an Element[T] which is a class of probablistic models over type T. Atomic elements (Constant,Flip, Uniform, Geometric) are combined with compound elements (If (Flip(0.9), Constant(0.5)...).
Under the hood, there is a probability monad to track the state. The Constant is the monadic unit and Chain(T,U) implements the monadic bind. Apply(T,U) implements monadic fmap. Most elements in Figaro are implemented using this monad.

Example 1: Probabilistic walk over a graph. In Figaro, we start by defining datastructures over the webpage graph (Edge, Node, Graph) and then we define some parameters of the random walk.
Example 2: Friends and smokers. Create a person with a smokes variable. Friends are three times as likely to have the same smoking habit than different, so this is a constraint variable. Finally this is applied to all pairs of friends by adding constraints to the pairs. This is then instantiated in a particular situation by building a set of people (alice, bob, clara) and running the inference algorithm (VariableElimination) to calculate the probability that Clara smokes.
Example 3: Hierarchical reasoning. We observe an object (say a vehicle on a road, or a person, or something) and we want to know what type of object it is. We have some observations about this object (its size, color or shape) and we want to know if its something we should be interested in. It's very natural to think of these objects via an inheritance hierarchy. In Figaro, every element has a name and belongs to an element collection. A reference is a sequence of names (such as vehicle size). Starting with an element collection, we can get to the vehicle associated with an element (go through through the sequence of nested element collections). However, there may be uncertainty in the identity of a reference (e.g. you dont know what vehicle1 is). Figaro resolve the element to the actual element.
Obtaining Figaro is free and open source (cra.com/figaro) but v2 is on GitHub (like everything else today, it seems).
Q: What are the limitations of Figaro, such as non-terminating Bernoulli tasks and what constraints cant be encoded?A: Anything you can express can be put in constraints since any code can go in there. It's very easy to express something that you cant compute. Figaro is working on helping the programmer that can be computed over, such as via rejection sampling or simply rejecting the bad constraint.
Q: Similar to Norman Ramsey's work (such as Church), whats the relationshipA: The languages fall into categories: their own languages and embedded libraries. The latter is Figaro and Infer.NET and Factory. INFER.NET is limited in expressivity and so can be compiled into a factor graph so is very efficient. Similarly Factory is lowlevel and compiles to factor graph. Lots to discuss here.
Haskell and FPComplete
Company exists to fix Haskell development tools. All the tools are cloud-based and run in a web browser. Content can be browsed anonymously. This talk will not have the Monad word in it!
There exists a feature to create tutorials and embed code within those tutorials.
Latest product (being announced today) is a Haskell IDE that is a web based tool which requires no setup and no download. It has a number of cool features:
Errors are displayed immediately as it compiled as you type
Git based so full version control

Company goals: increase Haskell adoption and make it more accessible. Offer commercial grade tools and support and simplify Haskell development. Also to support the Haskell community and leverage the community to improve FP Complete's own products.
The FP Haskell Center (released in Sep 2013) has:
a web front end (no setup,access to help and integrated with Haskell)
Haskell backend has project management, developer feedback with errors, build system and a cloud based execution and deployment platform. The cloud allows for faster product evolution!
Soon to have a personal version and version that will work behind firewalls etc.

It is all implemented in a Haskell web framework like Yesod. Lots of library support via conduits, and the UI uses Javascript (via Fay). All the heavy lifting is done in the backend and not via Javascript. Very few issues surfaced using Yesod and no major modifications were required to build their product.
Challenges were outside the scope of Haskell usually, but Haskell was used to fix them.
Javascript coding was significant in the frontend
Stable set of libraries are important
Compiler integration (feedback and errors) must be available as a library to let the IDE spot identifier location problems
Uses LXC containers to created isolation containers -- ephemeral storage and containers can run on either shared or dedicated systems.
IDE uses isolation git state to separate users from each other. When projects are created, there is a visual representation of projects (in git repositories each, in S3) and each repo is accessed via Gitlib-2. Interesting is that this offers multiple backends for git access: Git C library, GitHub C library, IDE local repo in S3 or others in the future.

For building projects, code is being precompiled continuously in the backend and so bytecode can generated quickly and run within the IDE. Bytecode runs in the GHC frontend container and exceptions dont break the IDE. Keter and Chef provide deployment assistance to get it into a shared application server or container, and FP Complete offers this deployment as part of their product.
Billing Processor provides SOAP APIs. Added SOAP Haskell libraries to Haskell via C++ bindings.
Mobile gaming from Japan
They normally built mobile games using Ruby, PHP, etc, but have switched to FP in recent years for reliability, high performance and productivity. However, they've suffered from memory leak (blaming Haskell's laziness, to audience laughter), and some data loss and performance degradation.
Gree is one of the largest mobile games platforms, with 37 million users and 2000 games as of June 2013. They develop social games and a platform for 3rd party games. They also do social media and advertising and licensing, and also venture capital.
Employees are 2582 with a significant number of engineers (cant disclose exact numbers). On client side, it's normally Java, ObjC, Javascript and Unity/C#. Server side is usually PHP, MySQL and Flare (a KV store written in C++ developed internally at Gree). They started a Haskell project in Jun 2012 and they have 6 Scala and 4 Haskell engineers.
They developed a KVS management system using Haskell, to setup and destroy KVS nodes in response to hardware faults and sudden access spikes. It's used in a large social game application. Another example is an image storage gateway, which converts WebDAV to memcached/S3 for the SNS photo uploader to store images. It uses Warp and http-conduit to do this conversion.
Some pitfalls they faced with Haskell:
Memory leaks via lazy evaluation: frontend server keeps a list of active thread ids in a TVar for monitoring. When deleting from the thread list, the function didnt reduce the function to a normal form and so held onto a reference. This led to a serious memory leak that took the service down. It was fixed easily by evaluating to the normalform and forcing sequentialisation. They felt it is too easy to mix up TVar/MVar writes with other IO operations (which do reduce to normal form and avoid the leak). There is much confusion over the strict/lazy alternatives for non-IO functions
Race conditions: Data put in a queue is very rarely lost. The reason is that the timeout functions can be used only once safely with an IO function. (details omitted)
Performance degradation: from forking a lot of threads to do monitoring. This was easy to fix but happens often since Haskell libraries are developed actively and the implications of changes are not always obvious.

How to fix all this? Better testing to start with. Haskell has a great QuickCheck framework and they developed HUnit. This lets services be started in the setup (in their example, memcached using a spare port), and then run services against it and make sure its torn down in a timely fashion. They used this to extensively test their KVS system and Flare (written in C++). Over 150 system tests and a control server with 5000+ assertions, so this is a big scale test.
Next up is documentation in Haskell. A problem report describes details of the problem and linked from a bug tracker. The timeline, workaround, impact and details caused must all be captured and are mixed up with a number of other reports, so none of the Haskell people read it. They collected up the aggregation and summarised it for other Haskell products. Despite this, other Haskell programmers still dont read it (audience laughter). To sort this out, they do automated checking using hlint. They customize hlint to "grep" for their particular pitfall, and then check for Emacs. hlint isnt quite good enough for a lot of their pitfalls though, such as the timeout issue from earlier, so they had some workarounds to deal with this. The good thing is that hlint is integrated into their text editor. Not all pitfalls can be caught by hlint: high level design issues will remain problematic.
Education is also important. They have a brown bag (lunch? -- editor) FP meeting where they "Make GREE a better place through the power of FP". They cover both Scala and Haskell topics. They also run an education program for new graduates, and they have a Haskell code puzzle series from Project Euler to help the new graduates.
They conclude that they love FP and they develop some key components of our service using FP. But there are many pitfalls, and they are developing tools to assist them sort them out.
Haskell for distributed systems
Jeff Epstein (formerly a masters student in the Cambridge SRG, so it's good to see him again!) talks about their new distributed system that they are building in Haskell. His company is also available for hiring!
HA middleware manages networked clusters, aiming for exabyte-scale clusters. It's architected by Mathieu Boespflug and Peter Braam (who has spoken about their FPGA work at past CUFPs). It manages 10000+ nodes and creates consensus in the cluster about the location of active services. It can handle events due to failures, recovery time should be approx. 5 seconds.
Zookeeper didn't scale (audience interruption: why didnt Zookeeper scale? Jeff says he doesn't know and should chat offline, audience member notes that they maintain Zookeeper and wants to know more).
Events are sent by nodes and aggregated at a proxy node, and enqueued at an HA station. Events are processed by the hA coordinator and updates a node state graph (which is also replicated -- everything is replicated and is a key theme in the talk). The HA coordinator is the decision maker which decides what to do on failure. Its main action is updating the node state graph to reflect the new strategy after failure.
Key components are: Haskell, Cloud Haskell and Paxos.
How we used Haskell:
Functional Data structures result in a purely functional graph and replicated (impurely) between cluster managers. Most of the feels imperative though, which was important.
Refactoring being easier was important due to strong typing.
Laziness was yet again a problem causer in IO due to space leaks in low-level networking code. He notes that distribution is a natural barrier to laziness, since the act of serializing a message forces its evaluation. He also notes that processes much behave the same between processes on the same host as between hosts, and so even local communication must be forced.Phil Walder interjects to ask "why did you use Haskell? It's a bad fit! Jeff replies that "we like Haskell" and introduces Cloud Haskell, the topic of his MPhil thesis.Cloud Haskell is a library for distributed concurrency in Haskell and provides an actor-style message-passing interface, similarly to Erlang. It provides a "typed erlang".Cloud Haskell is a good fit for this project in terms of interacting communicating components. Refactoring is easy, and this isn't a "big data" project in the sense of data size. There is a huge amount of communication messages though.CH has a pluggable transport layer such as TCP, but also for different underlying networks (they built Infiniband). However, CH requires ordered delivery of packets and this was a poor fit (they want out of order delivery, which could be built with UDP, but CH requires an ordering for some reason). Space leaks are a real problem in this layer.Cluster bringup is tough for CH, since nodes need to find each other at start of day, and there some hacks in there to work around a morning storm of messages.(Interjection from audience: why not use Erlang? Jeff: performance wasnt up to par, and static types are important for them). Possibly the native code / bytecode erlang might be useful but speaker wasn't sure if it would work.
Paxos: is all about making systems agree (well-known proven algorithm for consensus). They implemented this as a general purpose library on top of CH. The algorithm is a good match for the programming model, with the client, acceptor, proposer learning and leader all built as CH processes. It ended up being a small readable implementation (1.5kLOC) that closely matched the pseudocode in the original Paxos paper (made them feel better about correctness). It's used for synchronizing the state graph and for the distributed message queue.
Challenges: Paxos is hard and untyped messages in CH make it worse. Getting reasonable performance from it also requires the modified variants. Liveness also is tricky to avoid overapproximations to prevent nodes being fenced out of the consensus unnecessarily. Multipaxos is something that solves some of this, and they're working on more specific stuff also.
Debugging: The CH programming model makes it tractable to debug a distributed application on a single machine. Nevertheless, distributed code is still parallel and involves race conditions and mixing messages. They would like a distributed ThreadScope but instead mainly used oldskool logging.
They did build a deterministic CH thread scheduler to replace CH's core message handling primitives, and they also used PROMELA
Q: how much is open source or will be?A: none as far as speaker knows
Q: Marius: how are non-deterministic sources composed with the deterministic scheduler?A: deterministic scheduler is for testing only, so timeouts also so deterministic.
IQ: Functional Reporting
Speaker is Edward Kmett on how they got FP in the door at S&P Capital in the door, their extensions to Haskell and generally how they're working on opensourcing their tech. SAP is huge and have a number of products (S&P Capital IQ Platform, ClariFI, AlphaWorks, Compustat).
Getting FP in the door:
Monoids (introduced as with the Summingbird talk, for easy parallelisation)
Reducers are another really powerful abstraction which can take a container full of values and give you back a structure, and it has a comonadic structure that lets you (e.g.) pass it many containers full of structures and generally compose things nicely. Reduction can also be done simultaneously across monoidal structures and essentially "we dont see the wiring hanging out of our code any more".
Performance attribution: This is a model that breaks down the reasons why you make money (what did you do that was significant). The old and busted model (implemented in Java, full dataset in memory, hard to extend) was horrible, and the new hotness is implemented in Scala, runs in constant space, is really fast and results are flattened elegantly via combinators to a database backend. In other words: better, but some details are glossed over (multipass algorithms, iteratees, caching layers, and handling missing data) -- but all this was sorted out and really sold Scala to the bigger company as a real asset.



They developed Ermine, which is Haskell with Row Types. It's not quite Haskell, but it's main feature is that it's not Scala either (audience laughter). Writing functional programs in Scala is quite painful (e.g. not blowing stack with monads requires trampolining everything and is labour intensive just to get by day to day). They wrote Ermine in both Scala and Haskell with a portable Core that can run on the JVM.
Ermine has a strong type system, with novel row types and polymorphic and constraint kinds ("they were just coming into GHC at the time and I wanted to understand them") and Rank-N types via a derivative of HMF. It fit their domain requirements really wel, and has builtin database support, can be exposed to end users, a structured code editor that can only generate type-safe code as you're defining, and full control over error messages.
Row types are a powerful way of describing the presence of a particular field. Its modelled via has, lacks and/or subsumes constraints. This is easily checked, but now inference flows unidirectionally through a join.In Ermine, they have existentials in their constraint types (technical explanation follows, not shown here --editor).
Their reporting layer has one declarative specification to do all the relational algebra specifications they want, and push it into specific backends (SQL Server, etc) and ensure they run their queries as close to the data as possible. They can also create extensive HTML reports that render pretty consistently across different target platforms. Their example of Ermine was written by a functional programmer that had done no functional programming previously, which required a little hand holding to work through some of the more obscure type errors, but generally it worked ok.
All the code is being stuck onto Hackage/Github etc.
Q: has it been rolled out to a lot of users?A: being used on the main S&P website. One big client facing page for users. The next gen stuff is using Ermine more widely. The desktop app coming soon has Ermine and will be used by a huge number of users. Trend is to use it in an increasing number of their products.
Enterprise scheduling with Haskell at skedge.me
They have a cloud-based scheduling platform that takes care of complex enterprise scheduling. It has a branded customer experience (like Google Apps does), a flexible workflow and deep integration with backend systems and in-store workflow (e.g. on iPads).
Example is Sephora who do makeup, and their appointment system is integrated via skedge.me that is embedded in an iFrame that looks like a seamless part of their site so that endusers cant tell the difference.
When speaker started at skedge.me, there was a lot of fire fighting. It was 43k lines of Groovy on Grails, and several major intractable bugs were affecting the business: timezones, double bookings, recurring events not firing and notifications being delayed. Performance was an issue, as things that were interactive were sometime taking minutes to load. There was also a lot of inflexibility -- hilariously, they had to sometimes ask clients to change their business model to fit with the skedge.me model.
After some careful though, they decided they had to rewrite skedge.me to cope with growth and better service. The author had done Haskell before, but not specifically was with building a website using it.
Haskell had the perfect model for fixing their scaling problems. At the lowest level, they built a RawDB on top of the IO monad that did everything necessary to maintain ACID guarantees during transactions. An example is that once an effect gets committed (e.g. an email gets sent confirming an appointment and then is cancelled, which happened on their old racy platform). This RawDB layer gives them stronger consistency guarantees with tracked effects and automatic retries on temporary failure.
On top of the RawDB, they build a DB monad which provides a higher level than SQL via algebraic data types, caching and data validation. They then layer a security monad on top of the DB monad to guarantee that all accesses have been checked. Finally there is a business logic layer to enforce local policy and customise per customer (which was a real problem in the past).
Security is complex due to all the customization, and they needed roles by client (owner, staff, customer) but also because clients can customise their verbs that only make sense within the component (Appointments can be joined, rescheduled, or notifications can be sent, edit cancelled). Haskell lets all this fit within a multiparameter type class, instead of doing OO-style inheritance that will rapidly get out of control. They map customisations from the customer onto a more standard schema that they can manipulate generically on a component-by-component basis, and it's all statically checked and generally easy to read (important for security critical code).
Haskell "lets them build code for the long run". Sometimes though, you need to write code quickly and not care about longevity. This was the case for the importer, which is a quick-and-dirty hack to get code into their system. It has a limited set of inputs, the "end user" is the developer, and it's thrown away once the job's done. (shows epic type). Often when people need to do this sort of thing, they resort to bash or dynamically typed systems. In this case, even for this "quick hack", they couldnt have achieved what they did "as badly as they did, without the help of the type system". Generally very positive!
Libraries help them not have to write code. Haskell is generally lower in terms of package count, but their *use* of Hackage libraries has been a lot higher than Javascript. They use 9 Javascript libraries, and 71 unique Hackage libraries (+87 dependencies). Its a lot harder to find a quality library that works in the dynamically typed land. Only one of their Haskell libraries had to be replaced due to bugs. This is because Hackage is well-organized, and it's easier to vet a library since the purely functional ones are less risky (fewer sideeffects through other code). In contrast, Javascript libraries can sideeffect (e.g. on the DOM) very late in the QA process. So: fewer Haskell libraries, but the number of useful libraries seems to be higher.
Problems: cyclic module dependencies with hs-boot files are unwieldy, so avoid them. Scalability with 4+cores was limited, but they havent tested GHC 7.8 (which apparently fixes a lot of this). Idle GC can cause issues with long pauses, so disable this feature -- it depends on your workload but it was ok for their server based workload.
Debugging production problems without stack traces is really hard, so consider using profiling mode for all libraries.
Bottom line: 8200 lines of Haskell from 40k+ lines of Groovy. No recurring bugs and very good performance and flexibility for customer needs. Used daily by thousands of people. Check out the website!.
Q: what about testing (QuickCheck?)A: they've done some QuickCheck on critical components (such as an map from times to events). Overall though, just putting Haskell together in the most obvious way led to improvements over their previous platform. Their QA guys are now writing unit tests in Haskell (because their Selenium bindings were harder).
Integration of Mathematica with MapReduce
From Paul-Jean Letourneau, a data scientist at Wolfram Research. Started with a show of hands: most of the audience has used Mathematica and had heard of Wolfram.
The speaker has been responsible for a lot of the personal analytics posts on the Wolfram blog (the email analysis, genomics and several others). The theme of all his projects has been to work on large data. Mathematica has always kept its data oncore, which is a problem with very large data sets, and so this talk is about how they migrate over to a more big-data model to run offcore.
Fundamental principles of Mathematica: everything is an expression, expressions are transformed until they stop changing, and transformation rules are in the form of patterns. Expressions are data structures (actually m-expressions in Lisp parlance). (full explanation of these in Mathematica demonstrated but omitted here)
Expressions in Mathematica are immutable data structures ("program as data") and so expressions can be rebound. Homoiconicity is pervasive as expressions are the data structure, even internally during evaluation (where it's represented as a tree).
"everything is a one-liner in Mathematica... for a sufficiently long line" (Theo Gray). (audience laughter). He demonstrates some cool oneliners that are Tweetable (<140chars) and demonstrates how to construct an image recursively out of itself (wow).
He views it as a gateway drug to declarative programming
y = 0
For[i=1; i<=10; i++,y+=i^2]; y
385
(and then they discover Fold and all the advanced topics such as scoping, evaluation control and the MathLink protocol for IPC).
HadoopLink is how they link Mathmetica to Hadoop. Example is WordCount, wherein he shows code to do it on a single core first (to test the output), and then an export to HDFS so that Hadoop can see the code. This looks fairly straightforward via a DFSExport command from HadoopLik.
The Mapper looks something like:
Function[{k,v},
  With[{words=ToLowerCase 
    /@ StringSplit [k,RegularExpression[], ...
The reduces is similar, as it users an iterator to sum up each of the inputs. It actually communicates to the JVM to ensure that they dont have to load all the values into memory. Running the job is just a special HadoopMapReduceJob along with all the relevant expressions, and it goes ahead and submits it to Hadoop.
Used this to build a genome search engine using all this. He shows how to prep data using Mitochondrian genomic data. The mapper only knows about a single base in the genome and its position, and the query sequence, how can it possibly align the query sequence with the genome? Algorithm scales with the size of the query, and not the size of the dataset. Hadoop can be used to handle the full dataset so getting local algorithm right in Mathematica can be focussed on rather than the bigger problem.
Challenges: when this scaled up to search the entire human genome, my Hadoop cluster blows up. Nodes start dying, Nagios alerts, the works! After drilling down on these cryptic errors, the "garbage collector overhead was exceeded" which was hard to interpret. The memory consumption on each worker node when running with both Mathematica and the slave is quite high. This is a setting that's made on a clusterwide basis and so is problematic to tweak just for this. What he'd like to do is add job-specific options to run things like heap size.
Its on GitHub and open source (github.com/shadanan/HadoopLink) so go for it.
Announcements
Ashish Agarwal is organizing BoFs! On Monday @ 6pm there will be a bioinformatics session with Jeff Hammerbacher with location on the CUFP website.
On Tuesday there will be a BoF about organizing local events for FP. In the same way that politics is local, this BoF will help you figure out strategies for local events.
Yaron Minsky announces that 17 organizations have sponsored CUFP this year, which helps students. Industry attention to FP is really amazing to see how FP is moving into the mainstream. Industrial reception at 1830 on Monday downstairs.
Our thanks to Marius Eriksen and Mike Sperber for an amazing program!
Hide
        
      
                    by Anil Madhavapeddy at Sep 22, 2013 
      
      
    
  


       
                  OPAM 1.1 beta available, with pretty colours
      (Anil Madhavapeddy)
    
    
                                Thomas just announced the availability of the OPAM beta release.  This has been a huge amount of work for him and Louis, so I’m excited to see this release!  Aside from general stability, the main highlights for me are:

      A switch to the CC0 public-domain-like license for the repository, and LGPL2+linking exception for OPAM itself.  The cutover to the new license was the first non-gratuitous use of GitHub’s fancy issue lists I’ve seen, too!  As part of this, we’re also beginning a transition over to hosting it at opam.ocaml.org, to underline our committment to maintaining it as an OCaml community resource.
  
      Much-improved support for package pinning and updates.  This is the feature that makes OPAM work well with MirageOS, since we often need to do development work on a low-level library (such as a device driver and recompile all the reverse dependencies.
  
      Support for post-installation messages (e.g. to display licensing information or configuration hint…Read more...Thomas just announced the availability of the OPAM beta release.  This has been a huge amount of work for him and Louis, so I’m excited to see this release!  Aside from general stability, the main highlights for me are:

      A switch to the CC0 public-domain-like license for the repository, and LGPL2+linking exception for OPAM itself.  The cutover to the new license was the first non-gratuitous use of GitHub’s fancy issue lists I’ve seen, too!  As part of this, we’re also beginning a transition over to hosting it at opam.ocaml.org, to underline our committment to maintaining it as an OCaml community resource.
  
      Much-improved support for package pinning and updates.  This is the feature that makes OPAM work well with MirageOS, since we often need to do development work on a low-level library (such as a device driver and recompile all the reverse dependencies.
  
      Support for post-installation messages (e.g. to display licensing information or configuration hints) and better support for the external library management issues I explained in an earlier post about OCamlot testing.
  
      Better library structuring to let tools like Opam2web work with the package metadata.  For instance, my group’s OCaml Labs has a comprehensive list of the software packages that we work on generated directly from an OPAM remote.
  
      A growing set of administration tools (via the opam-admin binary) that run health checks and compute statistics over package repositories.   For example, here’s the result of running opam-admin stats over the latest package repository to show various growth curves.
  


Number of unique contributors to the main OPAM package repository.
Total number of unique packages (including multiple versions of the same package).
Total packages with multiple versions coalesced so you can see new package growth.



Is it a hockey stick graph?  Only time will tell!
See Thomas’ full release announcement and let us know how you get along with this new release…
Hide
        
      
                    by Anil Madhavapeddy at Sep 20, 2013 
      
      
    
  


       
                  Inaugural compiler hackers meeting
      (Compiler Hacking)
    
    
                                

The first OCaml Labs compiler hacking session brought together around twenty people from OCaml Labs, the wider Computer Lab, and various companies around Cambridge for an enjoyable few hours learning about and improving the OCaml compiler toolchain, fuelled by pizza and home-made ice cream (thanks, Philippe!).

We benefited from the presence of a few experienced compiler hackers, but for most of us it was the first attempt to modify the OCaml compiler internals.

The first surprise of the day was the discovery that work on the list of projects was underway before we even arrived!  Keen collaborators from The Internet had apparently spotted our triaged bug reports and submitted patches to Mantis.

Standard library and runtime

There was an exciting moment early on when it emerged that two teams had been working independently on the same issue!  When Jon Ludlam and Euan Harris submitted a patch to add a get_extension function to the Filename module they found that they had been pipped …Read more...

The first OCaml Labs compiler hacking session brought together around twenty people from OCaml Labs, the wider Computer Lab, and various companies around Cambridge for an enjoyable few hours learning about and improving the OCaml compiler toolchain, fuelled by pizza and home-made ice cream (thanks, Philippe!).

We benefited from the presence of a few experienced compiler hackers, but for most of us it was the first attempt to modify the OCaml compiler internals.

The first surprise of the day was the discovery that work on the list of projects was underway before we even arrived!  Keen collaborators from The Internet had apparently spotted our triaged bug reports and submitted patches to Mantis.

Standard library and runtime

There was an exciting moment early on when it emerged that two teams had been working independently on the same issue!  When Jon Ludlam and Euan Harris submitted a patch to add a get_extension function to the Filename module they found that they had been pipped to the post by Mike McClurg.  There's still the judging stage to go, though, as the patches wait on Mantis for official pronouncement from the Inria team. 

Vincent Bernardoff also spent some time improving the standard library, fleshing out the interface for translating between OCaml and C error codes, starting from a patch by Goswin von Brederlow.

Stephen Dolan looked at a long-standing issue with names exported by the OCaml runtime that can clash with other libraries, and submitted a patch which hides the sole remaining offender for the runtime library.  As he noted in the comments, there are still a couple of hundred global names without the caml_ prefix in the otherlibs section of the standard library.

Tools

There was a little flurry of work on new command-line options for the standard toolchain.

A Mantis issue submitted by Gabriel Scherer suggests adding options to stop the compiler at certain stages, to better support tools such as OCamlfind and to make it easier to debug the compiler itself.  The Ludlam / Harris team looked at this, and submitted a patch which provoked further thoughts from Gabriel.

Vincent looked at extending ocamldep with support for suffixes other than .ml and .mli.  Since the issue was originally submitted, ocamldep has acquired -ml-synonym and -mli-synonym options that serve this purpose, so Vincent looked at supporting other suffixes in the compiler, and submitted a patch as a new issue.

The OCaml top level has a simple feature for setting up the environment —  when it starts up it looks for the file .ocamlinit, and executes its contents.  It's sometimes useful to skip this stage and run the top level in a vanilla environment, so David Sheets submitted a patch that adds a -no-init option, due for inclusion in the next release.

Error-handling/reporting

Error handling issues saw a good deal of activity.  Raphaël Proust submitted a patch to improve the reporting of error-enabled warnings; David investigated handling out-of-range integer literals and return-code checking of C functions in the runtime, leading to some discussions on Mantis.  Stephen submitted a patch to improve the diagnostics for misuses of virtual.  Gabriel Kerneis and Wojciech looked at some typos in ocamlbuild error messages, and Mike opened an issue to clarify the appropriate use of the compiler-libs package.

Language

The open operation on modules can make it difficult for readers of a program to see where particular names are introduced, so its use is sometimes discouraged.  The basic feature of making names available without a module prefix is rather useful, though, so various new features (including local opens, warnings for shadowing, and explicit shadowing) have been introduced to tame its power. Stephen looked at adding a further feature, making it possible to open modules under a particular signature, so that open M : S will introduce only those names in M that are specified with S.  There's an initial prototype already, and we're looking forward to seeing the final results.

The second language feature of the evening was support for infix operators (such as the List constructor, ::) for user-defined types, a feature that is definitely not in any way motivated by envy of Haskell.  Mike's prototype implementation is available, and there's an additional patch that brings it closer to completion.

Next session

The next session is planned for 6pm on Wednesday 18th September 2013 at
Makespace, Cambridge.  If you're planning to come along it'd be
helpful if you could add yourself to the Doodle Poll.  Hope to see
you there!
Hide
        
      
                    by Compiler Hacking at Sep 17, 2013 
      
      
    
  


       
                  OCaml Monthly Meeting – Live Blog
      (Heidi Howard)
    
    
                                Today’s OCaml Labs Monthly Meeting is all about practise talks for OCaml2013 so in that spirit, I’ll practising a bit of live-blogging too.
13:53 – Today’s SRG Meeting is over and its time for some work before the OCaml Labs meeting at 4:00, see you then …
16:02 Techincal difficulties delayed the start
16:02 Intro from Anil
introducing Gabriel Scherer who is visiting us this week and going we are going to Maypole after this meeting. We had a cash prise from ASPLOS after winning the HiPEAC paper award and the money will go towards SRG wine for XMAS party. Signpost paper was accepted to FOCI and a HotNet paper on Trevi was also just accepted
OCL Website – Too much manual management at the moment, moving to an ocaml planet feed of blog posts. David has been busy hacking on OPAM2web, OPAM has 512 packages, Opam2web takes a subset of the OPAM packages and makes the metadata into a minisite, like on OPAM. Doesn’t require manual updates, like an ATOM feed.
Upcoming events – To…Read more...Today’s OCaml Labs Monthly Meeting is all about practise talks for OCaml2013 so in that spirit, I’ll practising a bit of live-blogging too.
13:53 – Today’s SRG Meeting is over and its time for some work before the OCaml Labs meeting at 4:00, see you then …
16:02 Techincal difficulties delayed the start
16:02 Intro from Anil
introducing Gabriel Scherer who is visiting us this week and going we are going to Maypole after this meeting. We had a cash prise from ASPLOS after winning the HiPEAC paper award and the money will go towards SRG wine for XMAS party. Signpost paper was accepted to FOCI and a HotNet paper on Trevi was also just accepted
OCL Website – Too much manual management at the moment, moving to an ocaml planet feed of blog posts. David has been busy hacking on OPAM2web, OPAM has 512 packages, Opam2web takes a subset of the OPAM packages and makes the metadata into a minisite, like on OPAM. Doesn’t require manual updates, like an ATOM feed.
Upcoming events – Tomorrow is the 2nd compiler hacking event, at the makespace. Anil will be talking at QCon on Mirage, Mirage 1.0 release date is October 22nd, so maybe a workshop before. We 3 talks for Ocaml2013 (Platform, OcamlOT and Ctypes) so here we go …
16:09 Anil practice talk on OCaml Platform 1.0
Languages take many difference approaches to platform, but what does platform even mean? As a late mover in this field, we can learn from other languages. A platforms is NOT a group of temporarily motivated hackers to build a replacement standard library. Its hard to adopt a particular approach without a domain specific purpose, there are too many opinions, we need objective way to determine what belongs in the platform, we need a genie community that is sustainable (even if a large party leaves). A platform is a bundle of tools that interoperate, with quantitative metric to judge success, built in agility and supporting developers thought the whole development life cycle. Industrial partners have a range of needs, as each work in different domains.
Tooling – Overview of 5 areas: OPAM from OCamlPro, IDE Tools, OPAM-DOC, OCaml compiler itself and Ocaml.org.
OPAM – 1.1 released today (maybe), over 100 contributors to OPAM,  500+ packages, 1500+ unique versions, external dependency solver using CUDF
IDE Support – OCaml has many intermediate files. In OCaml 4.0 onwards, we have a binary format of an abstract syntax tree with type annotations called cmt (and cmti for interface files), we can now create external tools to query this like opam-doc. ocp-index and ocp-indent from OCamlPro, and Merlin (I thinks this is EPIC) are also now available
opam-doc – Now we have cmt files, we need unified documentation across packages, this is much harder than it sounds as it touches every part of the tool stack. Not all packages can be installed at once due to conflicts. Module inclusion is tough to code in static html. (Need to make a demo) bindoc takes the Typed AST (in cmt) and generates cmd, which include the ocamldoc comments, Opamdoc takes the cmt database for opam and output a single website with your universe of packages.
ocaml.org – Demo of ocaml.org at ocaml-redesign.github.io/pkg/index.html, feedback is welcome says amir
Now we have the tools, what metrics can we extract to see how well our tools are doing.
Portability – windows compatibility ?
Maintainer – is there a place for docs and will people response to issues/comments/emails, where can issues be submitted ?
Tests – code coverage, multi variant benchmarking in core-bench
Stability – OPAM support pining, how stable are the interfaces of libraries ?
opam tracks compiler constraint, statically analyses the build system from logs (OCamlOT)
Agility – Building a platform is EXHAUSTING. We want to ask “WANT IF” questions: what if let was monomophic? what if we removed camlp4? what is the syntax precedence changes ?
Distrusted workflow – build on git, distributing tasks between 3 actors: Author (library writers), OCamlOL workers and maintainers. As we become more stable we move from staging to stable to inclusion in the platform.
We are building a tussle, we want to launch a game in janurary and let people put standard libraries into the ring, running OCamlOT to discover the winner
No clear winner: Lwt – portability, Batteries – free of syntax extensions, core – comprehensive.
16:36  Discussion over the battle of the standard libraries and talk feedback
C: talk is a bit long, not sure what to cut..
C: OPAM was dicussed last year at OCaml2013, we want to update everyone and follow on without overlapping too much
Q: Haven’t we already decided on JS’s core ?
A: No, we use all of them, i.e. Mirage used lwt extensively
Q: What if we don’t want any of the new standard libraries ? maybe I just want to use domain specific libraries from OPAM as and when I need them
A: We are not forcing the new standard libraries on anyone, but they are useful for beginners, nice to have consistent style, interoperability and few open statements e.g. Open Core.Std
Q: What if I have already decided which standard library I want to use ?
A: Again we are not forcing standard libraries on anyone, we are just trying to force effort more directly. OCaml tools will always be standard library agnoctic
C: the diagram of OCamlOT is confustion
C: how to not overlap with david talks
16:41 Davids talk on OCamlOT
State for the open source OCaml community
Outline: what is quality software? what is the user experience? what is feedback loop for package authors? How do we represent the thing underneath this all? utopian future ?
Quality: Work on every core (ANIL: We want multi-core  ), consistent results: work or die nicely with obvious solution, not more “What have I forgotten?” questions, it should just tell you. We need addictive actions (not sure what they are), consistency, quality functions…
Universal concerns: compiler hypothesis “what if” questions (anil already said this), build system hypotheses “what strange assumuptions is the buid system making?”, package manager hypothesis and environmner hypothesis
Workflow: Make a pull request, curator observes the proposal, predict the future, proposes amendments, feedback loop and finally agreement is reached. Core is release weekly for example, we are trying to work like linux kernal patches
New workflow: promote health of OCaml community, preaching compatibility, “observe, orient, decide and act”, Computer assisted curator will help a human, to run the loop faster, human can pose questions to the computer assisted curator e.g  “will this run on ARM ?”
Repository Observation: github binding with web hooks but we are not tied to github. We merge into the world and we need dependences from each possible users prospective of the world
Dependency Orientation: capabilities with environmental dependances, packages with constriant-based dependencies, repositories with revision dependencies and artifact dependencies. example of the android repo
Triage Decisions: taking plain text error and parsing them into categories such as unsatisfiability (can’t have these two packages), dependencies (if my dependency is broken, then I am broken), transient (network down), system, metadata, external dependences (you forgot to write a dependency), build errors and a combo of many of the above.
State Action: commit intention, build, error analysis and buid results
Internet res: The agents negotiates over REST API on HTTPS, independent metadata layers (not sure about this) ,everythings an s-exp, branch consistent store explained, like git or Irminsule
Current state: github web hooks, we are conservative so one byte changes and we rebuild everything, basic triage heuristics completed, no amendment are proposed by the system atm, we don’t commit the outcome but the evidence, simple reactions to results, a website with green and red boxes in the large table
History: we have found lots of metadata issues, many packages bugs, some tool bugs like a non relocatable compiler and ocamlbuild PATH ignorer, we currently have 30+ x84-64 30+x84-32, 8 ARMs , many Linux distros , dead Raspberry Pi, panicking *nix filesystems and lots of people have set warning as error
Future: opamfu for DAG analysis, schema migration overhead, lower overhead for administrating exotic workers contributed to OCamlOT, we need to authenticate machines using ocaml-sodium, we need more advanced automation, proposed amendments, lets have a dialogue, better website integration, benchmarking your upgrades (how much improves cost), run experiments on whole OPAM universe with differential analysis and VM-based test system, to specific the worker finely.
What I think quantity is, vision of the future, how its represented underneath and what’s next,
Discussions
C: that was 20mins, feedback to David regarding content to be cut,
17:23 Ctypes by Jeremy 
This is a update not a practice talk
An examples of puts from C, how we can write no C and link in OCaml,
NEW things in Ctypes:prettyprinting – for C types and C values, making it much eaiser to examine values for debuygging
biarray – support for lump of C memory
More type – nullable string, complex numbers
String conversions – much faster
Memory management issues – ctypes now gives the programmer more control over lifetime of OCaml passed to C,
finaliser – which you can attach to memory
Future
stub generation – instead of dynamically binding, it will generate stub code to act to the API
capability-style memory safty – one rogue pointer in a C library, can cause hell, loading each C library in a seperate address space so i library can only kill itself, you can then even run on C library on a foreign host or on a virtual machine
static strcut/union layout – checking layout of structures and unions against the API
17:40 Amir demo of ocaml-resdesign.githuib.io/docs/opam, (its look great :))

Hide
        
      
                    by Heidi Howard at Sep 17, 2013 
      
      
    
  


       
                  Camlpdf, the first good command-line PDF tool I've found
      (Anil Madhavapeddy)
    
    
                                The fine folks at O’Reilly have been proof-reading the Real World OCaml book
that I’ve been working on.  My heart leapt with joy when the copyeditor commented that she thought it was very well written,
but my joy was short-lived when it turns out that all her comments were safely ensconced as PDF comments.  A few
hundreds comments that required a response from us, too.

“No problem! MacOS X Preview can handle these!” was of course my first response, but it turns out it’s totally
broken with PDF notes.  The first note that you select will appear as the content of all the other subsequent notes.
Yaron Minsky then experimented with Adobe Acrobat, which I’ve sworn never to install again after an unfortunate
incident involving the uninstaller a couple of years ago.  That turned out to be incredibly slow.  I tried a
few open-source tools such as Skim which, while otherwise an
excellent bit of software, couldn’t render these particular annotations.



I literally jumped off my…Read more...The fine folks at O’Reilly have been proof-reading the Real World OCaml book
that I’ve been working on.  My heart leapt with joy when the copyeditor commented that she thought it was very well written,
but my joy was short-lived when it turns out that all her comments were safely ensconced as PDF comments.  A few
hundreds comments that required a response from us, too.

“No problem! MacOS X Preview can handle these!” was of course my first response, but it turns out it’s totally
broken with PDF notes.  The first note that you select will appear as the content of all the other subsequent notes.
Yaron Minsky then experimented with Adobe Acrobat, which I’ve sworn never to install again after an unfortunate
incident involving the uninstaller a couple of years ago.  That turned out to be incredibly slow.  I tried a
few open-source tools such as Skim which, while otherwise an
excellent bit of software, couldn’t render these particular annotations.



I literally jumped off my seat upon discovering cpdf.


Meanwhile, John Whitington just announced the release of the Coherent PDF command-line tools.
Since these are all written in OCaml (and have been developed over quite a few years now), he also sent in an
OPAM pull request to add it to the database.
And most conveniently, this ended up solving my little PDF conundrum in less than an hour of hacking, and
has almost cured me of my lifelong fear of dealing with anything in a PDF-like format.  Here’s what I did:

      I installed the tools via opam install cpdf.  This installed the library but not the binary (swiftly fixed).
  
      Reading the license told me that it’s for non-commercial use only, so I bought a license from the
Coherent PDF website (a bargain price, given how much it does!).
  
      I ran cpdf -list-annotations over the PDF, and it dumped out all the comments as a text file to stdout.
This wasn’t quite enough for me, since I needed to match the annotation to a page number.  But since
John has released it as open-source, I forked the repository and patched the support directly into the
command-line tools, and sent a pull request
back over to John.  Since it’s under a non-standard license, I decided to place my patch in the public
domain to make it easier for him to accept it if he chooses.
  
      My co-authors can just run opam pin cpdf git://github.com/avsm/cpdf-source#annotation-page-numbers
to pin their local copy of CPDF to my forked branch in their own OPAM installations, and easily use my
copy until John gets a chance to integrate my changes properly upstream.
  


Total time including this blog post: 40 minutes. Now, onto fixing the author
responses comments for Real World OCaml now.  I’m so happy to have cpdf as a
simple, hackable PDF utility, as it does things like page combining and rotations
that have always been a little flaky in other tools for me.  It’s the
Pandoc of PDFs!
Hide
        
      
                    by Anil Madhavapeddy at Sep 16, 2013 
      
      
    
  


       
                  OCaml Development in Vim
      (Heidi Howard)
    
    
                                This is a quick run-through of how I set up my development environment in vim:
Install pathogen.vim

mkdir -p ~/.vim/autoload ~/.vim/bundle; \
curl -Sso ~/.vim/autoload/pathogen.vim \
    https://raw.github.com/tpope/vim-pathogen/master/autoload/pathogen.vim

Add the following to ~/.vimrc:

execute pathogen#infect()
syntax on
filetype plugin indent on

Install Syntastic 

cd ~/.vim/bundle
git clone https://github.com/scrooloose/syntastic.git

Then quit vim and used :Helptags to check installs so far have worked.
Install Merlin

opam switch 4.01.0dev+trunk
opam update
opam upgrade
opam install merlin

Add the following to ~/.vimrc

:set rtp+=~/.opam/4.01.0dev+trunk/share/ocamlmerlin/vim
:set rtp+=~/.opam/4.01.0dev+trunk/share/ocamlmerlin/vimbufsync
let g:syntastic_ocaml_checkers=['merlin']

:SyntasticInfo will return a list of syntax checkers available to Syntastic, check that this now includes merlin
Install OCP Indent

opam install ocp-indent

Add the following to ~/.vimrc

autocmd F…Read more...This is a quick run-through of how I set up my development environment in vim:
Install pathogen.vim

mkdir -p ~/.vim/autoload ~/.vim/bundle; \
curl -Sso ~/.vim/autoload/pathogen.vim \
    https://raw.github.com/tpope/vim-pathogen/master/autoload/pathogen.vim

Add the following to ~/.vimrc:

execute pathogen#infect()
syntax on
filetype plugin indent on

Install Syntastic 

cd ~/.vim/bundle
git clone https://github.com/scrooloose/syntastic.git

Then quit vim and used :Helptags to check installs so far have worked.
Install Merlin

opam switch 4.01.0dev+trunk
opam update
opam upgrade
opam install merlin

Add the following to ~/.vimrc

:set rtp+=~/.opam/4.01.0dev+trunk/share/ocamlmerlin/vim
:set rtp+=~/.opam/4.01.0dev+trunk/share/ocamlmerlin/vimbufsync
let g:syntastic_ocaml_checkers=['merlin']

:SyntasticInfo will return a list of syntax checkers available to Syntastic, check that this now includes merlin
Install OCP Indent

opam install ocp-indent

Add the following to ~/.vimrc

autocmd FileType ocaml source /home/heidi-ann/.opam/4.01.0dev+trunk/share/typerex/ocp-indent/ocp-indent.vim

Hide
        
      
                    by Heidi Howard at Sep 09, 2013 
      
      
    
  


       
                  OCamlot--exploring the edges of OPAM packages
      (Anil Madhavapeddy)
    
    
                                The new release of OCaml (4.01.0) was just announced!
The runup to a major release like this is normally a frantic time to test that
your favourite applications don’t break unexpectedly due to some incompatible
language feature.  This release cycle has a little different though, as we’ve
been working hard on using the OPAM package
database to build an online regression testing infrastructure to mechanize much
of this process.

I wanted to share some of what OCamlot does today, some of the results from
about 3 months worth of runs that may help OCaml package maintainers, and
finally where we’re going with future developments.  This work has been
done in collaboration with David Sheets (who
built the OCamlot daemon) and the OPAM team ably led by Thomas Gazagnaire at OCamlPro.
We’ve also had a lot of help from Jeremie Dimino and Yury Sulsky from Jane Street
and Dave Scott and Jon Ludlam from Citrix acting as guinea pigs for their respective regular releases of the Core
and Xen/XAP…Read more...The new release of OCaml (4.01.0) was just announced!
The runup to a major release like this is normally a frantic time to test that
your favourite applications don’t break unexpectedly due to some incompatible
language feature.  This release cycle has a little different though, as we’ve
been working hard on using the OPAM package
database to build an online regression testing infrastructure to mechanize much
of this process.

I wanted to share some of what OCamlot does today, some of the results from
about 3 months worth of runs that may help OCaml package maintainers, and
finally where we’re going with future developments.  This work has been
done in collaboration with David Sheets (who
built the OCamlot daemon) and the OPAM team ably led by Thomas Gazagnaire at OCamlPro.
We’ve also had a lot of help from Jeremie Dimino and Yury Sulsky from Jane Street
and Dave Scott and Jon Ludlam from Citrix acting as guinea pigs for their respective regular releases of the Core
and Xen/XAPI releases to OPAM.

Towards a truely portable OCaml

The upstream OCaml toolchain is built on very UNIX-like principles, with a
number of command-line tools that form a build pipeline.  This process usually
ends with linking the intermediate object files with a runtime library that provides the
garbage collector and other intrinsic OS functions.

Given these raw compiler tools, it’s very easy to compile OCaml into all
sorts of weird and wonderful architectures.  We’ve seen it run on
8-bit PICs,
several efficient Javascript backends (originally ocamljs and more recently js_of_ocaml),
and of course my own Mirage Xen unikernel.

While the compiler tools themselves are quite portable and work on almost any
UNIX-like system, the build system scaffolding around third-party packages is
less portable.  Some features such as C bindings often contribute to build
breakage on some less-used operating systems such as FreeBSD or OpenBSD, as
they usually require probing for header file locations or adding custom CFLAGS
before building.

Every Internet of Things starts with a tangled pile of ARM Dreamplugs.
And in our server room, venerable Sparc and PowerPC G4 OpenBSD boxen still live.
Finding older machines is getting tough, but here's Dave's old iMac G5 running Linux.



Most OCaml developers use x86-based machines and so foreign architectures also
get less day-to-day testing (OCaml has superb support for fast native code
compilation on ARM, PowerPC, Sparc32, and we’re working on MIPS64 here as part
of the CHERI
project).

We want to make sure that OCaml and its package ecosystem works just as well in
the embedded ecosystem as well as it does on vanilla x86 Linux.  This includes running on my
Linux iMac G5, my FreeBSD Raspberry Pi, my OpenBSD Pandaboard, or even on a
bytecode-only architecture like an OpenBSD/Sparc64.

In the longer term, this paves the way to reliable cross-compiled packages for Windows, Android and iPhone (all of which have OCaml ports, but aren’t heavily tested with the full package database).  The only practical way to get started on this is by building an automated test infrastructure for OCaml that explores the feature matrix (which eerily for me, happened in the early days of Xen too, via XenRT to stabilize the hypervisor).

Why not Jenkins or Travis?

When we first started hacking on the OPAM testing infrastructure earlier this
year, I maintained a local Jenkins installation that
monitored the repository.  While Jenkins is a superb tool for many continuous
integration tasks, it fell over badly when trying to use it on the non-x86 and
non-Linux (or Windows) operating systems.  Jenkins requires a full Java runtime
stack to be available on each of the client machines, which was taking up more
time to get up and running than a simple OCaml-based client and server that
could compile to both portable bytecode or fast native code.

The other difficulty with OPAM is selecting which packages actually need to be
tested, as it has a constraint-based package solver that supports expressive
forwards and backwards version restrictions.  While basic tests of the latest
packages worked with Jenkins, we needed to increasingly customize it to
automate interfacing directly with the OPAM libraries and calculating test
schedules based on incoming change requests.

Another factor that ruled out depending on hosted services such as
Travis is that they tend to support x86-only
architectures, whereas we really want to test the full spectrum of CPU
variants supported by OCaml.  This doesn’t mean that there’s no place for
Travis of course, and in fact Mike Lin has already made this work
with OPAM.

For our full testing needs though,
OCamlot was born: an OCaml client
and server system which coordinates different versions of the compiler,
architectures and OPAM versions and records the results for triage and
fixing issues.

Running OCamlot

The latest alpha release of OCamlot is pretty straightforward to run locally,
if you are so inclined.  First start a server process:

$ git clone git://github.com/ocamllabs/ocamlot
$ cd ocamlot
$ ./install_deps.sh
$ oasis setup
# (edit lib/config.ml if you need to change ports)
$ make
$ ./_build/lib/ocamlot_cmd.native --help
$ ./_build/lib/ocamlot_cmd.native serve


The server listens on localhost only by default, and normally an SSL-to-TCP
proxy is deployed to listen for external connections (I use
stud, which is fast and easy to configure).

The OCamlot clients require a local compilation of OCaml, and they autodetect
their local CPU architecture and operating system.  I’ve put a gist
script together that automates this on
most Linux and BSD variants.  Just customize the top variables (set MAKE to
gmake under BSD) and set the hostname to your server process.

The results repository and auto-triage

Once the client is running, the server dispatches tasks from its test matrix,
which is calculated from the OPAM package repository.  The server maintains a
results repository, which is a Git
filesystem database that records the build results and logs via an
s-expression per task.  It also uses Cohttp
to serve up the results for a web browser.

It’s very convenient using Git repositories like this since we can use GitHub
(or any other Git host) to coordinate and record results, and restart the
server without needing any local state.  So convenient, in fact, that Thomas
and I have been working on a more formal engine for this called
Irminsule (more on that in a later post,
though).

It’s almost unheard of to have a full OCamlot run go by without some errors,
and so David put together the ocamlot triage command.  This takes the
state repository and runs a set of regular expressions over it to classify them
into common errors.  The full file is
here, but an
excerpt should give you an idea of what we look for:

(* ordered minor to severe *)
	type analysis =
	  | Solver of solver_error option
	  | Dep of string * analysis
	  | Transient of transient_error
	  | System of system_error
	  | Meta of meta_error
	  | Ext_dep of ext_dep_error
	  | Build of build_error
	  | Multiple of analysis list
	    with sexp

The errors are ordered by severity to aid in color highlighting. They start with
OPAM solver failures and dependency failures (e.g. due to trying to build a
package that requires a specific OCaml version that isn’t available), and
move onto missing package dependencies or system libraris.

Testing the run up to OCaml 4.01

Of course, running all these tests is useless without taking action on the results. I’ve been
keeping track of them in issue #1029.
The nice thing about GitHub issues is that when this bug is referenced in commits (even in
other repositories) a cross-reference shows up on that webpage and lets everything be tracked
nicely.

So what were the most common failures in the runup to 4.01, and what should you avoid when
writing your own code?

Different standard library module signatures

There have been a few changes to some of the functor signatures in the standard library, such as
adding a find function to Set (mantis #5864).
A third-party library that tries to match the functor signature will fail to compile with a type error, such
as this one below for zipperposition.0.2:

Error: The implementation src/ptset.ml
       does not match the interface src/ptset.cmi:
       ...
       In module Big:
       The field `find' is required but not provided


The relevant code in zipperposition makes it clear what the problem is:

(* Big-endian Patricia trees *)
	module Big : sig
	  include Set.S with type elt = int
	  val intersect : t -> t -> bool
	end

This particular bug was reported
upstream, and the fix
requires implementing the find function for the Patricia-tree-based Set.
Since the OPAM package will always be broken on OCaml 4.01, it was marked with
a compiler version constraint
to prevent it being selected for installation under that compiler.  When a new
version with the fix is uploaded to OPAM, it will always be selected in
preference to this broken one.

One other 4.01 change that temporarily broke most of the bigger networking libraries such as
Core, Lwt and Batteries
was the addition of the close-on-exec flag to the Unix module.
This change only affects upstream packages that redefine the UNIX module for their own purposes (such as adding an asynchronous I/O monad as Lwt does),
hence it affects the standard library replacement packages.

The fix here was to locally add patches into the relevant OPAM packages to
immediately unbreak things when the fix when into the 4.01 branch of the
compiler, and notify upstream maintainers to release new versions of their
projects.  There’s a subtle problem here: when a patch such as this goes into
an unreleased branch of the compiler (such as 4.01.0dev), it’s hard to
reliably detect if the user has got the very latest version of the compiler or
not.
If you do have problems like this in the future, try recompiling via opam switch reinstall <version> to the latest branch.

It’s very useful to be able to drop in bleeding-edge compiler tools into the
OPAM repository using compiler constraints like this.  For an example, see
Alain Frisch’s ppx_tools,
that require the very latest 4.02dev trunk release to compile his new
extension-points feature.

Multiple object definitions

OCaml 4.01 also restricts multiple method definitions with the same name in the
same object.  This leaves only inheritance as the way to override method names,
but some packages such as OCamlnet and Mlorg had minor uses of the old mechanism.

You can see this by using opam switch:

$ opam switch 4.00.1
$ eval `opam config env`
$ ocaml
# object method x = 1 method x = 2 end;;
- : < x : int > = <obj>
$ opam switch 4.01.0
$ eval `opam config env`
$ ocaml
# object method x = 1 method x = 2 end;;
Error: The method `x' has multiple definitions in this object


New warnings, and the dreaded warnings-as-errors

After a decade of being deprecated, the (or) and (&) operators
finally had a warning turned on by default.

$ ocaml
# true or false;;
Warning 3: deprecated feature: operator (or); you should use (||) instead
- : bool = true


This wouldn’t normally be so bad, except that a surprising number of
released packages also turn warnings into fatal errors (by using the
-w @ flags explained in the manual).
Warnings-as-errors is extremely useful when developing code but is
rather harmful in released code, since a future compiler can choose
to emit new warnings that aren’t necessarily fatal bugs.

Packages that failed like this include ocamlgraph, spotlib, quickcheck,
OPA, Lablgtk-extras and many more.  Please do make an effort to not
leave this option turned on in your packages, or else it makes life
more difficult for testing your code on bleeding edge versions of the
compiler in the future.

It’s worth noting here that OCaml 4.01 has introduced a fair number
of new and very useful warnings across a number of areas, mainly to do
with detecting unexpected ambiguation or shadowing of values.  I’ll
cover more on these in a future post about the new 4.01 goodies.

External system dependencies

While there any many packages in OPAM that are pure OCaml, there are
also a substantial number that require other system tools to be installed.
The Lablgtk GUI library obviously requires the C gtk library to be
installed.

Determining if these libraries are installed on a particular OS is
well beyond the scope of OPAM, as there are almost as many package
managers as there are operating systems.  However, it’s important for
automated testing and user-friendly error messages to have some notion
of detecting if the environment is ready for the OCaml package or not.

We’re solving this by using a depexts field in OPAM that consists
of a set of tags that identify OS-specific packages that need to be
present.  A separate script can query these tags from OPAM and do the
OS-specific tests or installation.

For example, here’s the sqlite3-ocaml OPAM description:

opam-version: "1"
maintainer: "markus.mottl@gmail.com"
build: [
  ["ocaml" "setup.ml" "-configure"]
  ["ocaml" "setup.ml" "-build"]
  ["ocaml" "setup.ml" "-install"]
]
remove: [ ["ocamlfind" "remove" "sqlite3"] ]
depends: ["ocamlfind"]
depexts: [
  [ ["debian"  ] [ "libsqlite3-dev"]    ]
  [ ["freebsd" ] [ "database/sqlite3"]  ]
  [ ["openbsd" ] [ "database/sqlite3"]  ]
]


The depexts field here lists APT package for Debian, and the ports
tree locations for FreeBSD and OpenBSD.  It could also list more specialised
tags for particular versions of an OS.  You can query this from OPAM
as follows:

$ opam install -e debian sqlite3-ocaml
libsqlite3-dev
$ opam install -e openbsd sqlite3-ocaml
database/sqlite3


OCamlot therefore needs to query the depexts field from the package and
run the right apt or pkg_add commands.  I’ll write about this in more
detail when it’s fully baked, as we’ve modified the semantics of the tag
querying between OPAM 1.0 and OPAM 1.1 to make it easier to use in OCamlot.

Portable shell scripts

Once we’ve gotten past the hurdle of the compiler version causing failures, there is the
small matter of testing non-Linux operating systems, as well as non-x86
CPU architectures.  The #1029 overview
lists many of these failures under the Portability section.



Damien Doligez made some excellent points about how to write portable
Makefiles
that works across both GNU and BSD makes.  This is why the carefully crafted
OCaml Makefiles do not require GNU make to be installed when compiling on
FreeBSD or OpenBSD (MacOS X gave up the fight a long time ago and installs GNU
make as its default make command).

OPAM tries to help out BSD by providing a make macro in opam files that
is substituted with either "make" (by default) or "gmake" (for BSD).
While this works for for the toplevel invocation of the Makefile, it fails
if the Makefile recursively invokes further targets without using the $(MAKE)
variable instead of directly calling the command.
Patching these sorts of things is easy but tedious: see the patchfile for the
Facile constraint programming library for an example.

The real problem here, of course, is that package maintainers cannot be reasonably
expected to test their code on systems that they don’t normally use–if we demanded
perfect portability to be present in the main OPAM repository, we would’t get
any submissions!

OCamlot automates this nicely though, by finding lots of portability bugs
automatically, and maintainers are by-and-large very responsive when we report
the problem upstream.

The emerging distributed workflow

The big drawback to OCamlot in its current form is the amount of triage effort
it puts on the OPAM maintainers.  The package database has now exceeded 500
packages in just a short few months, and has over 1500 unique versions that
all need build testing and more accurate constraints.  The wider community has
been really keen to participate in helping with triage (just look at all the
other people that leapt in on bug #1029),
so its our immediate priority to make OCamlot more transparent for people that
want to use it to improve their own packages, and in the future also use it
to test various hypotheses about all the available open-source OCaml code
(see Jacques’ experiment with monomorphic let as
an example of something that can benefit from wider automated compilation).

I’ll talk more about how we’re solving this in my upcoming OCaml 2013
Workshop talk about the Platform.  I
don’t want spoil it too much, but it involves a lovely distributed Git
workflow, an enhanced opam2web, and a
brand new metadata overlay system for OPAM that lets us enhance the package
database with extra information such as statistics, portability and test
results, but without polluting the main Git repository with all this extra
non-essential data.

If you’re really curious to know right now, then you can see the outline of the
new system at Amir’s new ocaml.org
wireframes blog post,
where Part III contains the continuous integration workflow.  A lot of infrastructure
work has gone into building all of this over the summer, and now it’s all
starting to be deployed in a very satisfying way…
Hide
        
      
                    by Anil Madhavapeddy at Sep 09, 2013 
      
      
    
  


       
                  Option handling with OCaml polymorphic variants
      (Thomas Leonard)
    
    
                                After we selected OCaml as the new language for 0install, I’ve been steadily converting the old Python code across. We now have more than 10,000 lines of OCaml, so I thought it’s time to share what I’ve learnt.

OCaml is actually a pretty small language. Once you’ve read the short tutorials you know most of the language. However, I did skip one interesting feature during my first attempts:

  There are also “polymorphic variants” which allow the same field name to be used in different structures, but I haven’t tried using them.


I’ve since found a good use for these for handling command-line options…



The problem

The 0install command has many subcommands (0install run, 0install download, etc), which accept different, but overlapping, sets of options. Running a command happens in two phases: first we parse the options, then we pass them to a handler function. We split the parsing and handling because the tab-completion and help system also need to know which optio…Read more...After we selected OCaml as the new language for 0install, I’ve been steadily converting the old Python code across. We now have more than 10,000 lines of OCaml, so I thought it’s time to share what I’ve learnt.

OCaml is actually a pretty small language. Once you’ve read the short tutorials you know most of the language. However, I did skip one interesting feature during my first attempts:

  There are also “polymorphic variants” which allow the same field name to be used in different structures, but I haven’t tried using them.


I’ve since found a good use for these for handling command-line options…



The problem

The 0install command has many subcommands (0install run, 0install download, etc), which accept different, but overlapping, sets of options. Running a command happens in two phases: first we parse the options, then we pass them to a handler function. We split the parsing and handling because the tab-completion and help system also need to know which options go with which command.

Using plain (non-polymorphic) variants I originally implemented it a bit like this (simplified). I had a single type which listed all the possible options:

1
2
3
4
type zi_option =
  | Refresh		(* --refresh *)
  | Show		(* --show *)
  | Wrapper of string	(* --wrapper=echo *)


Each command handler takes a list of options and processes them:

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
let handle_run options =
  let refresh = ref false in
  let wrapper = ref None in
  ListLabels.iter options ~f:(function
    | Refresh -> refresh := true
    | Wrapper w -> wrapper := Some w
    | _ -> assert false   (* can't happen *)
  );
  (* use refresh/wrapper... *)
let handle_download options =
  let refresh = ref false in
  let show = ref false in
  ListLabels.iter options ~f:(function
    | Refresh -> refresh := true
    | Show -> show := true
    | _ -> assert false   (* can't happen *)
  );
  (* use refresh/show... *)


Each handler function has the same type: zi_option list -> unit (they take a list of options and return nothing).

Finally, there is a table of sub-commands, giving the parser and handler for each one:

1
2
3
4
let subcommands = [
  ("run", (parse_run, handle_run));
  ("download", (parse_download, handle_download));
]


But those assert false lines are worrying. An assert false means the programmer believes the code can’t be executed, but didn’t manage to convince the compiler. If we declare that a subcommand accepts a flag, but forget to implement it, the program will crash at runtime (this isn’t as unlikely as it sounds, because we declare options in groups, so adding an option to a group affects several subcommands).

Polymorphic variants

Polymorphic variants are written with a back-tick/grave before them, and you don’t need to declare them before use. For example, we can declare
handle_run like this:

1
2
3
4
5
6
7
8
let handle_run options =
  let refresh = ref false in
  let wrapper = ref None in
  ListLabels.iter options ~f:(function
    | `Refresh -> refresh := true
    | `Wrapper w -> wrapper := Some w
  );
  (* use refresh/wrapper... *)


OCaml will automatically infer the type of this function as:

[< `Refresh | `Wrapper of string ] list -> unit


That is, handle_run takes of list of options, where the options are a subset of Refresh and Wrapper. Notice that the assert is gone.

Now you can call handle_run (parse_run argv), and it’s a compile-time error if handle_run doesn’t handle every option that parse_run may produce.

There is, however, a problem when we try to put these functions in the subcommands list. OCaml wants every list item to have the same type, and so wants every subcommand to handle every option. The compile then fails because they don’t.

My first thought to fix this was to declare an existential type. e.g.

1
2
3
type 'a option_parser = string list -> 'a list
type 'a handler = 'a list -> unit
type subcommand = exists 'a. ('a option_parser * 'a handler)


I’m trying to say that each subcommand has a parser and a handler and, while we don’t know what subset of the options they process, the subsets are the same.
Sadly, OCaml doesn’t have existential types.

However, we can get the same effect by declaring a class or closure:

1
2
3
4
5
6
7
8
9
let subcommand option_parser handler =
  object
    method parse_and_run args = handler (option_parser args)
  end
let subcommands = [
  ("run", subcommand parse_run handle_run);
  ("download", subcommand parse_download handle_download);
]


This works because the subcommand function has a for-all type (for all types a, it accepts an a parser and an a handler and produces an object that doesn’t expose the type a in its interface: parse_and_run just has the type string list -> unit.

However, if we want to expose the parser on its own (e.g. for the tab-completion) we have to cast it first. Here, the parse method simply returns a zi_option list, losing the information about exactly which subset of the options it might return (which is fine for the completion code). This allows all subcommand objects to expose the same interface:

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
type zi_option =
  [ `Refresh
  | `Show
  | `Wrapper of string ]
let subcommand option_parser handler =
  object
    method parse args = (option_parser args :> zi_option list)
    method parse_and_run args = handler (option_parser args)
  end
let subcommands = [
  ("run", subcommand parse_run handle_run);
  ("download", subcommand parse_download handle_download);
]


So, I think this is rather nice:

  Every option displayed in the help for a command is accepted by that command.
  We don’t need any asserts in the handlers (indeed, adding the assert destroys the safety, since the handler will then accept any option).


One final trick: when matching variants you can use the #type syntax to match a set of options. e.g. the real handle_run looks more like this:

1
2
3
4
5
6
let select_opts = ref [] in
ListLabels.iter options ~f:(function
  | #common_option as o -> Common_options.process_common_option o
  | #select_option as o -> select_opts := o :: !select_opts
  | `Wrapper w -> wrapper := Some w
);


That is, it processes the run-specific options itself, while delegating common options (--offline, etc) and storing selection options (--version, etc) in a separate list to be passed to the selection code. The select_opts list gets the correct sub-type (select_option list).
Hide
        
      
                    by Thomas Leonard at Aug 31, 2013 
      
      
    
  


       
                  ICFP, CUFP & OCaml2013
      (Heidi Howard)
    
    
                                I’m busy planning my first trip across the Atlantic to attend ICFP, CUFP and OCaml 2013. Today, I’ve been given the duty of “live blogging” the event, over at the syslog, the Cambridge Systems Research Group blog.
My other job for the event is to improve the documentation for Janestreet’s Async library. if anyone else is keen, I would love to organise a doc-a-thon to populate the .mli files

        
      
                    by Heidi Howard at Aug 28, 2013 
      
      
    
  


       
          Introducing vchan
      (Mirage OS)
    
                                      Editor: Note that some of the toolchain details of this blog post are
now out-of-date with Mirage 1.1, so we will update this shortly.
Unless you are familiar with Xen's source code, there is little chance
that you've ever heard of the vchan library or
protocol. Documentation about it is very scarce: a description can be
found on vchan's
public header file,
that I quote here for convenience:
Originally borrowed from the
Qubes OS Project, this code (i.e. libvchan)
has been substantially rewritten [...]
This is a library for inter-domain communication.  A standard Xen ring
buffer is used, with a datagram-based interface built on top.  The
grant reference and event channels are shared in XenStore under a
user-specified path.


This protocol uses shared memory for inter-domain communication,
i.e. between two VMs residing in the same Xen host, and uses Xen's
mechanisms -- more specifically,
ring buffers
and
event channels
-- in order to achieve its aims. Datagram-based interface simp…Read more...      Editor: Note that some of the toolchain details of this blog post are
now out-of-date with Mirage 1.1, so we will update this shortly.
Unless you are familiar with Xen's source code, there is little chance
that you've ever heard of the vchan library or
protocol. Documentation about it is very scarce: a description can be
found on vchan's
public header file,
that I quote here for convenience:
Originally borrowed from the
Qubes OS Project, this code (i.e. libvchan)
has been substantially rewritten [...]
This is a library for inter-domain communication.  A standard Xen ring
buffer is used, with a datagram-based interface built on top.  The
grant reference and event channels are shared in XenStore under a
user-specified path.


This protocol uses shared memory for inter-domain communication,
i.e. between two VMs residing in the same Xen host, and uses Xen's
mechanisms -- more specifically,
ring buffers
and
event channels
-- in order to achieve its aims. Datagram-based interface simply
means that the
interface
resembles UDP, although there is support for stream based communication (like
TCP) as well.
Over the last two months or so, I worked on a pure OCaml
implementation of this library, meaning
that Mirage-based unikernels can now take full advantage of vchan to
communicate with neighboring VMs! If your endpoint -- a Linux VM or another
unikernel -- is on the same host, it is much faster and more efficient to use
vchan rather than the network stack (although unfortunately, it is currently
incompatible with existing programs written against the socket library under
UNIX or the Flow module of Mirage, although this will improve). It also
provides a higher level of security compared to network sockets as messages
will never leave the host's shared memory.
Building the vchan echo domain
Provided that you have a Xen-enabled machine, do the following from
dom0:
    opam install mirari mirage-xen mirage vchan

This will install the library and its dependencies. mirari is
necessary to build the echo unikernel:
    git clone git://github.com/mirage/ocaml-vchan
    cd test
    mirari configure --xen --no-install
    mirari build --xen
    sudo mirari run --xen

This will boot a vchan echo domain for dom0, with connection
parameters stored in xenstore at /local/domain/<domid>/data/vchan,
where <domid> is the domain id of the vchan echo domain. The echo
domain is simply an unikernel hosting a vchan server accepting
connections from dom0, and echo'ing everything that is sent to it.
The command xl list will give you the domain id of the echo
server.
Building the vchan CLI from Xen's sources
You can try it using a vchan client that can be found in Xen's sources
at tools/libvchan: Just type make in this directory. It will
compile the executable vchan-node2 that you can use to connect to
our freshly created echo domain:
    ./vchan-node2 client <domid>/local/domain/<domid>/data/vchan

If everything goes well, what you type in there will be echoed.
You can obtain the full API documentation for ocaml-vchan by doing a
cd ocaml-vchan && make doc. If you are doing network programming
under UNIX, vchan's interface will not surprise you. If you are
already using vchan for a C project, you will see that the OCaml API
is nearly identical to what you are used to.
Please let us know if you use or plan to use this library in any way!
If you need tremedous speed or more security, this might fit your
needs.

   Hide
        
      
                    by Vincent Bernardoff at Aug 23, 2013 
      
      
    
  


       
                  Real World OCaml beta3 release
      (Heidi Howard)
    
    
                                Beta3 of RWO is now available: https://realworldocaml.org/ and anil (one of the co-authors) comments on the release http://anil.recoil.org/2013/08/06/real-world-ocaml-beta2.html
Cover of RWO

        
      
                    by Heidi Howard at Aug 19, 2013 
      
      
    
  


       
          MirageOS travels to OSCON'13: a trip report
      (Mirage OS)
    
                                      Now that Mirage OS is rapidly converging on a
Developer Preview Release 1, we
took it for a first public outing at
OSCON'13, the O'Reilly Open Source
Conference. OSCON is in its 15th year now, and is a meeting place for
developers, business people and investors. It was a great opportunity to show
MirageOS off to some of the movers and shakers in the OSS world.
Partly because MirageOS is about synthesising extremely specialised guest
kernels from high-level code, and partly because both Anil and I are
constitutionally incapable of taking the easy way out, we self-hosted the
slide deck on Mirage: after some last-minute hacking -- on content not Mirage
I should add! -- we built a self-contained unikernel of the talk.
This was what you might call a "full stack" presentation: the custom
unikernel (flawlessly!) ran a type-safe
network device driver,
OCaml TCP/IP stack supporting an OCaml
HTTP framework that served slides
rendered using reveal.js. The slide deck,
including the turbo-bo…Read more...      Now that Mirage OS is rapidly converging on a
Developer Preview Release 1, we
took it for a first public outing at
OSCON'13, the O'Reilly Open Source
Conference. OSCON is in its 15th year now, and is a meeting place for
developers, business people and investors. It was a great opportunity to show
MirageOS off to some of the movers and shakers in the OSS world.
Partly because MirageOS is about synthesising extremely specialised guest
kernels from high-level code, and partly because both Anil and I are
constitutionally incapable of taking the easy way out, we self-hosted the
slide deck on Mirage: after some last-minute hacking -- on content not Mirage
I should add! -- we built a self-contained unikernel of the talk.
This was what you might call a "full stack" presentation: the custom
unikernel (flawlessly!) ran a type-safe
network device driver,
OCaml TCP/IP stack supporting an OCaml
HTTP framework that served slides
rendered using reveal.js. The slide deck,
including the turbo-boosted
screencast of the slide deck
compilation, is hosted as another MirageOS virtual machine at
decks.openmirage.org. We hope to add more
slide decks there soon, including resurrecting the tutorial! The source code
for all this is in the mirage-decks
GitHub repo.
The Talk

The talk went down pretty well -- given we were in a graveyard slot on Friday
after many people had left, attendance was fairly high (around 30-40), and the
feedback scores
have been positive (averaging 4.7/5) with comments including "excellent
content and well done" and "one of the most excited projects I heard about"
(though we are suspicious that just refers to Anil's usual high-energy
presentation style...).
   

Probably the most interesting chat after the talk was with the Rust authors
at Mozilla (@pcwalton and
@brson) about combining the Mirage
unikernel techniques
with the Rust runtime. But perhaps the most
surprising feedback was when Anil and I were stopped in the street while
walking back from some well-earned sushi, by a cyclist who loudly declared
that he'd really enjoyed the talk and thought it was a really exciting project
-- never done something that achieved public acclaim from the streets before
:)
Book Signing and Xen.org

Anil also took some time to sit in a book signing for his forthcoming
Real World OCaml O'Reilly book.  This is
really important to making OCaml easier to learn, especially given that
all the Mirage libraries are using it.  Most of the dev team (and especially
thanks to Heidi Howard who bravely worked
through really early alpha revisions) have been giving
us feedback as the book is written, using the online commenting system.
The Xen.org booth was also huge, and we spent quite a while plotting the
forthcoming Mirage/Xen/ARM backend. We're pretty much just waiting for the
Cubieboard2 kernel patches to be upstreamed (keep an
eye here) so that we can boot Xen/ARM VMs
on tiny ARM devices.  There's a full report about this on the
xen.org
blog post about OSCon.
Galois and HalVM

We also stopped by the Galois to chat with Adam
Wick, who is the leader of the
HalVM project at Galois. This is a similar
project to Mirage, but, since it's written in Haskell, has more of a focus
on elegant compositional semantics rather than the more brutal performance
and predictability that Mirage currently has at its lower levels.
The future of all this ultimately lies in making it easier for these
multi-lingual unikernels to be managed and for all of them to communicate more
easily, so we chatted about code sharing and common protocols (such as
vchan) to help interoperability.
Expect to see more of this once our respective implementations get more
stable.
All-in-all OSCON'13 was a fun event and definitely one that we look forward
returning to with a more mature version of MirageOS, to build on the momentum
begun this year!  Portland was an amazing host city too, but what happens in
Portland, stays in Portland...

   Hide
        
      
                    by Richard Mortier at Aug 08, 2013 
      
      
    
  


       
                  Final Real World OCaml beta; the good, the bad and the ugly
      (Anil Madhavapeddy)
    
    
                                The second and final public beta of Real World OCaml is now available:
  https://realworldocaml.org

Release notes:

      Over 2,000 comments from proofreaders have been resolved.  We realize that
reading early content is hard work, and hugely appreciate the spirited
feedback!  The book is now a week away from being handed over to the
O’Reilly production team for copyediting, so the window for changes are
limited after that.  Comments reset between milestones and so beta2 is a
clean slate; we’re still working through some remaining older issues.
  
      The chapters on first-class modules, parsing with Menhir, and objects
and classes have been significantly revised from beta1. Our thanks to
Leo White for contributing significantly to the latter two chapters.
  
      All the code snippets and terminal outputs are now mechanically generated.
The source code is as close to public domain as practical, at:
    https://github.com/realworldocaml/examples
  
      The final version…Read more...The second and final public beta of Real World OCaml is now available:
  https://realworldocaml.org

Release notes:

      Over 2,000 comments from proofreaders have been resolved.  We realize that
reading early content is hard work, and hugely appreciate the spirited
feedback!  The book is now a week away from being handed over to the
O’Reilly production team for copyediting, so the window for changes are
limited after that.  Comments reset between milestones and so beta2 is a
clean slate; we’re still working through some remaining older issues.
  
      The chapters on first-class modules, parsing with Menhir, and objects
and classes have been significantly revised from beta1. Our thanks to
Leo White for contributing significantly to the latter two chapters.
  
      All the code snippets and terminal outputs are now mechanically generated.
The source code is as close to public domain as practical, at:
    https://github.com/realworldocaml/examples
  
      The final version will have the installation chapter moved to be online
only, and we intend to publish updates there to elaborate on installation
and packaging mechanisms.
  
      Exercises will be available after we go into production, and also only be
available online. We really like the collaborative spirit of the commenting
system, and will likely extend this to collecting exercises from our readers
on an ongoing basis.
  


There’s been quite a bit of feedback and conversation about the book, so this
also seemed like a good point to checkpoint the process somewhat.

Crowd sourcing community feedback


Good: The decision to crowdsource feedback has been exhausting but very
worthwhile, with over 2,200 comments posted (and over 2,000
resolved by us too!).  O’Reilly has a similar platform called
Atlas that wasn’t quite ready when we started
our book, but I’d highly encourage new authors to go down this route and not
stick with a traditional editorial scheme.

It’s simply not possible for a
small group of technical reviewers to notice as many errors as the wider
community has. Having said this, it’s interesting how much more focussed and
critical the comments of our editor Andy Oram
were when compared to most of the wider community feedback, so the commenting system is
definitely a complement and not a replacement to the editorial process.

The GitHub requirement

Bad: After the first beta, we got criticized on a Hacker News thread
for passing around Github oAuth tokens without SSL.
This was entirely my fault, and I corrected the site to be pure-SSL within 24 hours.

Ugly: In my defence though, I dont want the authority that all the
reviewers have granted to me for their Github accounts!  We need just two
things to enable commenting: an identity service to cut down on spam comments,
and the ability to create issues in a public repository.  Unfortunately, Github’s
scope API requires you to also
grant us access to commit to public code repositories.  Add on the fact that
around 6,000 people have clicked through the oAuth API to review RWO, and you
start to see just how much code we potentially have access to.
I did try to reduce the damage by not actually storing the oAuth tokens on the
server-side.  Instead, we store it in the client using a secure cookie, so you
can easily reset your browser to log out.

It’s not just about authentication either: another reader
points out
that if they use GitHub during work hours, they have no real way of separating
the news streams that result.

Much of the frustration here is that there’s nothing I can do to fix this except
wait for GitHub to hopefully improve their service.  I very much hope that GitHub
is listening to this and has internal plans to overhaul their privilege management
APIs.

Infrastructure-free hosting

Good and Bad: One of my goals with the commenting infrastructure was to try
and eliminate all server-side code, so that we could simply publish the book
onto Github Pages and use JavaScript for the comment creation and listing.

This almost worked out. We still need a tiny HTTP proxy for comment creation,
as we add contextual information such as a milestone to every new comment to
make it easier to index.  Setting a milestone requires privileged access to the
repository and so our server-side proxy creates the issue using the
user-supplied oAuth token (so that it originates from the commenter), and then
updates it (via the bactrian account) to add the
milestone add insert a little contextual comment pointing back to the book
paragraph where the comment originated from.

Good: The other criticism from the online
feedback
was the requirement to have a Github login to read the book at all.  This is a
restriction that we intend to lift for the final release (which will be freely
available online under a CC-BY-NC-ND license),
but I think it’s absolutely the right decision to gateway early adopters to get
useful feedback.  Even if we lost 90% of our potential reviewers through the
Github auth wall, I don’t think we could have coped with another 10,000
comments in any case.

On the positive side, we didn’t have a single spam comment or other abuses of
the commenting system at all.

I’ve had quite a few queries been open-sourcing the
scripts that drive the server-side
commenting, and this on my TODO list for after the final book has gone to
production.

Auto-generating the examples

Bad: We tried for far too long during the book writing to stumble through
with manual installation instructions and hand-copied code snippets and 
outputs.  Some of our alpha reviewers pointed out vociferously
that spending time on installation and dealing with code typos was not a good
use of their time.

Good: Michael Bolin was entirely correct in his
criticism (and incidentally, one of our most superstar reviewers).  The latest
beta has an entirely mechanically generated toolchain that lets us regenerate
the entire book output from a cold start by cloning the examples
repository.  In retrospect, I should have written this infrastructure a year ago,
and I’d recommend any new books of this sort focus hard on automation from
the early days.

Luckily, my automation scripts
could crib heavily from existing open-source OCaml projects that had portions
of what we needed, such as uTop and
ocaml.org (and my thanks to
Jeremie Dimino and Christophe Troestler for
their help here).

Awesome: We’re hacking on a little surprise for the final online version of the
book, based on this build infrastructure.  Stay tuned!
Hide
        
      
                    by Anil Madhavapeddy at Aug 06, 2013 
      
      
    
  


       
                  OCaml Lecture Notes
      (Heidi Howard)
    
    
                                I just wanted to share these sildes from Yaron Minsky guest lecture at Princeton on “Abstractions and Types for Concurrent Programming”: [pdf]  to the COS 326 class and the notes from Cornell’s OCaml course called “Data Structures and Functional Programming“, edit the URL to see notes from different years

        
      
                    by Heidi Howard at Aug 01, 2013 
      
      
    
  


       
          Creating Xen block devices with MirageOS
      (Mirage OS)
    
                                      MirageOS is a
unikernel
or "library operating system" that allows us to build applications
which can be compiled to very diverse environments: the same code can be linked
to run as a regular Unix app, relinked to run as a FreeBSD kernel module,
and even linked into a
self-contained kernel which can run on the Xen
hypervisor.
Mirage has access to an extensive suite of pure OCaml libraries,
covering everything from Xen block and network virtual device drivers,
a TCP/IP stack, OpenFlow learning switches and controllers, to
SSH and HTTP server implementations.
I normally use Mirage to deploy applications as kernels on top of
a XenServer hypervisor. I start by
first using the Mirage libraries within a normal Unix userspace
application -- where I have access to excellent debugging tools --
and then finally link my app as a high-performance Xen kernel for
production.
However Mirage is great for more than simply building Xen kernels.
In this post I'll describe how I've been using Mirage…Read more...      MirageOS is a
unikernel
or "library operating system" that allows us to build applications
which can be compiled to very diverse environments: the same code can be linked
to run as a regular Unix app, relinked to run as a FreeBSD kernel module,
and even linked into a
self-contained kernel which can run on the Xen
hypervisor.
Mirage has access to an extensive suite of pure OCaml libraries,
covering everything from Xen block and network virtual device drivers,
a TCP/IP stack, OpenFlow learning switches and controllers, to
SSH and HTTP server implementations.
I normally use Mirage to deploy applications as kernels on top of
a XenServer hypervisor. I start by
first using the Mirage libraries within a normal Unix userspace
application -- where I have access to excellent debugging tools --
and then finally link my app as a high-performance Xen kernel for
production.
However Mirage is great for more than simply building Xen kernels.
In this post I'll describe how I've been using Mirage to create
experimental virtual disk devices for existing Xen VMs (which may
themselves be Linux, *BSD, Windows or even Mirage kernels).
The Mirage libraries let me easily
experiment with different backend file formats and protocols, all while
writing only type-safe OCaml code thats runs in userspace in a normal
Linux domain 0.
Disk devices under Xen
The protocols used by Xen disk and network devices are designed to
permit fast and efficient software implementations, avoiding the
inefficiencies inherent in emulating physical hardware in software.
The protocols are based on two primitives:
shared memory pages: used for sharing both data and metadataevent channels: similar to interrupts, these allow one side to signal the other

In the disk block protocol, the protocol starts with the client
("frontend" in Xen jargon) sharing a page with the server ("backend").
This single page will contain the request/response metadata, arranged
as a circular buffer or "ring". The client ("frontend") can then start
sharing pages containing disk blocks with the backend and pushing request
structures to the ring, updating shared pointers as it goes. The client
will give the server end a kick via an event channel signal and then both
ends start running simultaneously. There are no locks in the protocol so
updates to the shared metadata must be handled carefully, using write
memory barriers to ensure consistency.
Xen disk devices in MirageOS
Like everything else in Mirage, Xen disk devices are implemented as
libraries. The ocamlfind library called "xenctrl" provides support for
manipulating blocks of raw memory pages, "granting" access to them to
other domains and signalling event channels. There are two implementations
of "xenctrl":
one that invokes Xen "hypercalls" directly
 and one which uses the Xen userspace library libxc.
Both implementations satisfy a common signature, so it's easy to write
code which will work in both userspace and kernelspace.
The ocamlfind library
shared-memory-ring
provides functions to create and manipulate request/response rings in shared
memory as used by the disk and network protocols. This library is a mix of
99.9% OCaml and 0.1% asm, where the asm is only needed to invoke memory
barrier operations to ensure that metadata writes issued by one CPU core
appear in the same order when viewed from another CPU core.
Finally the ocamlfind library
xenblock
provides functions to hotplug and hotunplug disk devices, together with an
implementation of the disk block protocol itself.
Making custom virtual disk servers with MirageOS
Let's experiment with making our own virtual disk server based on
the Mirage example program, xen-disk.
First, install Xen, OCaml
and OPAM. Second initialise your system:
  opam init
  eval `opam config env`

At the time of writing, not all the libraries were released as upstream
OPAM packages, so it was necessary to add some extra repositories. This
should not be necessary after the Mirage developer preview at
OSCON 2013.
  opam remote add mirage-dev git://github.com/mirage/opam-repo-dev
  opam remote add xapi-dev git://github.com/xapi-project/opam-repo-dev

Install the unmodified xen-disk package, this will ensure all the build
dependencies are installed:
  opam install xen-diskWhen this completes it will have installed a command-line tool called
xen-disk. If you start a VM using your Xen toolstack of choice
("xl create ..." or "xe vm-install ..." or "virsh create ...") then you
should be able to run:
  xen-disk connect <vmname>

which will hotplug a fresh block device into the VM "<vmname>" using the
"discard" backend, which returns "success" to all read and write requests,
but actually throws all data away. Obviously this backend should only be
used for basic testing!
Assuming that worked ok, clone and build the source for xen-disk yourself:
  git clone git://github.com/mirage/xen-disk
  cd xen-disk
  make

Making a custom virtual disk implementation
The xen-disk program has a set of simple built-in virtual disk implementations.
Each one satisifies a simple signature, contained in
src/storage.mli:
type configuration = {
  filename: string;      (** path where the data will be stored *)
  format: string option; (** format of physical data *)
}
(** Information needed to "open" a disk *)

module type S = sig
  (** A concrete mechanism to access and update a virtual disk. *)

  type t
  (** An open virtual disk *)

  val open_disk: configuration -> t option Lwt.t
  (** Given a configuration, attempt to open a virtual disk *)

  val size: t -> int64
  (** [size t] is the size of the virtual disk in bytes. The actual
      number of bytes stored on media may be different. *)

  val read: t -> Cstruct.t -> int64 -> int -> unit Lwt.t
  (** [read t buf offset_sectors len_sectors] copies [len_sectors]
      sectors beginning at sector [offset_sectors] from [t] into [buf] *)

  val write: t -> Cstruct.t -> int64 -> int -> unit Lwt.t
  (** [write t buf offset_sectors len_sectors] copies [len_sectors]
      sectors from [buf] into [t] beginning at sector [offset_sectors]. *)
end

Let's make a virtual disk implementation which uses an existing disk
image file as a "gold image", but uses copy-on-write so that no writes
persist.
This is a common configuration in Virtual Desktop Infrastructure deployments
and is generally handy when you want to test a change quickly, and
revert it cleanly afterwards.
A useful Unix technique for file I/O is to "memory map" an existing file:
this associates the file contents with a range of virtual memory addresses
so that reading and writing within this address range will actually
read or write the file contents.
The "mmap" C function has a number of flags, which can be used to request
"copy on write" behaviour. Reading the
OCaml manual Bigarray.map_file
it says:
If shared is true, all modifications performed on the array are reflected
in the file. This requires that fd be opened with write permissions. If
shared is false, modifications performed on the array are done in memory
only, using copy-on-write of the modified pages; the underlying file is
not affected.


So we should be able to make a virtual disk implementation which memory
maps the image file and achieves copy-on-write by setting "shared" to false.
For extra safety we can also open the file read-only.
Luckily there is already an
"mmap" implementation
in xen-disk; all we need to do is tweak it slightly.
Note that the xen-disk program uses a co-operative threading library called
lwt
which replaces functions from the OCaml standard library which might block
with non-blocking variants. In
particular lwt uses Lwt_bytes.map_file as a wrapper for the
Bigarray.Array1.map_file function.
In the "open-disk" function we simply need to set "shared" to "false" to
achieve the behaviour we want i.e.
  let open_disk configuration =
    let fd = Unix.openfile configuration.filename [ Unix.O_RDONLY ] 0o0 in
    let stats = Unix.LargeFile.fstat fd in
    let mmap = Lwt_bytes.map_file ~fd ~shared:false () in
    Unix.close fd;
    return (Some (stats.Unix.LargeFile.st_size, Cstruct.of_bigarray mmap))

The read and write functions can be left as they are:
  let read (_, mmap) buf offset_sectors len_sectors =
    let offset_sectors = Int64.to_int offset_sectors in
    let len_bytes = len_sectors * sector_size in
    let offset_bytes = offset_sectors * sector_size in
    Cstruct.blit mmap offset_bytes buf 0 len_bytes;
    return ()

  let write (_, mmap) buf offset_sectors len_sectors =
    let offset_sectors = Int64.to_int offset_sectors in
    let offset_bytes = offset_sectors * sector_size in
    let len_bytes = len_sectors * sector_size in
    Cstruct.blit buf 0 mmap offset_bytes len_bytes;
    return () 

Now if we rebuild and run something like:
  dd if=/dev/zero of=disk.raw bs=1M seek=1024 count=1
  losetup /dev/loop0 disk.raw
  mkfs.ext3 /dev/loop0
  losetup -d /dev/loop0

  dist/build/xen-disk/xen-disk connect <myvm> --path disk.raw

Inside the VM we should be able to do some basic speed testing:
  # dd if=/dev/xvdb of=/dev/null bs=1M iflag=direct count=100
  100+0 records in
  100+0 records out
  104857600 bytes (105 MB) copied, 0.125296 s, 837 MB/s

Plus we should be able to mount the filesystem inside the VM, make changes and
then disconnect (send SIGINT to xen-disk by hitting Control+C on your terminal)
without disturbing the underlying disk contents.
So what else can we do?
Thanks to Mirage it's now really easy to experiment with custom storage types
for your existing VMs. If you have a cunning scheme where you want to hash block contents,
and use the hashes as keys in some distributed datastructure -- go ahead, it's
all easy to do. If you have ideas for improving the low-level block access protocol
then Mirage makes those experiments very easy too.
If you come up with a cool example with Mirage, then send us a
pull request or send us an email to the
Mirage mailing list -- we'd
love to hear about it!

   Hide
        
      
                    by Dave Scott at Jul 18, 2013 
      
      
    
  


       
                  Profiling OCaml – Getting Started Guide
      (Heidi Howard)
    
    
                                “Perf” is a common command line linux tool used for code profiling, (perf wiki). A alpha version of a OCaml native code compiler that output code, that can be analysis by perf is now avalaible in OPAM
Installation
Installing the perf compatible OCaml compiler is straight forward with OPAM, though quite time-consuming due to the need to re-install many packages

$ opam remote add perf git://github.com/mshinwell/opam-repo-dev
$ opam switch 4.01-perf-annotate
$ eval `opam config env`
$ opam install

Installing perf was also straight forward, in fact I already had it via the linux-tools package in apt-get for Ubuntu.

sudo apt-get install linux-tools

UsageCompiling with the new perf-compatable Ocaml compiler was beautifully simple, running make within an existing project working first time without any further changes necessary.
BasicsBasic reporting is collected and viewed using:

sudo perf record ./myprogram.native -o myreport.data
sudo perf report -i myreport.data
sudo perf script…Read more...“Perf” is a common command line linux tool used for code profiling, (perf wiki). A alpha version of a OCaml native code compiler that output code, that can be analysis by perf is now avalaible in OPAM
Installation
Installing the perf compatible OCaml compiler is straight forward with OPAM, though quite time-consuming due to the need to re-install many packages

$ opam remote add perf git://github.com/mshinwell/opam-repo-dev
$ opam switch 4.01-perf-annotate
$ eval `opam config env`
$ opam install

Installing perf was also straight forward, in fact I already had it via the linux-tools package in apt-get for Ubuntu.

sudo apt-get install linux-tools

UsageCompiling with the new perf-compatable Ocaml compiler was beautifully simple, running make within an existing project working first time without any further changes necessary.
BasicsBasic reporting is collected and viewed using:

sudo perf record ./myprogram.native -o myreport.data
sudo perf report -i myreport.data
sudo perf script -i myreport.data

Similarly basic stats can be collected using:

sudo perf stat ./myprogram.native -o myreport.data
sudo cat myreport.data

When finished you can switch back to your normal compiler version, i.e.

$ opam switch 4.00.1
$ eval `opam config env`

Hide
        
      
                    by Heidi Howard at Jul 10, 2013 
      
      
    
  


       
                  OCaml binary compatibility
      (Thomas Leonard)
    
    
                                In the initial language analysis, OCaml did well in most areas except for diagnostics (which turned out to have an easy solution) and shared libraries / binary compatibility. Now it’s time to look for a solution to that.



Table of Contents

  Introduction
  Distribution packages
  Upstream packages
  OCaml library compatibility
  Windows / Linux compatibility          Statically-linking both versions
      Using the ocaml interpreter to link
      Using Dynlink
      Timings
    
  
  Conclusions
  Update (2013-07-14)


Introduction

0install 2.3 was released last week with an (optional) OCaml front-end. This code can handle the startup-time-critical operations of running applications and generating shell tab-completions by itself, and will fall back to the Python version for any other case.

Converting the Python to OCaml was mostly straight-forward. The only difficulty was getting access to the SHGetFolderPath function on Windows. The standard library doesn’t include this fun…Read more...In the initial language analysis, OCaml did well in most areas except for diagnostics (which turned out to have an easy solution) and shared libraries / binary compatibility. Now it’s time to look for a solution to that.



Table of Contents

  Introduction
  Distribution packages
  Upstream packages
  OCaml library compatibility
  Windows / Linux compatibility          Statically-linking both versions
      Using the ocaml interpreter to link
      Using Dynlink
      Timings
    
  
  Conclusions
  Update (2013-07-14)


Introduction

0install 2.3 was released last week with an (optional) OCaml front-end. This code can handle the startup-time-critical operations of running applications and generating shell tab-completions by itself, and will fall back to the Python version for any other case.

Converting the Python to OCaml was mostly straight-forward. The only difficulty was getting access to the SHGetFolderPath function on Windows. The standard library doesn’t include this function, so I had to write a wrapper for it in C, and use the OCaml pre-processor to make the OCaml use my wrapper only on Windows.

However this OCaml front-end means we now have some duplicated code, which must be kept in sync and creates extra opportunities for bugs. So the next step is to eliminate the duplicated Python code and use the OCaml in all cases. This means that the OCaml part of 0install will no longer be optional, which in turn means that it has to work for everyone.

There are three ways people end up running the 0install code:

  They install the zeroinstall-injector package from their distribution.
  They install manually using a generic tarball or Windows installer from 0install.net.
  They run a tool (e.g. 0compile) that depends on 0install (and will often require a newer version than was provided by their distribution).


This last case may seem a little confusing. The user is using their local (probably distribution-provided) 0install to run 0compile with its libraries, one of which happens to be another version of 0install.

Distribution packages

Distribution packages are the simplest from a binary-compatibility point of view. Each distribution runs a build farm, which builds separate binary packages for each supported architecture. Even here, compatibility can be an issue however. For example, if someone hits a bug in the version of 0install in the stable/LTS version of the distribution, we often tell them to try the package from a newer release.

OCaml provides two options when compiling:

  ocamlc compiles to bytecode.
  ocamlopt compiles to native platform-specific code. This is not available on all platforms.


The Debian OCaml packaging guide says that “The bytecode versions are portable. In order to spare the buildds and the Debian archive, bytecode versions should be compiled once for all for big packages (which either take a lot of place on disks or take a lot of time to build).”

However, this turned out not to be the case. Packages compiled on 64-bit systems didn’t install on 32-bit systems. I had to change the Debian source package to build a different binary for each architecture (and since I had to do that anyway, I also changed it to compile to native code where possible, since that’s slightly faster and more portable between distribution releases).

Upstream packages

For making upstream packages, we don’t have the ability to build (or test) native binaries for multiple platforms. It would be far more convenient to release a single package containing bytecode and have it work everywhere, the way we currently do with the Python. However, there are some problems to solve here:

      The 64-bit issue which affected the Debian packages, as noted above.
  
      OCaml bytecode compiled on Linux doesn’t run on Windows.
  
      Even backwards compatible changes to OCaml libraries prevent bytecode from linking (see next section). This includes e.g. OCaml adding a new function to its standard library.
  


I tried to reproduce the 64-bit issue by building the bytecode on a 32-bit Ubuntu Raring VM and then running it on a 64-bit Arch Linux system. It worked fine. So, I’m going to assume for now that this is an unnecessary incompatibility introduced by Debian’s OCaml packaging system, and not a genuine problem with the bytecode.

OCaml library compatibility

To understand how OCaml checks bytecode compatibility, let’s look at a simple example (based on the one in
Enforcing type-safe linking using package dependencies):

Say you have a library providing a function:

lib.ml 
1
let inc x = x + 1


You can compile it to bytecode like this, getting a lib.cmo file:

$ ocamlc -c lib.ml


You can compile a program using the library in the same way (note that module
names are capitalised in OCaml code):

prog.ml 
1
Printf.printf "Result: %d\n" (Lib.inc 5)


$ ocamlc -c prog.ml


Then you can link them together and run like this:

$ ocamlc -o prog lib.cmo prog.cmo
$ ./prog
Result: 6


If you change the implementation of the function, it still works:

lib.ml 
1
let inc x = x + 100


$ ocamlc -c lib.ml
$ ocamlc -o prog lib.cmo prog.cmo
$ ./prog
Result: 105


But, if you add a new function to the library then it breaks:

lib.ml 
1
2
let inc x = x + 1
let dec x = x - 1


$ ocamlc -c lib.ml
$ ocamlc -o prog lib.cmo prog.cmo
File "_none_", line 1:
Error: Files prog.cmo and lib.cmo
   make inconsistent assumptions over interface Lib


The reason is that OCaml calculates a hash over the module’s
signature. You can see a .cmo file’s dependencies (with their hashes)
like this:

$ ocamlobjinfo prog.cmo
File prog.cmo
Unit name: Prog
Interfaces imported:
    265928798c0b8a63fa48cf9ac202f0ce        Int32
    10fca44c912c9342cf3d611984d42e34        Printf
    3f6c994721573c9f8b5411e6824249f4        Buffer
    ad977b422bbde52cd6cd3b9d04d71db1        Obj
    5c4b312910d7250e3a67621b317619f0        Prog
    4836c254f0eacad92fbf67abc525fdda        Pervasives
    8ce323e7f6c1a7ba1b604d93cde0af3d        Lib
Uses unsafe features: no
Force link: no


The hash for “Lib” covers the “inc” and “dec” functions together and
OCaml refuses to link prog with the new library, even though all the
functions it needs are still there, unchanged.

At first, I thought we could just disable the hash checks and use some cleverer
tools to check that libraries remained backwards compatible. However, OCaml doesn’t
use symbol names to find functions in OCaml libraries. A module is just an array of
values (the inc and dec closures in this case) and prog locates the function it
wants by index. Here’s prog and its compiled bytecode (comments added by me):

prog.ml 
1
Printf.printf "Result: %d\n" (Lib.inc 5)


$ dumpobj prog.cmo
## start of ocaml dump of "prog.cmo"
   0  CONSTINT 5
   2  PUSHGETGLOBALFIELD Lib, 0		(* Lib[0] = Lib.inc *)
   5  APPLY1 				(* Call inc with 1 argument *)
   6  PUSHGETGLOBAL "Result: %d\n"
   8  PUSHGETGLOBALFIELD Printf, 1	(* Printf[1] = Printf.printf *)
  11  APPLY2 				(* Call printf with 2 arguments *)
  12  ATOM0 				(* The empty array *)
  13  SETGLOBAL Prog			(* Prog = [] *)
## end of ocaml dump of "prog.cmo"


So, it’s as if we’d written:

prog.ml 
1
Printf.[1] "Result: %d\n" (Lib.[0] 5)


Therefore, OCaml cannot cope with any change to the signature of a library.
For example, if the inc and dec functions are switched around so that dec
is defined first, prog will then call the dec function instead. The hashes
allow OCaml to detect such changes and refuse to run the bytecode.

To allow dynamic linking, there are several options:

      Disable the hash checks and then ensure that we never make a backwards incompatible change
(e.g. we only add new methods at the end of a module, never change signatures, etc). That would require
a bit of care, and it wouldn’t help with changes to libraries we don’t control.
  
      Export a series of submodules: ZeroInstall.APIv1, ZeroInstall.APIv2, etc. Then we only ever
create new modules; we never change existing ones. That works with OCaml’s existing hash scheme, but it
also doesn’t support third-party libraries (e.g. Libxmlm and the standard library).
  
      Write a front-end for ocamlrun that dynamically compiles and caches everything on demand. That’s rather 
inefficient for users, though, and requires installing a complete development environment everywhere. Also, 
it may make the first run of an upgraded program very slow, with potentially no way to display a progress 
indicator.
  
      Make the compiler add a map of symbol names to each module and use that for dynamic linking, based on
the Dynlink module module.
That would be the most useful, but also the most difficult to implement. You’d also need extra code to
handle extensions to class interfaces, new tags in variant types, etc. Not impossible (languages like
Java and C# do this well), but not simple either.
  


For now, we can just statically link all bytecode (which OCaml does by default) and build all libraries from source on the build system. That’s not a long-term solution, because every time we made a new release of 0install we’d have to make new releases of all the tools that depend on it (0compile, 0test, 0release, etc). But we’re not yet trying to provide an OCaml API to other tools, just a portable OCaml binary. We won’t get automatic updates to the libraries we use (e.g. Xmlm and Yojson), but we can probably live with that for now.

Windows / Linux compatibility

The cause of the Windows / Linux incompatibility is the “Unix” module in the standard library. Despite the name, this includes general-purpose operating system functions such as rename, create_process, etc, and is used on Windows too.

However, there are actually two separate unix.ml modules in the OCaml source: ./otherlibs/unix/unix.ml and ./otherlibs/win32unix/unix.ml. When you compile OCaml bytecode, it will statically link one of these versions, which means that the generated bytecode will support only the platform on which it was built.

In the short-term, we could create separate binaries of 0install for Windows and Linux. However, that makes the release process more complicated and error-prone. And if we provide an OCaml API to other tools, everyone developing tools would need to produce separate binaries too.

The two modules implement the same interface (i.e. they have the same hash), so code compiled for one would work with the other if it could find it. I experimented with several approaches here:

Statically-linking both versions

My first attempt was to make a single module that contained code to support both Windows and Linux. The most natural thing to do here would be to create a class type (interface) with two implementing classes. However, the Unix interface is a module, not a class, and I wanted it to be compatible with existing code. Asking on the Ocaml Beginners group, Gabriel Scherer recommended the first-class modules extension, which allows treating modules as values (it’s called an “extension”, but it’s supported by the standard compiler). So here’s my first attempt, which defines RealUnix and Win32 submodules and then sets Unix to the correct one at runtime (Posix contains the current Unix API):

portable_unix.ml 
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
module type Posix = module type of Posix
module RealUnix : Posix =
  struct
    [ contents of unix/unix.ml ]
  end
module Win32 : module type of Posix =
  struct
    [ contents of win32unix/unix.ml ]
  end
module Unix = (val
  match Sys.os_type with
  | "Win32" -> (module Win32 : Posix)
  | _ -> (module RealUnix : Posix)
  );;


One problem with this is that it needs to link against all the C symbols for both versions, so you need to provide stubs for win_waitpid on Unix and for unix_waitpid on Windows, etc. Only the OCaml code is linked statically into the executable; C libraries are resolved dynamically on the target platform. Turns out, there are quite a lot of stub symbols to define. For testing, I just hacked it to stop complaining about missing C primitives.

It almost worked, expect that I got a strange error on Windows trying to resolve the hostname “0.0.0.0” (which the Win32 version does during initialisation). However, I didn’t track it down because I got a better suggestion…

Using the ocaml interpreter to link

Gerd Stolpmann suggested just compiling to a library (not an executable) and then using a script to load the modules dynamically:

launcher.ml 
1
2
3
#!/usr/bin/env ocaml
#load "unix.cma";;
#load "myprog.cma";;


The advantage here is that we don’t ship copies of unix.ml with our code; we just use the one that comes with the runtime. However, this also has a few problems:

  It’s a bit slower.
  It depends on the ocaml binary (1.3 MB), not just ocamlrun (170 KB).
  For other libraries (e.g. Xmlm, Yojson), if we want to link statically, we have to include the whole library archive, not just the modules we need, because the OCaml compiler only knows what we need when it does the final link to generate an executable, which we’re not doing here.


Still, if ocaml can link unix.cma dynamically, why can’t we?

Using Dynlink

OCaml comes with the Dynlink module, which allows loading bytecode at runtime. However, it has quite a few limitations. Unlike ocaml, it doesn’t search for the library in the default paths (easy enough to fix), doesn’t load dependencies recursively, and doesn’t let you access the module after you’ve loaded it (it’s intended for plugins, where the plugin knows the API for the main system, not for libraries where the main program knows the API of the library).

I had a dig through the ocaml code to see how it does it. It seems to find the names using the Symtable module. I couldn’t find a public API for that, so I hacked the Dynlink module to export a lookup_module function (it needs better error reporting; this is just for testing):

dynlink.ml 
1
2
3
4
5
6
let lookup_module name =
  try Symtable.get_global_value (Ident.create_persistent name)
  with Symtable.Error ex ->
    Symtable.report_error Format.err_formatter ex;
    Format.pp_print_flush Format.err_formatter ();
    raise Not_found


Now you can find modules after loading them, and use the first-class modules stuff to treat the result as a regular module:

test_dyn.ml 
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
(* Load Unix module dynamically *)
open Dynlink
let libdir =
  if Sys.os_type = "Win32" then "c:\\ocamlmgw\\lib"
  else "/opt/ocaml/lib/ocaml"
let () =
  try
    let _ = Callback.register in
    allow_unsafe_modules true;
    Dynlink.loadfile (Filename.concat libdir "unix.cma");
  with Error ex -> failwith (error_message ex)
module type UnixType = module type of Unix
module Unix = (val (Obj.obj (lookup_module "Unix")) : UnixType)
(* Use Unix module in the normal way *)
open Unix
let child =
  create_process "gpg" (Array.of_list ["gpg"; "--version"])
                 stdin stdout stderr;;
Printf.printf "Child %d\n" child;;
match snd (Unix.waitpid [] child) with
| WEXITED 0 -> print_endline "Success!"
| _ -> failwith "Failure!"


Yes, there are some hard-coded paths in there. We could fix that easily enough, or get 0install to set them for you. The dummy reference to Callback.register is to ensure Callback gets linked (it’s a dependency of Unix, but Dynlink doesn’t handle dependencies).

I also had to modify the Dll module to use the correct extension (.so or .dll) based on the current platform.
The original version used whatever extension was correct for the platform where the code was compiled.

With that, it’s now working: code compiled on Linux runs on Windows and vice versa!

  GitHub repository


Timings

All approaches are reasonably fast (faster than Python, anyway, and this use isn’t speed critical):

            Time
      Method
    
  
            7 ms
      Static native code (not portable)
    
          10 ms
      Static bytecode (not portable)
    
          11 ms
      Bytecode using Dynlink
    
          20 ms
      Using #load with ocaml
    
          26 ms
      Python 2
    
          60 ms
      Python 3
    
  


On balance, I think we should go for the #load trick for now. It’s a bit less efficient than using Dynlink, but it doesn’t require any modifications to the OCaml libraries and it handles recursive dependencies. Also, it doesn’t require any changes to code.

The times above are for the “gpg –version” test script. This shell script and launcher can be used to test the actual 0install OCaml code:

launch.sh 
1
ocaml `ocamlfind query -r -i-format yojson xmlm` ./launch.ml "$@"


launch.ml 
1
2
3
4
5
6
7
8
#load "easy_format.cmo";;
#load "biniou.cma";;
#load "yojson_biniou.cmo";;
#load "yojson.cmo";;
#load "xmlm.cmo";;
#load "str.cma";;
#load "unix.cma";;
#load "main.cma";;


Times are around 40 ms, compared to 10 ms for static byte-code and 5 ms for native code. We should be able to get 0install to pass the -I flags itself, if we want to avoid calling ocamlfind and using a shell script.

Conclusions

I think that dynamically linking the Unix module, as described above, is sufficient for the next step in converting 0install: we should be able to ship cross-platform bytecode that statically links all libraries except Unix and which works everywhere. It will run with an unmodified ocaml and unix.cma, providing the runtime versions match exactly the compile-time ones. Essentially, that means that we ship binaries of ocaml though 0install and just stick with a single version for as long as possible. Fixing that will have to wait for later.

Update (2013-07-14)

Using #load isn’t safe. When you do ocaml /path/to/script.ml, it adds the current directory (not the directory containing the script) to the start of the search path. Thus:

$ cd /tmp
$ /usr/bin/myprog


will first try to load myprog’s libraries (e.g. unix.cma) from /tmp!

Looks like we will need to do something with Dynlink after all.
Hide
        
      
                    by Thomas Leonard at Jul 07, 2013 
      
      
    
  


       
                  Replacing Python: second round
      (Thomas Leonard)
    
    
                                In the first post, I took a brief look at the programming
languages ATS, C#, Go, Haskell, OCaml, Python and Rust to try to decide which would be the best language in which to write 0install (which is currently implemented in Python). Now it’s time to eliminate a few candidates and look in more detail at the others.

Last time, I converted 4 lines of Python code from 0install into each language. This time I’m converting 576 lines, so this should
give a more accurate picture of how each performs for a real-world task.



As before, note that I’m a beginner in these languages. Please post corrections or suggestions of better ways of doing things
in the comments. Thanks.

(this post also appeared on reddit and on Hacker News, where there are more comments)

Table of Contents

  Conclusions from last time
  Test case
  Syntax
  The “run” module
  Data structures
  Variants
  Using the data structures
  Handling XML
  Building lists
  String processing
  Finding resources
  API do…Read more...In the first post, I took a brief look at the programming
languages ATS, C#, Go, Haskell, OCaml, Python and Rust to try to decide which would be the best language in which to write 0install (which is currently implemented in Python). Now it’s time to eliminate a few candidates and look in more detail at the others.

Last time, I converted 4 lines of Python code from 0install into each language. This time I’m converting 576 lines, so this should
give a more accurate picture of how each performs for a real-world task.



As before, note that I’m a beginner in these languages. Please post corrections or suggestions of better ways of doing things
in the comments. Thanks.

(this post also appeared on reddit and on Hacker News, where there are more comments)

Table of Contents

  Conclusions from last time
  Test case
  Syntax
  The “run” module
  Data structures
  Variants
  Using the data structures
  Handling XML
  Building lists
  String processing
  Finding resources
  API docs
  Speed
  Conclusions


Conclusions from last time

Based on the initial evaluation and feedback (thanks everyone!), I’m not going to look any further at these:

  ATS
  I’m glad I included ATS. Its excellent performance and tiny binary put the other languages into perspective. Still, it’s
too hard to use, and makes it too difficult to separate safe code from unsafe code.
  C#
  Although C# is widely used, has an excellent cross-platform bytecode format and many libraries available, it is too
large and too slow to start for 0install (to the people who suggested profiling: when even “Hello World” is too slow,
there isn’t much you can do).
  Go
      Although many Go users complained that Go’s score was unfairly low, they didn’t seem to disagree that it was the worst of
the candidates for our requirements, only about by how much it was the worst. To summarise the discussion briefly:

                  Go is good because errors are handled where they occur. Maybe, but ATS, Haskell, OCaml and Rust do that too, and they
help you get it right.
      
              You can write pretty reliable code in Go. No doubt, since you can do it in C too. But maybe I can write even better code
with less effort using the other languages?
      
              It’s OK to ignore errors silently in some places, because handling all errors would clutter the code up too much. This
seems like a trade-off the other languages don’t require me to make.
      
    
  
  Rust
  Rust has excellent safety and a familiar imperative syntax. It also has excellent support for shared libraries, which I
didn’t understand when I wrote the previous post (although I don’t feel too bad about this as it seems that almost no-one
else in the Rust community understood it either). Speed is OK but not great, though likely to improve. Rust’s main
weakness is its immaturity. The language is likely to change in incompatible ways in the near future and there are few
libraries available (for example, there is no XML library for Rust). It will be a few years before this is usable in
production (and the developers make no secret of this).


So, here are the remaining candidates and a summary of the conclusions from last time:

  Haskell (7.6.3)
  Haskell is fast, but it has problems with shared libraries: libraries are only compatible when compiled by the
exact same version of the compiler. Its pure functional style may make it very difficult to convert the existing code
to Haskell. Diagnostics (e.g. getting stack-traces) may be a problem too.
  OCaml (4.00.1)
  OCaml is also fast and had good safety, but its support for shared libraries seems limited. Getting good diagnostics from
production code may be tricky, as enabling stack-traces has a performance cost (OCaml code assumes exceptions are cheap).
  Python (2.7.5 and 3.3.2)
  Python is much slower than the other candidates, but it does have the advantages of an excellent standard library,
easy distribution (no need to make platform-specific binaries), being the language we’re
currently using, and being very well known. But it has no static type
checking, which means a lot of work writing unit-tests for even trivial code (e.g. testing __repr__ methods
and logging in obscure error paths to make sure it won’t crash due to a typo).


Several people said that D has improved a lot in the last
few years. I didn’t have time to look at it carefully again, though. It’s a nice
language, but probably too low-level/unsafe for us. For example, it’s trivial to make
it segfault by dereferencing a null pointer.

Test case

0install collects information about all available versions of a program and its libraries from around the
web. Then it runs a solver to determine the best valid combination and writes the results to an XML
selections document. For an app, foo, the current
selections can be found in ~/.config/0install.net/apps/foo/selections.xml.

When it’s time to run an application, we read this XML, set up a suitable environment for the process
and then exec it. As before, this should happen as quickly as possible.

The test program can be given either the name of an app (e.g. “foo”) or the path to a selections document.

This task involves:

  Using the XDG basedir spec to
find the current selections and the cached software.
  Parsing XML into a suitable data structure.
  Manipulating pathnames, files, directories and symlinks.
  Updating environment variables based on the requested bindings.
  String manipulation.
  Creating launcher executables (as in the previous test).


We can’t go over every line in this post, so I’ll just highlight the most interesting bits.
The full version in each language is in this GitHub repository.
If you get bored, there’s a summary at the end.

Syntax

Python

I guess most people are familiar with Python. It’s clear, simple and straight-forward. It uses indentation
to see when a block ends, which means that the structure of a program is exactly what it looks like.
Here’s a sample of the Python main module:

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
config = get_default_config()
args = sys.argv[1:]
if not args:
	raise Exception("Syntax: runsels.py [APP | SELECTIONS] [ARGS]")
app_or_sels = args[0]
app_args = args[1:]
app_mgr = apps.AppMgr(config)
app_path = app_mgr.lookup_app(app_or_sels, missing_ok = True)
sels_path = join(app_path, "selections.xml") if app_path is not None else app_or_sels
with open(sels_path, 'rb') as stream:
	sels = qdom.parse(stream)
runner = run.Runner(config)
runner.execute_selections(sels, app_args)


Note: The real Python code uses the “optparse” module to handle option parsing and generate help text. However,
because the Haskell/OCaml code developed here will be used as a front-end to the real Python, we don’t want any
special handling. For example, if invoked with --help they should fall back to the Python version instead of
handling it themselves. So proper option parsing isn’t part of this task.

OCaml

OCaml syntax is very compact, but somewhat prone to confusing syntax errors. Often an error reported at a
particular line means you used “;” rather than “;;” 20 or 30 lines earlier. Here’s the OCaml main module:

1
2
3
4
5
6
7
8
9
10
11
12
let () =
  let config = Config.get_default_config () in
  match List.tl (Array.to_list Sys.argv) with
  | [] -> failwith "usage: runsels selections.xml arg..."
  | (app_or_sels :: args) ->
      let sels_path = match Apps.lookup_app app_or_sels config with
      | None -> app_or_sels
      | Some app_path -> app_path +/ "selections.xml" in
      let doc = Qdom.parse_file sels_path in
      let selections = Selections.make doc in
      Run.execute_selections selections args config
;;


Some useful things to know when reading the examples:

  let a = b in ... assigns a variable (well, constant), like a = b; ... in Python.
  let foo a b = ... defines a function, like def foo(a, b): ... in Python.
  foo (a + 1) b means to call foo with two arguments, like foo(a + 1, b) in Python.
  foo a creates a partially applied function, like functools.partial(foo, a) in Python.
  () means no data (you can think of it as an empty tuple).
  Module names start with a capital letter (e.g. Run.execute_selections).


We assign the result of Run.execute_selections to () just to check that it didn’t return anything (so if it
did, we’d get a type error rather than just ignoring it).

The syntax tends to emphasise the body of functions while minimising the signature, which I’m not
convinced is a good thing. Consider:

1
2
3
4
5
6
7
8
let get_source b =
  let get name = ZI.get_attribute_opt name b in
  match (get "insert", get "value") with
  | (None, None) -> failwith "Missing 'insert' or 'value'"
  | (Some i, None) -> InsertPath i
  | (None, Some v) -> Value v
  | (Some i, Some v) -> failwith "Can't use 'insert' and 'value' together"
;;


What is the argument b here? What is the return type? These things are inferred by the compiler.
b is an element because ZI.get_attribute_opt takes an element as its last argument. The
return type is env_source, because InsertPath and Value are constructors for env_source objects.

You can include the types, but generally you don’t and that makes it hard to know the type of a
function just by looking at its definition in the source code.

Update: ygrek points out that most text editors can tell you the type of an OCaml identifier, e.g. see these
instructions for Vim.

It also tends to bulk out the code: e.g. Map.find key map rather than the more
object-oriented map.find key. OCaml does support objects if you want them, but
the normal style seems to be to avoid them. On the other hand, it does make it very easy to
see which bit of code is being called, which isn’t so obvious in an object-oriented style.

One thing to watch out for in OCaml is that, unlike Python and Haskell, it doesn’t look at your indentation. Consider the following
code (this is a simplified version of a mistake I actually made):

1
2
3
4
5
6
7
8
9
let () =
  print_endline "Start";
  match 1 with
  | 1 -> print_endline "one"
  | _ -> print_endline "many";
  print_endline "End"
;;


This code never prints “End”, because it treats that as part of the “many”
branch, even with all warnings enabled. It would be nice if it would issue a
warning if a single block (e.g. the last match case) contains lines with
different levels of indentation.

Haskell

Like Python, Haskell uses whitespace for indentation and generally avoids semi-colons and braces. Here’s the
main function in Haskell:

1
2
3
4
5
6
7
8
9
10
11
12
13
14
main :: IO ()
main = do progPath <- getFullProgName
	  absProgPath <- absolute_path progPath
	  conf <- Config.getDefaultConfig (dropFileName absProgPath)
	  argv <- getArgs
	  case argv of
	       [] -> error "Syntax: runsels [APP | SELECTIONS] [ARGS]"
	       (appOrSels:args) -> do
			app <- lookupApp appOrSels conf
			let selsPath = case app of
				Nothing -> appOrSels
				Just path -> path </> "selections.xml"
			sels <- loadSelections selsPath
			executeSelections sels args conf


Some things you should know to help you read the examples:

  :: means “has type”. Functions typically declare their type at the top.
  myfunc :: String -> Int -> Bool means that myFunc takes a String argument and then an Int argument
and returns a Bool.
  do notation means that the expressions inside are connected together in some type-specific way (this is quite
confusing). Since main has the type IO (), these statements are connected by the “IO monad”, which does
them one at a time, interacting with the system for each one.
If the do block had a different type, it might do something completely different.
  a $ b $ c d means a (b (c d)) (without them it means ((a b) c ) d). It helps to avoid having lots of close brackets.


The “run” module

Once we’ve loaded the selections XML document, the basic steps to execute the program are:

  For each selection, get the path to its directory (for packages not provided by the distribution).
  Scan the XML and collect all the bindings (things we need to set up).
  Ensure the launcher helper utility is installed (see last post).
  Set up environment variables requested in the bindings.
  Set up any launchers requested in the bindings.
  Build the argv for the new process and exec it.


OCaml

This is all very straight-forward:

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
let execute_selections sels args config =
  let env = Env.copy_current_env () in
  let impls = make_selection_map config.Config.stores sels in
  let bindings = Binding.collect_bindings impls sels in
  ensure_runenv config;
  (* Do <environment> bindings *)
  List.iter (Binding.do_env_binding env impls) bindings;
  (* Do <executable-in-*> bindings *)
  List.iter (do_exec_binding config env impls) bindings;
  let command = ZI.get_attribute "command" sels in
  let prog_args = (Command.build_command impls (ZI.get_attribute "interface" sels) command env) @ args in
  Unix.execve (List.hd prog_args) (Array.of_list prog_args) (Env.to_array env);;


Haskell

This is rather complicated. Haskell functions can’t have side effects (like, say, creating a launcher or execing a
process). Instead, the function returns an IO request to main, which returns it to whatever is driving Haskell.
The IO request is basically a request to perform an operation and a callback to invoke when done. To avoid this becoming
a syntax nightmare, Haskell’s do notation lets you write it in an imperative style:

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
executeSelections :: Selections -> [Arg] -> Config -> IO ()
executeSelections sels userArgs config = do
		origEnv <- getEnvironment
		paths <- mapM resolvePath (toList $ selections sels)
		let pathMap = fromList $ catMaybes paths
		let env = foldl (doEnv pathMap) (fromList origEnv) bindings
		envWithExec <- doExecBindings config sels pathMap env bindings
		let argv = (buildCommand sels envWithExec pathMap (interface sels) commandName) ++ userArgs
		executeFile (head argv) False (tail argv) (Just $ toList envWithExec)
	where bindings = collectBindings sels
	      doEnv pathMap env (iface, binding) = doEnvBinding (Data.Map.lookup iface pathMap) env binding
	      Just commandName = (command sels)
	      resolvePath (iface, sel) = do mPath <- getPath config sel
	      				    case mPath of
						    Nothing -> return Nothing
						    Just path -> return $ Just (iface, path)


For example, take the first line: origEnv <- getEnvironment. getEnvironment is a request to get the current environment.
It has the type IO [(String, String)] - a request for a list of (name, value) mappings. The rest of the do block is
effectively a function which takes an argument origEnv, of type [(String, String)] (i.e. an actual list of mappings).
When this is returned by main, the system gets the mappings and then calls this function with the results. The same thing then
happens with the second line, and so on.

There’s one tricky bit: resolvePath takes a single selection and finds where it’s stored (which requires access to the filesystem).
It returns an IO request to check whether some paths exist. But when we loop over all the selections, we get a list of IO operations, not an
IO operation. So you need to use mapM (monadic map), which turns a list of IO requests into an IO request for a list. That’s a lot
of thinking to do something quite simple.

Doing IO in Haskell is hard. Here’s another example (reading an environment variable):

1
2
3
4
5
6
getEnvOpt :: VarName -> IO (Maybe String)
getEnvOpt var = do
	maybe_value <- tryIOError (getEnv var)
	case maybe_value of
		Left e -> if isDoesNotExistError e then return Nothing else ioError e
		Right ok -> return $ Just ok


I want to do a getEnv operation and return Nothing if the variable isn’t set. At first, I tried calling getEnv and
catching the exception using the normal exception handing function. That doesn’t work. The reason is that getEnv doesn’t
throw an exception; it successfully returns an IO request for an environment variable. You have to use the special tryIOError
function to get a request for either an environment variable or an error, and then pattern-match on that.

The benefit of all this extra work is that you can instantly see which functions do (or rather request) IO by looking at their
type signature. Take the XML parsing function, for example:

1
parseXMLDoc :: String -> Maybe Element


Just by looking at the type, we can see that it does no IO (and, therefore, isn’t vulnerable to attacks which might try to trick
it into loading local files while parsing the DTD). It also, incidentally, suggests that we’re not going the get any useful error
message if it doesn’t parse: it will just return Nothing.

Data structures

We need to store the user’s configured search paths,
following the XDG Base Directory Specification.
We need a list of directories to search for configuration settings, cached data (e.g. downloads) and “data” (which
we use mainly for binaries we’ve compiled).

Python

For storing general records, Python provides a choice of classes and tuples. Here’s a typical class and an example of
creating an instance and accessing a field:

1
2
3
4
5
6
7
8
9
10
class Basedirs:
	def __init__(self, data, cache, config):
		self.data = data
		self.cache = cache
		self.config = config
b = Basedirs(data = [".../share"],
	     cache = [".../cache"],
	     config = [".../config"])
print(b.data)


When calling any Python function or constructor, you can use the (optional) name = value syntax so you don’t have to
remember the order of the arguments.

Python also provides named tuples, which saves some boilerplate code when you just want pure data with no
methods attached. The syntax isn’t great, though, and you can’t do pattern matching on the names, only by
remembering the order:

1
2
3
4
5
6
7
8
9
10
from collections import namedtuple
Basedirs = namedtuple("Basedirs", ["data", "cache", "config"])
b = Basedirs(data = [".../share"],
	     cache = [".../cache"],
	     config = [".../config"])
print(b.data)
# Pattern matching
(one, two, three) = b
print(one)


OCaml

OCaml provides classes, unnamed tuples and records (essentially named tuples). A record would be the obvious choice here:

1
2
3
4
5
6
7
8
9
10
11
12
13
type basedirs = {
  data: filepath list;
  cache: filepath list;
  config: filepath list;
};;
let b = {
  data = [".../share"];
  cache = [".../cache"];
  config = [".../config"];
};;
print_endline (String.concat "," b.data);;


Curiously, there’s no need to tell it the type of the records you’re building. It works it out from the field
names. However, this does mean that you can’t define two different record types with the same field name in the
same module. Also, if you want to access a field from a record defined in a
different module, you need to qualify the field name, e.g.

1
print_endline (String.concat "," b.Basedirs.data);;


There are also “polymorphic variants” which allow the same field name to be used in different structures, but
I haven’t tried using them. The manual
notes that the compiler can do a better job of finding errors with plain variants, however.

A big win over Python is the ability to pattern-match on field names. e.g. to extract the cache and config
fields:

1
2
3
4
let {cache; config} = b in
print_endline (String.concat "," cache);
print_endline (String.concat "," config)
;;


Update: if you compile with warnings on (and you should), it will complain that you’re ignoring the data field. This is a really useful check because if you add a new field later it will tell you all the places that might need updating. You can use data = _ to ignore that field explicitly, or just _ to ignore all remaining fields.

Haskell

Haskell doesn’t have classes (in the Python / OCaml sense), but it does provide named tuples:

1
2
3
4
5
6
7
8
9
10
11
data Basedirs = Basedirs { share :: [FilePath]
			 , cache :: [FilePath]
			 , config :: [FilePath]
			 }
b = Basedirs { share = [".../share"]
	     , cache = [".../cache"]
	     , config = [".../config"]
             }
main = print $ show $ share b


Note that the syntax for accessing a field is field record, not record.field as in other languages.
Like OCaml, you can’t use the same field name in different structures. Pattern matching works:

1
2
3
4
main = do let Basedirs { cache = cacheDirs
                       , config = configDirs } = b
          print (show cacheDirs)
          print (show configDirs)


However, it doesn’t have OCaml’s short-cut for the common case where the field you
want to match has the same name as the variable you want to store it in (to use Python terms). In fact,
doing that is strongly discouraged, because if you matched with config = config, then that would
shadow the config function used to access the record.

Variants

Sometimes, a value can be one of several sub-types. Bindings in 0install are a good example of this:

      There are two types of binding: an EnvironmentBinding sets an
environment variable to tell a program where some resource is, while an
ExecutableBinding gives the program an executable launcher for another
program.
  
      An EnvironmentBinding can be used to find a path within the selected component,
or to provide a constant value.
  
      An EnvironmentBinding can affect its variable in three different ways:
it can append the new value to the end of the old value, prepend
it at the start, or replace the old value completely.
  
      An ExecutableBinding can
store the launcher’s location in a variable or
add the launcher’s directory to the application’s $PATH.
  


Here’s an example of an environment binding which prepends a package’s lib directory to CLASSPATH:

1
<environment name='CLASSPATH' insert='lib' mode='prepend'/>


The code that parses the XML and generates a list of bindings needs to store different values depending on
which kind it is. For example, “append” and “prepend” bindings let you specify optional separator and default
values, while “replace” ones don’t.

Then, the code that applies the bindings needs to handle each of the separate cases. We’d like to make sure
that we didn’t forget to handle any case, and that we don’t try to access a field that’s only defined for a
different case.

Python

The traditional object-oriented way to handle this is with subclasses (e.g.
ExtendEnvironment with default and separator fields, and
ReplaceEnvironment without; InsertPath and Value, etc). However, that’s a lot of classes to
write, so 0install actually does everything in just two classes:

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
class EnvironmentBinding(Binding):
	PREPEND = 'prepend'
	APPEND = 'append'
	REPLACE = 'replace'
	def __init__(self, name, insert, default = None, mode = PREPEND, value = None, separator = None):
		self.name = name
		self.insert = insert
		self.default = default
		self.mode = mode
		self.value = value
		if separator is None:
			self.separator = os.pathsep
		else:
			self.separator = separator
	...
class ExecutableBinding(Binding):
	IN_PATH = 'path'
	IN_VAR = 'var'
	def __init__(self, exec_type, name, command):
		self.exec_type = exec_type
		self.name = name
		self.command = command
	...


Note the use of strings (PREPEND, APPEND, REPLACE) in place of a proper enum, as Python
doesn’t have them.

OCaml

Here’s a definition in OCaml. Variants use | to separate the various possibilities:

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
type filepath = string;;
type varname = string;;
type which_end = Prepend | Append;;
type add_mode = {
	pos :which_end;
	default :string option;
	separator :string
};;
type mode = Add of add_mode | Replace;;
type env_source = InsertPath of filepath | Value of string;;
type env_binding = {
	var_name: varname;
	mode: mode;
	source: env_source
};;
type exec_type = InPath | InVar;;
type exec_binding = {
	exec_type: exec_type;
	name: string;
	command: string
};;
type binding =
  | EnvironmentBinding of env_binding
  | ExecutableBinding of exec_binding;;


That’s far more useful than the Python. It accurately describes the possible combinations, and is clear
about the types and which bits are optional. Using varname and filepath as aliases for string doesn’t
add any type safety, but it does make the signatures easier to read and gives better error messages.

Note that the extra | on the first line after type binding isn’t strictly necessary, but it helps to
line things up.

Haskell

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
type VarName = String
data WhichEnd = Prepend | Append deriving Show
data AddMode = AddMode { pos :: WhichEnd
		       , defaultValue :: Maybe String
		       , separator :: String
		       } deriving Show
data Mode = Add AddMode | Replace deriving Show
data EnvSource = InsertPath FilePath | Value String deriving Show
data ExecBindingType = InPath | InVar deriving Show
data Binding = EnvironmentBinding VarName Mode EnvSource
	     | ExecutableBinding ExecBindingType String String
	     deriving Show


This is essentially the same as the OCaml, except that I used tuples rather than records in Binding,
because handling records is more awkward in Haskell due to the pattern matching problems noted above.
Using tuples (which I could have done in OCaml too) makes the definitions shorter, because the
definition of a tuple can be done in-line instead of with a separate structure.

deriving Show causes Haskell to automatically generate code to convert these types to strings,
which is handy for debugging (and sadly missing from OCaml).

Using the data structures

To apply the bindings, a runner module needs to collect all the bindings and then:

  Process each EnvironmentBinding, updating the environment.
  Process each ExecutableBinding, using the new environment to create the launchers.


Here, we’ll look at the code to process an EnvironmentBinding.

Python

Here’s the Python code for applying an <environment> binding:

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
class EnvironmentBinding(Binding):
	PREPEND = 'prepend'
	APPEND = 'append'
	REPLACE = 'replace'
	...
	def get_value(self, path, old_value):
		"""Calculate the new value of the environment variable after applying this binding.
		@param path: the path to the selected implementation; None for native packages
		@param old_value: the current value of the environment variable
		@return: the new value for the environment variable"""
		if self.insert is not None:
			if path is None:
				return None
			extra = os.path.join(path, self.insert)
		else:
			assert self.value is not None
			extra = self.value
		if self.mode == EnvironmentBinding.REPLACE:
			return extra
		if old_value is None:
			old_value = self.default
			if old_value is None:
				old_value = defaults.get(self.name, None)
				if old_value is None:
					return extra
		if self.mode == EnvironmentBinding.PREPEND:
			return extra + self.separator + old_value
		else:
			return old_value + self.separator + extra


The code for getting the value to append to is a bit messy. We’re trying to say:

  Use the current value of the environment variable. Or, if not set:
  Use the default from the <environment> element. Or, if not set:
  Use the built-in default value for this variable.


Python often makes this easy with its a or b or c syntax, but had to use the longer if syntax for
this case because or treats the both None and the empty string as false,
whereas we want to treat an empty string as a valid value.

1
2
3
4
if old_value is None:
	old_value = self.default
	if old_value is None:
		old_value = defaults.get(self.name, None)


Actually, I noticed while writing this post that I got that wrong in the Python (I used the shorter or in one
place) and had to fix it; a common mistake in Python.

The two binding types must be handled differently. In the run code, we (rather messily) check the type to
decide what to do:

1
2
3
4
if isinstance(binding, EnvironmentBinding):
	...
elif isinstance(binding, ExecutableBinding):
	...


This isn’t a very object-oriented way to do it. But it made more sense to put the logic for handling
the bindings all together in the run module rather than in the module which defines the data types
(which I prefer to keep side-effect free). Also, the rule that we need to
process all the EnvironmentBindings before all of the ExecutableBindings can’t
easily go in the classes themselves.

So, the existing Python code is really pretty poor. We’re using strings to simulate enums (simple variants),
a single class with a load of if statements in place of variants for the different ways of setting an
environment variable, and messy isinstance checks to let us keep the logic for applying bindings together
in the run module. If we add or change binding classes, there’s several places we need to check, and no
static checking to help us. Let’s see if the other languages can help us do better…

OCaml

Here’s the code to apply an EnvironmentBinding in OCaml:

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
let calc_new_value name mode value env =
  match mode with
  | Replace -> value
  | Add {pos; default; separator} ->
    let add_to old = match pos with
      | Prepend -> value ^ separator ^ old
      | Append -> old ^ separator ^ value in
    match Env.find_opt name env with
      | Some v -> add_to v                  (* add to current value of variable *)
      | None -> match default with
        | Some d -> add_to d                (* or to the specified default *)
        | None -> match get_default name with
          | Some d -> add_to d              (* or to the standard default *)
          | None -> value                   (* no old value; use new value directly *)
;;
let do_env_binding env impls = function
| (iface, EnvironmentBinding {var_name; mode; source}) -> (
    let add value = Env.putenv var_name (calc_new_value var_name mode value env) env in
    match source with
    | Value v -> add v
    | InsertPath i -> match StringMap.find iface impls with
      | (_, None) -> ()  (* a PackageSelection; skip binding *)
      | (_, Some p) -> add (p +/ i)
)
| _ -> ()
;;


It’s not bad, and it’s nice to see all the different cases laid out.

By the way let do_env_binding env impls = function ... is a convenient way to pattern match on the
last (unnamed) argument. It means the same as let do_env_binding env impls
binding = match binding with ....

I initially thought that the Python version was easier to read. On reflection, however, I think it’s more subtle. It’s easier
to see what the Python version does, but it’s not easy to see that what it does is correct.
By contrast, it’s easy to see that the OCaml version handles every case (the compiler checks this), and you can just check that each individual case is handled correctly.

As in the Python, getting old_value is a bit messy as there’s no null coalescing operator in OCaml:

1
2
3
4
5
6
7
match Env.find_opt name env with
  | Some v -> add_to v
  | None -> match default with
    | Some d -> add_to d
    | None -> match get_default name with
      | Some d -> add_to d
      | None -> value


Haskell

And here is the code to apply an EnvironmentBinding in Haskell:

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
join :: WhichEnd -> String -> Maybe String -> String -> String
join _ _ Nothing new = new
join Prepend sep (Just old) new = new ++ sep ++ old
join Append sep (Just old) new = old ++ sep ++ new
doEnvBinding :: Maybe FilePath -> Env -> Binding -> Env
doEnvBinding mPath env (EnvironmentBinding name mode bindingSource) =
    case (mPath, bindingSource) of
	    (_, Value v) -> use v
	    (Nothing, InsertPath i) -> env
	    (Just implPath, InsertPath i) -> use $ implPath </> i
    where use newValue = insert name (add newValue) env
          add newValue = case mode of
	    Replace -> newValue
	    Add AddMode {pos = whichEnd, defaultValue = def, separator = sep} ->
		join whichEnd sep oldValue newValue
		where oldValue = (Data.Map.lookup name env) `mplus`
				 def `mplus` (standardDefault name)
doEnvBinding _ env _ = env


That’s a good bit shorter than both the Python and the OCaml, because I was able to use
a `mplus` b handle the defaults easily. That’s “monadic plus”, in case you
were wondering whether mplus is a bit of a silly name.

Handling XML

We need to parse the XML into some kind of internal representation. Originally, 0install parsed the XML
into custom classes, but it turns out that we often want to write XML back out again, preserving attributes
and elements we didn’t understand. So we’ve been slowly moving towards using generic XML trees as the in-memory
representation, which saves having to write code to serialise our data structures as XML (which then requires
ensuring that they’re consistent with the parsing code).

Python

Python’s standard library includes a selection of XML parsers: minidom, pulldom, ElementTree, expat and SAX.
Disappointingly, the documentation says that none of them is safe
with untrusted data.
0install uses the low-level “expat” parser, which isn’t in the vulnerabilities table, so hopefully we’re OK
(many of the vulnerabilities are denial of service attacks, which isn’t a big problem for us).

We build our own “qdom” tree structure rather than use the more standard
minidom module because import xml.dom.minidom takes too long to be used in this
speed-critical code (or at least, it did when I wrote the qdom code, I haven’t tested it recently).
One of the nice things about writing in the other languages is not having to worry about speed all
the time.

OCaml

OCaml doesn’t include an XML parser in the standard libraries, so I Googled for one. The first one I
found was Xml-Light, but it turned out that it didn’t support XML namespaces. Then I tried the PXP parser,
which is enormous, and parsed the test document I gave it incorrectly (but they have fixed that now - thanks!). Finally, I
tried Xmlm, which is small and works.

Xmlm doesn’t generate a tree structure, just events (like SAX), so you have to build your own structure.
That could be a problem in some cases (it’s convenient to have a standard data structure in case you
want to pass documents between modules). On the other hand, we already use our own document structure
(“qdom”) in Python and the standard “DOM” interface is awkward anyway.

Xmlm suggests the following structure:

1
type tree = E of Xmlm.tag * tree list | D of string


However, this is quite annoying to process, because every time someone gives you a tree you have
to pattern match and handle the case of them giving you a D (a text node). Instead, I used
ElementTree’s trick of attaching text to element nodes:

1
2
3
4
5
6
7
type element = {
  tag: Xmlm.name;
  mutable attrs: Xmlm.attribute list;
  mutable child_nodes: element list;
  mutable text_before: string;        (** The text node immediately before us *)
  mutable last_text_inside: string;   (** The last text node inside us with no following element *)
};;


With this, it’s easy to iterate over all the elements and ignore text, and you don’t have to
worry about getting two text nodes next to each other.

You generally don’t need text_before unless you’re using mixed content, but having it here means we don’t
lose any data if we read a document and then write it out again.

It was convenient to have the structure be mutable while building it, and in other code we may want to
manipulate nodes, so I marked most of the fields as mutable.

I made a serious mistake in my first attempt at pattern matching on elements. I wanted to match the elements
<arg> or <for-each> in the 0install namespace:

1
2
3
4
5
6
let xmlns_feed = "http://zero-install.sourceforge.net/2004/injector/interface";;
let process child = match child.Qdom.tag with
| (xmlns_feed, "arg") -> process_arg
| (xmlns_feed, "for-each") -> process_for_each
| _ -> ()


This is the worst kind of mistake: the kind that seems to work fine when you test it. I made the same mistake in Haskell, but
luckily I decided to enable warnings (ghc -Wall) and spotted the problem. The code above doesn’t check that the namespace is
equal to xmlns_feed. Instead, it creates a new xmlns_feed binding with whatever the namespace actually was. Obvious in
hindsight (note: turning on warnings in OCaml also catches the mistake, because it complains that xmlns_feed is unused).

So, how can we fix this? Dealing with namespaces all the time when processing XML is annoying even when you get it right, so
I decided to make a helper interface for doing queries in a particular namespace. Of course, I’d like this to generalise to
other namespaces. I found an OCaml feature called “functors”, which seem to be basically module templates. Here’s how I made
a namespace-specific query interface:

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
module type NsType = sig
  val ns : string;;
end;;
module NsQuery (Ns : NsType) = struct
  (** Return the local name part of this element's tag.
      Returns [None] if it's in a different namespace. *)
  let tag elem =
    let (elem_ns, name) = elem.tag in
    if elem_ns = Ns.ns then Some name
    else None
  (** Apply [fn] to each child node in our namespace
      with local name [tag] *)
  let map fn node tag =
    let rec loop = function
      | [] -> []
      | (node::xs) ->
          if node.tag = (Ns.ns, tag)
          then (fn node) :: loop xs
          else loop xs in
    loop node.child_nodes
  ;;
  (** Get the value of the non-namespaced attribute [attr].
      Throws an exception if [elem] isn't in our namespace. *)
  let get_attribute attr elem =
  ...
end


Then I create a specialised version of this module for our namespace in constants.ml:

1
2
3
4
5
module ZI_NS = struct
  let ns = "http://zero-install.sourceforge.net/2004/injector/interface";;
end;;
module ZI = Qdom.NsQuery (ZI_NS);;


With this, ZI.map applies a function to all elements in the 0install namespace and with the given name. ZI.tag returns the
local name for 0install elements or Nothing for others, etc. Now the original code becomes:

1
2
3
4
let process child = match ZI.tag child with
| Some "arg" -> process_arg
| Some "for-each" -> process_for_each
| _ -> ()


Much better!

Haskell

Again, there’s a choice of library. I went for Text.XML.Light which seems to work well.

Dealing with namespaces is fairly painful; I’m not convinced that I’m using it right. Here’s
how I find the <runner> element inside a <command>, for example:

1
2
3
getRunnerElement :: Element -> Maybe Element
getRunnerElement commandElem = filterChildName isRunner commandElem
	where isRunner qname = (qURI qname) == Just xmlns_feed && (qName qname) == "runner"


Using pattern matching doesn’t seem to improve things:

1
2
3
4
getRunnerElement :: Element -> Maybe Element
getRunnerElement commandElem = filterChildName isRunner commandElem
	where isRunner (QName { qName = "runner", qURI = Just uri }) = uri == xmlns_feed
	      isRunner _ = False


I didn’t find any way to create a namespace-specific interface as in OCaml (Haskell “functors” do something
different).

Testing it on malformed XML, it sometimes just returns Nothing, which is rather unhelpful, and sometimes
it successfully parses it anyway! For reference, here is a test document that Haskell loads happily:

1
2
<?xml version="1.0" ?>
<root a='h/>


Python and OCaml, by contrast, both detect the problem and report the location of the error.

Building lists

We need to walk the XML tree and return all the bindings, in document order. The places where a binding may be found are:

  <selection> / [binding]
  <selection> / [dependency] / [binding]
  <selection> / <command> / [binding]
  <selection> / <command> / [dependency] / [binding]


For bindings in a dependency, we need to know the dependency’s interface. For other bindings, we want the selection’s own interface.

Python

Python’s easy syntax for sets provides one way to do it, and its append method on lists makes it easy to collect the results:

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
binding_elements = {'environment', 'executable-in-var', 'executable-in-path'}
dep_elements = {'requires', 'runner'}
def collect_bindings(impls, sels):
	bindings = []
	def process(elem, iface, valid_children):
		for child in elem.childNodes:
			if child.uri != ZI.ns: continue
			if child.name not in valid_children: continue
			if child.name == 'command':
				process(child, iface, binding_elements | dep_elements)
			elif child.name in dep_elements:
				dep_iface = child.attrs["interface"]
				if dep_iface in impls:
					process(child, dep_iface, binding_elements)
			elif child.name == 'environment':
				bindings.append((iface, EnvironmentBinding( ... ))
			elif child.name in ('executable-in-var', 'executable-in-path'):
				bindings.append((iface, ExecutableBinding( ... ))
			else:
				raise Exception(child.name)
	for sel in ZI.children(sels, "selection"):
		process(sel, sel.attrs["interface"], {'command'} | binding_elements | dep_elements)
	return bindings


I liked the ZI namespace query thing I made in OCaml so much, I made a Python class to do the same thing.

OCaml

I originally wrote this in a purely functional style, threading the results list through all the functions using fold_left.
That was pretty messy. Then I decided to take advantage of OCaml’s support for imperative programming and just mutate a single
list (bindings), as in the Python, which worked much better.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
let collect_bindings impls root =
  let bindings = ref [] in
  let rec process ~deps ~commands iface parent =
    let process_child node =
      match ZI.tag node with
      | Some "requires" | Some "runner" when deps ->
          let dep_iface = ZI.get_attribute "interface" node in
          if StringMap.mem dep_iface impls then process ~deps:false ~commands:false dep_iface node
          else ()
      | Some "command" when commands -> process ~deps:true ~commands:false iface node
      | _ -> match parse_binding node with
             | None -> ()
             | Some binding -> bindings := (iface, binding) :: !bindings in
    ZI.iter process_child parent in
  let process_sel node = process ~deps:true ~commands:true (ZI.get_attribute "interface" node) node in
  ZI.iter_with_name process_sel root "selection";
  List.rev !bindings
;;


Note the use of named arguments (process ~deps:true) to avoid confusion between the two boolean arguments.
This works like process(deps = true) in Python.

A strange aspect of OCaml is that you generally write loops backwards, first declaring the code to
handle each item and then calling an iter function to say what you want to loop over. OCaml does have
a for loop, but it’s one of those old-fashioned BBC BASIC style ones that just updates a counter in
that particular way that programmers never actually want to do.

Note also that in OCaml you add to the start of a list, not the end, so we need to reverse it as the
last step.

Update: You can use the pipe operator (|>) to make loops easier to write. It lets you write the input to a function first:

1
2
3
["one"; "two"; "three"] |> List.iter  (fun item ->
  print_endline item
)


Haskell

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
collectBindings :: Selections -> [(InterfaceURI, Binding)]
collectBindings sels = do sel <- filterChildrenName (hasZName "selection") (root sels)
                          let iface = requireAttr "interface" sel
                          getBindings True True iface sel
  where getBindings deps commands iface parent =
            do (name, child) <- ziChildren parent
               case name of
                    "command" | commands -> getBindings True False iface child
                    x | (x == "requires" || x == "runner") && deps ->
                             let depIface = requireAttr "interface" child in
                             if depIface `member` (selections sels)
                               then getBindings False False depIface child
                               else []  -- Optional dependency which was not selected
                    _ -> do binding <- processBinding name child
                            [(iface, binding)]


Nice and short again, but again it requires some explanation. The do expressions here (unlike the previous one)
have the list type, so they do the lines of the block in a way appropriate for lists. For lists, x <- items means
to run the rest of the do block once for each item in the list items, producing a list for each one, and then join
the resulting lists together end-to-end. So these do blocks are actually nested loops.

There are no named arguments, so we just have to remember what the True and False mean here.

Also, the | in the matches doesn’t separate alternatives (as in OCaml), but instead gives a condition which
must also be true for it to match, like when in OCaml.

String processing

We need to expand environment variables in arguments. In the XML, it looks like this:

1
<arg>-X${item}</arg>


Python

1
2
from string import Template
value = Template("-X${item}").substitute({"item": "debug"})


I guess that’s cheating, since I picked Python’s preferred syntax when I designed the XML format. Here’s
how to do it without using Template:

1
2
3
4
5
6
7
8
9
10
import re
re_expand = re.compile(r"\$(\$|([a-zA-Z_][a-zA-Z0-9_]*)|{[^}]*})")
def expandArg(template, env):
	def expand_one(matched):
		match = matched.group(1)
		if match == '$': return '$'
		if match.startswith('{'): return env[match[1:-1]]
		return env[match]
	return re_expand.sub(expand_one, template)


Easy. Notice the handy r"..." syntax for raw strings, which avoids the need to escape backslashes everywhere.

Oh, and I seem to have written an OCaml-style backwards loop here. Guess they’re not unique to OCaml after all.

OCaml

The regex is looking a bit ugly, but still pretty good. Oddly, instead of getting some kind of MatchResult object
passed to expand, we just get the original string and then pass that to the global match function. I guess that’s
just how the underlying library works.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
let re_template = Str.regexp ("\\$\\(\\$\\|\\([a-zA-Z_][a-zA-Z0-9_]*\\)\\|{[^}]*}\\)")
let expand_arg template env =
  let remove_braces s =
    let l = String.length s in
    if s.[0] = '{' then (
      assert (s.[l - 1] = '}');
      String.sub s 1 (l - 2)
    ) else s; in
  let expand s = match (Str.matched_group 1 s) with
  | "$" -> "$"
  | "" | "{}" -> failwith ("Error: empty variable name in template: " ^ template)
  | m -> Env.find (remove_braces m) env in
  Str.global_substitute re_template expand template
;;


Haskell

This was hard, but stackoverflow to the rescue!

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
expandArg :: String -> Env -> String
expandArg template env = replaceAll re replace template
	where re = makeRegex "\\$(\\$|([a-zA-Z_][a-zA-Z0-9_]*)|\\{[^\\}]*})"
	      replace match = case match of
	      			"$$" -> "$"
				'$' : '{' : ms -> replaceVar $ init ms
				'$' : var -> replaceVar var
				_ -> error "regex failed"
	      replaceVar var = case (Data.Map.lookup var env) of
	      				Nothing -> error (printf "Variable '%s' not set!" var)
					Just value -> value
-- Based on http://stackoverflow.com/a/9072362/50926
-- Use the given function to replace each occurance of 're' in 's'
replaceAll :: Regex -> (String -> String) -> String -> String
replaceAll re f s = prefix end
  where (_, end, prefix) = foldl' go (0, s, id) $ getAllMatches $ match re s
        go (ind,toRead,write) (off,len) =
          let (skip, start) = splitAt (off - ind) toRead
              (matched, remaining) = splitAt len start
          in (off + len, remaining, write . (skip++) . (f matched ++))


Finding resources

When shipping stand-alone binaries or 0install packages, it’s useful if the program can find other resources
in its own code directory. In our case, we need to find the launcher program so we can symlink to it.

Python

In Python, this is really easy. Every module has an attribute __file__ which has the location of that module:

1
my_dir = os.path.dirname(__file__)


OCaml

When called as a binary, we can get the path from argv[0] and use that. If we’re being used as a library then
we’re either installed as a system package (and can look in some hard-coded path) or being run as a 0install
dependency (and can get 0install to set an environment variable for us). Easy enough, but abspath (and
realpath) are missing from the standard library.

1
2
3
4
5
6
let abspath path =
  if path.[0] = '/' then path
  else (Sys.getcwd ()) +/ path
;;
my_dir = Filename.dirname (abspath Sys.argv.(0))


Haskell

Wow. I have no idea how this works.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
{-# LANGUAGE ForeignFunctionInterface #-}
module Main where
import Foreign
import Foreign.C
import System.Directory (getCurrentDirectory)
main :: IO ()
main = do progPath <- getFullProgName
	  absProgPath <- absolute_path progPath
	  let myDir = dropFileName absProgPath
	  ...
-- From http://hackage.haskell.org/trac/ghc/ticket/3199
getFullProgName :: IO String
getFullProgName =
  alloca $ \ p_argc ->
  alloca $ \ p_argv -> do
   getFullProgArgv p_argc p_argv
   peek p_argv >>= peek >>= peekCString
foreign import ccall unsafe "getFullProgArgv"
    getFullProgArgv :: Ptr CInt -> Ptr (Ptr CString) -> IO ()
-- From http://hackage.haskell.org/packages/archive/MissingH/1.2.0.0/doc/html/System-Path-NameManip.html
absolute_path :: String -> IO String
absolute_path path@('/':_) = return path
absolute_path path = do
   cwd <- getCurrentDirectory
   return (cwd ++ "/" ++ path)


API docs

All three languages make it easy to generate cross-referenced API docs. I didn’t bother to fill in many
strings for the OCaml and Haskell, but the linked sample pages show what it looks like:

  Python API
  OCaml API sample
  Haskell API sample


Python and Haskell require you to document the types in the code if you want them to appear in the docs.
The Python syntax for this is a bit awkward.
OCaml infers the types automatically.

Speed

My test case is running 0release (which has the largest selections.xml of all
my apps), except that I edited the selections so it runs /bin/echo at the end
rather than /usr/bin/python2. Otherwise we’re just measuring the speed of Python.
Running echo directly with a trivial (hard-coded) C program takes 2-3 ms, so that’s the
theoretical best time.

            Language
      Time / ms
      Lines of code
      Non-blank chars
    
  
            (baseline)
      2
      -
      -
    
          OCaml
      7
      678
      17,201
    
          Haskell
      14
      543
      16,818
    
          Python (test)
      45 / 76
      576
      14,685
    
          Python (real)
      64 / 109
      21,427
      609,082
    
  


Enabling stack traces in OCaml (OCAMLRUNPARAM=b) increased the time from 7ms to 8ms.

“Python (test)” is a version I wrote for this comparison to get a feel for the number of
lines. It doesn’t do as much checking as the real version and lacks some
features. “Python (real)” is the regular “0launch” command. The first number is
the time with Python 2, the second with Python 3. It’s interesting how much faster the
test version is!

By the way, if you’re thinking “parsing XML just to set up environment variables is inefficient; I’ll just
use a shell script to start my program”, a /bin/sh script which just execs “echo” takes 10 ms.

Conclusions

All three languages are easy to read, avoiding verbose boilerplate code, and ending up fairly similar in length.
No doubt an expert in OCaml or Haskell would write much shorter code than I did.
OCaml’s syntax is simple, but the indentation doesn’t necessarily match up with OCaml’s interpretation of the code,
which is a concern. In Python and Haskell, the compiler always sees the same structure as the programmer.

Haskell provides all kinds of special notations for writing shorter code.
This means that reading other people’s Haskell code can be very difficult.
Also, the same structure (e.g. do) can mean
wildly different things in different contexts. OCaml syntax is easy to learn and easy to understand.

Many things that are simple in Python or OCaml become very complex in Haskell. In this example: reading an environment
variable that may be unset, doing any kind of IO, reporting errors in XML documents, search-and-replace in strings and
reading argv. And although not part of this test, I think the converting 0install’s solver to Haskell would be very difficult.

Haskell’s ability to control where IO happens is useful, but other languages (e.g. E and (hopefully) OCaml with Emily) achieve
the same thing without all the complexity.

Python’s data structures are very limited and error prone. OCaml and Haskell record and variant types significantly improve
the code. OCaml makes working with record types easier, while Haskell simplifies debugging by making it easy to convert
structures to strings.

XML handling was easiest in OCaml, though I did have to make my own tree structure.
Python’s XML libraries aren’t safe and Haskell’s doesn’t report (or even detect, in some cases) errors.

Walking the XML tree and building a list was easy in all three languages. In Python and OCaml, because they let us mutate a single
list while walking the tree, and in Haskell thanks to its List monad which handles everything behind the scenes.

A major benefit of OCaml and Haskell is the ease of refactoring. In Python, once the code is working it’s best not to change
things, in case you break something that isn’t covered by the unit-tests. In OCaml and Haskell, you can rename a function, delete
old code or change a data structure and rely on the compiler to check that everything’s still OK. The static typing in OCaml and
Haskell also gives me more confidence that that various untested error reporting code paths will actually work. Untested Python
code is buggy code, generally.

I’m not sure how stable the OCaml library APIs are (I assume they don’t change much), but it’s clear that Haskell’s APIs change
frequently: code samples and documentation I found on the ‘net were frequently out of date and wouldn’t compile without changes.
Python is generally pretty good, if you overlook the massive changes in Python 3.

In the previous (trivial) test, OCaml and Haskell had very similar performance, but here OCaml clearly moves into the lead. Python is
far behind, and only getting slower with the move to Python 3. Being able to write code without worrying about speed all the time
is very liberating!

The two statically typed languages didn’t require much debugging, except that OCaml’s find function throws an exception if the
key isn’t found rather than using an option type (as in Haskell), which can lead to non-obvious errors. Turning on stack-traces
in OCaml makes it easy to track these down however, and making my own wrappers for find and similar should mean it won’t be a problem
in general.

The big surprise for me in these tests was how little you lose going from Python to OCaml. You still have classes, objects, functions,
mutable data and low-level access to the OS, all with an easy and concise syntax, but you gain type checking, much better data
structures and a huge amount of speed for no additional effort. Why aren’t more people using it?

Although Haskell and OCaml look more similar to each other than to Python, this is just syntax. In fact, OCaml and Python are conceptually
pretty similar, while Haskell is different in almost every way.

Overall, binary compatibility (i.e. difficulty of making cross-platform releases of the library, which is currently easy with Python)
is my main remaining concern. But while further testing is required (particularly for networking, threading and GUI support),
so far I’d be happy to move to OCaml.
Hide
        
      
                    by Thomas Leonard at Jun 20, 2013 
      
      
    
  


       
                  Phew, Real World OCaml beta now available.
      (Anil Madhavapeddy)
    
    
                                When I finished writing my PhD, I swore (as most recent graduates do) to never
write a thesis again.  Instead, life would be a series of pleasantly short
papers, interspersed with the occasional journal article, and lots of
not-writing-huge-book-activity in general.


Then CUFP 2011 happened, and I find myself
in a bar in Tokyo with Yaron Minsky and Marius Eriksen, and a dangerous bet ensued.
A few short weeks after that, and Yaron and I are chatting with Jason Hickey in California about
writing a book about the language we love.  I’m still telling myself that this will never actually happen,
but then everyone’s favourite Californirishman Bryan O’Sullivan puts us in touch with O’Reilly,
who published his excellent Haskell tome.

O’Reilly arranged everything incredibly fast, with our editor Andy Oram driving us through the process.  We
decided early on that we wanted to write a book that had opinions based our
personal experience: about how OCaml code should be written, about …Read more...When I finished writing my PhD, I swore (as most recent graduates do) to never
write a thesis again.  Instead, life would be a series of pleasantly short
papers, interspersed with the occasional journal article, and lots of
not-writing-huge-book-activity in general.


Then CUFP 2011 happened, and I find myself
in a bar in Tokyo with Yaron Minsky and Marius Eriksen, and a dangerous bet ensued.
A few short weeks after that, and Yaron and I are chatting with Jason Hickey in California about
writing a book about the language we love.  I’m still telling myself that this will never actually happen,
but then everyone’s favourite Californirishman Bryan O’Sullivan puts us in touch with O’Reilly,
who published his excellent Haskell tome.

O’Reilly arranged everything incredibly fast, with our editor Andy Oram driving us through the process.  We
decided early on that we wanted to write a book that had opinions based our
personal experience: about how OCaml code should be written, about the standard
library involved, and generally making functional programming more accessible.
Along the way, we’ve been working incredibly hard on the underlying software
platform too, with Jane Street, OCamlPro and my own
group OCaml Labs working together on all the pieces.  There’s
still a lot of work left to do, of course, but we’re right on track to get
all this released very soon now.

So, without further ado, I was very pleased to send this e-mail this morning.
(and once again reaffirm my committment to never writing another book ever
again.  Until next time!)

  Yaron Minsky, Jason Hickey and I are pleased to announce the beta release of
our forthcoming O’Reilly book, called “Real World OCaml”, available online at
http://realworldocaml.org

  The book is split into three parts: language concepts, tools and techniques,
and understanding the runtime.  As promised last year, we are making a public
beta available for community review and to help us hunt down inaccuracies and
find areas that need more clarification.

  We’ve had the book in closed alpha for six months or so and have developed a
feedback system that uses Github to record your comments. This lets us follow
up to each review with clarifications and keep track of our progress in fixing
issues.  During alpha, we’ve received over 1400 comments in this fashion (and
addressed the vast majority of them!).  However, since we anticipate more
comments coming in from a public beta, we would request that you read the FAQ
to avoid drowning us in repeat comments: http://www.realworldocaml.org/#faq.

  (TL;DR followup another comment on Github directly if you can instead of
creating a new issue via the web interface)

  This release is available in HTML format online at:
   http://www.realworldocaml.org

  O’Reilly is currently preparing a Rough Cuts release that will make the beta
available as PDF and in popular eBook formats.   We anticipate that this will
be available later this week, and I’ll send a followup when that happens.

  Finally, we would especially like to thank our alpha reviewers.  Their feedback has been invaluable to the
beta release. The book also includes substantial contributions to individual
chapters from Jeremy Yallop (FFI), Stephen Weeks (GC) and Leo White (objects).

  If you have any comments that you’d like to send directly by e-mail, please
contact us at rwo-authors@recoil.org.

  Release notes for beta1:

      The first-class modules chapter is incomplete, pending some portability
improvements to the ocaml-plugins Core library.
    The binary serialization chapter is also incomplete, but has just enough
to teach you about the Async RPC library.
    The installation chapter will be revised in anticipation of the OCaml 4.1
release, and is currently quite source-based.
    The packaging and build systems chapter hasn’t been started yet.  We’re
still deciding whether or not to make this an online pointer rather than
a print chapter, since it’s likely to change quite fast.
    We are preparing exercises per chapter that are not included in this
particular beta release, but will be available online as soon as possible.
    The code examples will all be clonable as a separate repository in beta2.
  

  best,
Yaron, Jason and Anil

Hide
        
      
                    by Anil Madhavapeddy at Jun 17, 2013 
      
      
    
  


       
                  Replacing Python: candidates
      (Thomas Leonard)
    
    
                                This post evaluates the programming languages ATS, C#, Go, Haskell, OCaml, Python and Rust to try to decide which would be the best language in which to write 0install (which is currently implemented in Python). Hopefully it will also be interesting to anyone curious about these languages.

I’m not an expert in these languages (except Python). My test-case is to read the tutorial for each language and reimplement one trivial function of 0install in the language. These languages were suggested by various people on the 0install mailing list. If I’ve got anything wrong, please add a comment.



Table of Contents

  Why replace Python?
  Brief summary of the candidates
  Test case
  Speed and size
  Binary compatibility
  Safety
  Diagnostics
  Ease of writing
  Shared libraries
  Static types          Prevention of null pointer errors
      Dependent types
      Managing resources (linear types)
    
  
  Bounds on privilege
  Mutability
  C interoperability
  Asynchronous code
  Su…Read more...This post evaluates the programming languages ATS, C#, Go, Haskell, OCaml, Python and Rust to try to decide which would be the best language in which to write 0install (which is currently implemented in Python). Hopefully it will also be interesting to anyone curious about these languages.

I’m not an expert in these languages (except Python). My test-case is to read the tutorial for each language and reimplement one trivial function of 0install in the language. These languages were suggested by various people on the 0install mailing list. If I’ve got anything wrong, please add a comment.



Table of Contents

  Why replace Python?
  Brief summary of the candidates
  Test case
  Speed and size
  Binary compatibility
  Safety
  Diagnostics
  Ease of writing
  Shared libraries
  Static types          Prevention of null pointer errors
      Dependent types
      Managing resources (linear types)
    
  
  Bounds on privilege
  Mutability
  C interoperability
  Asynchronous code
  Summary
  Round 2


Why replace Python?

Several people have asked for a version of 0install in a compiled language:

  Tim Cuthbertson wants to build a statically-linked binary for bootstrapping on systems which don’t already have 0install, to increase performance and to simplify installation on OS X. He has started prototyping a Haskell version.
  Canonical’s Colin Watson is worried about Python’s performance on mobile phones.
  Marco Jez and Dave Abrahams proposed a C++ version.
  Bastian Eicher would like a .NET version (though IronPython might work here).
  The Sugar developers would like maximum performance on their low-powered XO laptops.


Personally, I’d like to use a language with static type checking to make changes to the code less risky, and to detect problems due to API changes in the platform (e.g. Python 3 broke the code in subtle ways which we’re still discovering occasionally in less-used code-paths).

However, Python has worked well for us over the years and has many benefits:

  Widely known and easy to learn.
  Clear, readable syntax.
  A large standard library.
  Easy to debug.
  Generators make asynchronous code easy.
  You only need to ship source code (interpreted).
  Can run inside a Java or .NET VM (using Jython/IronPython).
  Can support multiple versions of library APIs (if hasattr(...) etc).
  All current 0install contributors know it.
  The current code is all Python and is well-tested.


Brief summary of the candidates

  ATS (version 0.2.8)
  A functional language with a very advanced type system, which includes
dependent types (e.g. “a String of length n”) and linear types (e.g. an
obligation to close a file descriptor after use). An interesting feature
of ATS is that its run-time types are identical to C’s (e.g. an ATS string
is a null-terminated array of chars), allowing interoperability with C without
wrapping.


1
implement main () = print "Hello world\n"


  C# (mcs 3.0.7.0)
  Microsoft’s Java alternative. Much of the Windows version of 0install is written
in C# (it currently uses IPC to talk to the Python solver process). It compiles
to .NET bytecode, which can run on any platform. On non-Windows platforms,
Mono can run .NET code.


1
2
3
4
5
  public class Hello {
     public static void Main() {
	System.Console.WriteLine("Hello, World!");
     }
  }


  Go (1.1)
  Google’s C replacement, with a focus on being light-weight and easy to use.


1
2
3
4
5
6
7
package main
import "fmt"
func main() {
    fmt.Println("Hello, world")
}


  Haskell (7.6.3)
  A lazy, purely functional language (no function can have side-effects; main essentially returns
a request to print hello world).


1
main = putStrLn "Hello, World!"


  OCaml (4.00.1)
  Another functional language. It can be interpreted, compiled to platform-independent bytecode, or
compiled to native code.


1
print_endline "Hello world"


  Python (2.7.5 and 3.3.2)
  A popular, easy to learn interpreted language.


1
print("Hello world")


  Rust (0.6)
  Mozilla’s experimental new language, which aims to be a safe replacement for C. It
supports manual memory management (as well as optional garbage collection),
using linear types to ensure that everything is used safely and freed
correctly.


1
2
3
fn main() {
    println("hello?");
}


Test case
To get a feel for each language, we implemented a trivial piece of 0install in each one. Here’s the current (complete) Python version:

runenv.py 
1
2
3
4
5
#!/usr/bin/env python
import os, sys, json
envname = os.path.basename(sys.argv[0])
args = json.loads(os.environ["0install-runenv-" + envname])
os.execv(args[0], args + sys.argv[1:])


In case you’re wondering what this is for: this executable is used when a program A depends on another, B. A has this launcher script in its $PATH under the appropriate name. When A runs the launcher, the launcher runs the correct version of B with the appropriate interpreter and arguments. For example, A might run foo --process and the launcher might invoke /usr/bin/python2.7 /var/cache/0install.net/.../foo.py --process.

To avoid creating one launcher for every possible set of versions, the exact strings to use are placed in an environment variable in program A’s environment when it is launched, and foo is just a symlink to the launcher. So, this program (the launcher):

  Finds out the name of the symlink used to invoke it.
  Gets the details of how to invoke program B from the environment (this is a JSON string list).
  Invokes the target with those arguments, plus any extra arguments passed to the launcher.


Note: I didn’t make any particular effort to write the test code carefully. If it seemed to work, I went with it. In real code I would obviously try
to be more careful, but since real code would be bigger, I’d also make more mistakes. I want to see how well each language prevents me from making
mistakes.

Speed and size
0install doesn’t require much CPU time, but it does need to start quickly (and this particular bit especially so). The table below shows how many
times per second each version of the launcher was able to run a trivial C program. “Overhead” is the amount
of time each run took over running the binary directly without a launcher. “Size” is the size of the binary
being executed, excluding any shared libraries or runtime components.

            Time (ms)
      Overhead (ms)
      Speed (runs/s)
      Size (KB)
      Language
    
  
            1.74
      +  0.63
      574
      72.83
      ATS
    
          3.02
      +  1.90
      332
      1210.88
      OCaml (native)
    
          3.38
      +  2.26
      296
      1323.30
      Haskell
    
          8.99
      +  7.87
      111
      1907.68
      Go
    
          9.33
      +  8.22
      107
      326.79
      OCaml (bytecode)
    
          10.20
      +  9.09
      98
      130.58
      Rust
    
          51.88
      + 50.76
      19
      0.18
      Python3
    
          82.98
      + 81.86
      12
      0.18
      Python2
    
          83.37
      + 82.26
      12
      3.50
      C# (Mono)
    
  




  ATS (5)
  ATS is the clear winner here. It’s significantly faster than its closest
rival, and at a fraction of the size. Note that the only smaller executables
(Python and C#) depend on huge external runtimes. The ATS binary depends only
on libjson.so, a 39 KB C library that, on my system at least, was already
installed.
  Haskell and OCaml (4)
  These two garbage-collected functional languages both put in impressive
performances. 
  Rust (3)
  I was surprised and disappointed by how badly Rust did here. With its low-level
focus and manual memory management, I was expecting it to get close to ATS. Maybe
things will improve when it’s more mature.
  Go (3)
  A very disappointing performance from Go too.
  Python (2)
  Python executables are small, but very slow. Note that Python 2 is usually faster than Python 3, but after a recent
package update, Python 2 has suddenly become slower on my system. Python 2 was previously getting 27 runs per second.
  C# (1)
  Fiddling around with ahead-of-time compilation and static linking didn’t seem to
help here.


Except for C#, I didn’t make any particular attempt to make the binaries smaller or faster, but just
went with the compilers’ defaults.

Some of these languages depend on shared runtimes or libraries, which may or may not already be installed.
For each one, I looked at how much extra software I had to install to make the binary run on a Debian 7
clean install (just “SSH server” and “Standard system utilities” selected during install). This is also a
good test for binary compatibility problems (see next section).

“Dependencies” is the “additional disk space used” reported by apt-get install ... --no-install-recommends.

            Total (KB)
      Binary size (KB)
      Dependencies (KB)
      Language
    
  
            0.18
      0.18
      0
      Python
    
          156.83
      72.83
      84
      ATS
    
          1210.88
      1210.88
      0
      OCaml (native)
    
          1323.30
      1323.30
      0
      Haskell
    
          1907.68
      1907.68
      0
      Go
    
          13242.58
      130.58
      13112
      Rust
    
          13335.79
      326.79
      13009
      OCaml (bytecode)
    
          31133.50
      3.50
      31130
      C# (Mono)
    
  


However, this is slightly unfair because we’d need to use many other features for a full version of 0install.
Languages with large standard libraries (e.g. Python and .NET) won’t need much extra stuff.

  Python (5)
  Like most Linux systems (and OS X), Python is installed by default.
  ATS (5)
  ATS would be the smallest by far, if Python wasn’t pre-installed.

    apt-get install libjson0 --no-install-recommends
  
  OCaml (4)
  Having both native and bytecode options is convenient: we can use bytecode for programs that aren’t speed
critical, and native code for embedded situations. The native version has no dependencies. The bytecode
version requires:

    apt-get install ocaml-base libyojson-ocaml --no-install-recommends
  
  Haskell (3)
  Only slightly larger than OCaml, but no bytecode option.
  Go (3)
  Go doesn’t support dynamic linking, so there are no dependencies.
  Rust (2)
  Rust has a surprisingly large runtime, considering that its standard library is quite limited.
  C# (1)
  Debian’s libnewtonsoft-json4.5-cil was incompatible with the one I’d used, so I used my copy of Newtonsoft.Json.dll.

    apt-get install binfmt-support mono-runtime libmono-system-windows-forms4.0-cil --no-install-recommends
  


Binary compatibility

Several of these programs, compiled on my Arch Linux system, failed to run on
Debian because they’d picked up a dependency on GLIBC 2.14’s memcpy (glibc uses symbol versioning,
so that when it changes in an incompatible way, your current binaries continue working for a bit,
before breaking mysteriously next time you recompile).

For the affected programs, I recompiled them on Debian. This isn’t a huge problem, because we
can just compile binaries on the oldest system we want to support and they will
still work on newer systems.

  Python (5)
  Worked fine. /usr/bin/python was Python 2 rather than Python 3 (as on
Arch), which can be a hassle, but 0install is written to run on either.
  C# (5)
  .NET is a nice portable bytecode.
However, the binary wouldn’t run when executed directly until I installed binfmt-support. Debian’s
libnewtonsoft-json4.5-cil was incompatible with the one I’d used, so I bundled my copy of Newtonsoft.Json.dll.
  ATS (4)
  Had the GLIBC 2.14 problem. However, I didn’t need to recompile on Debian, as ATS allowed me to
specify the desired symbol version with a few lines of embedded C.
Then the binary compiled on Arch also worked on Debian.
  OCaml (4)
  The OCaml native binary failed to work due to the GLIBC 2.14 dependency and had to be recompiled.
The OCaml bytecode version failed with Fatal error: unknown C primitive 'caml_array_blit' and
had to be recompiled.
The resulting recompiled binaries worked on my modern Arch Linux system.
  Go (3)
  Worked, but only because it doesn’t support dynamic linking. That’s not very useful if we need to upgrade
a library. It’s hard to give a score here. On the one hand, it did work perfectly (and so should get a 5).
On the other hand, any language can get binary compatibility by statically linking everything; that’s not
really what we’re interested in.
  Haskell (2)
  Also failed with the GLIBC 2.14 problem. I rebuilt the binary on Debian 7, but the new binary then
didn’t work on my newer Arch system: libffi.so.5: cannot open shared object file: No such file or directory.
  Rust (2)
  Failed with libcore-c3ca5d77d81b46c1-0.6.so: cannot open shared object file. Rust is not available on
Debian 7. I compiled Rust from source to get the libcore library (got the compiler hit an unexpected failure path. this is a bug, but I seemed to have a working compiler at the end anyway, in the stage1 directory).
I then hit same the GLIBC problem with my test binary. I used the new rust
compiler to rebuild the binary on Debian.


To clarify what we want to do here: Currently, to make a new release of
0install I publish a single tarball containing the Python code. Tools which
depend on this library (e.g. 0compile) start using the new version
automatically (they don’t need to be rebuilt). Also, if a library 0install
depends on (e.g. GTK or glibc) gets updated, I don’t need to make a new release
there either.

Safety
For me, a “safe” language is one which stops and reports a problem when something unexpected occurs at runtime. An unsafe one carries on, using
incorrect data. Unsafe behaviour often causes security problems and data loss. For example, many programs, including 0install, update files atomically by
writing out the new data to a new file and then renaming it over the original on success. If the function says it successfully saved the data when
it didn’t (e.g. because the disk was full) data loss will occur.

As a basic test of each language’s approach to safety, I took the “Hello World” example program from the language’s own tutorial, compiled
it, and ran it like this:

$ ./hello 1< /dev/null; echo Exit status: $?


This runs it with a read-only stdout, so the program will fail to output its message. A safe language will print an error to stderr and return a non-zero exit status to indicate failure. An unsafe language will print nothing and return 0 (success). If you’re not sure why this is important, imagine the command is dump-database > backup and the filesystem is full.

My theory is, if the language designers can’t write hello world safely, what hope do the rest of us have?

  Rust (5)
  Amazingly, Rust was the only language to pass this test!
  ATS, C#, Go, Haskell, OCaml, Python (1)
      Rubbish.

    Update: OCaml would have passed if they’d used print_endline, but the tutorial used print_string, which doesn’t abort.
  


Note: Isn’t it a bit silly to generalise from this one data-point to the
behaviour of the whole language? Yes. But as a starting off point for
discussion, it’s working quite well. e.g. the OCaml response was surprise that
it didn’t work, whereas Go users regard this behaviour as normal and expected.

Next, what does each of the sample programs do if the environment variable isn’t set? The program should abort with an error message when it tries to read the environment variable.

  Python, OCaml, Haskell (5)
  All abort correctly with an exception. No special code needed.
  Rust, ATS (5)
  The compiler forces me to handle the case of the variable not being set. Good.
  C# (3)
  getenv returns null and continues. The program aborts later as the JSON parser can’t parse null.
  Go (1)
  Getenv returns the empty string and continues. Then Go somehow manages to parse the empty string as an empty JSON list and still continues. Then it tries to interpret the first of the user arguments to the program as the path of the program to run and execs that instead! Utter failure.


Finally, does the language allow unsafe memory access (e.g. reading from memory after it has been freed)? A safe language will not allow this unless the programmer explicitly requests an “unsafe” mode.

  C#, Python, OCaml, Haskell (5)
  These languages generally don’t provide any unsafe memory access (or if they do, it’s an obscure feature you wouldn’t use in normal code).
  Go (5)
  Go has an “unsafe” package for unsafe operations.
  Rust (4)
  Rust is safe unless you use unsafe {} blocks. Unfortunately, I did have to use a couple to implement the sample code, because Rust’s standard library didn’t provide an execv call and I had to make my own wrapper.
  ATS (3)
  In theory, ATS’s type system prevents unsafe access. In reality, the standard
library (which defines the types of functions in various C libraries) contains
many unsafe type declarations. Often, you have something like a foo_get_name: Foo -> String function
which returns a pointer into a Foo structure. The simplest way to define this in
ATS doesn’t indicate that the result is only valid until Foo goes away. With a
little extra effort (using minus), you can prevent that, but it’s unreasonably
hard to say that the string will also become invalid if Foo is mutated. Rust, on the
other hand, makes these things very easy to express.

    Another problem with ATS is that types may not be correct if exceptions are used. For
example, the sequence m = alloc(); use(m); free(m) compiles (because it thinks that
m is always freed), but if use throws an exception then m will not be freed. This
can be avoided by never catching exceptions.
  


            Language
      Hello
      Missing env
      Memory safety
    
  
            Rust
      5
      5
      4
    
          Haskell
      1
      5
      5
    
          OCaml
      1
      5
      5
    
          Python
      1
      5
      5
    
          ATS
      1
      5
      3
    
          C#
      1
      3
      5
    
          Go
      1
      1
      5
    
  


Finally, I should note that, while Python generally has safe defaults, the HTTPS handling is an exception to this (it doesn’t validate the certificates), and the documentation for the XML modules in the standard library notes that “The XML modules are not secure against erroneous or maliciously constructed data.”. However, I only know about these because I’m very familiar with Python - the other languages may have similar issues - so I’m not going to count it here.

Diagnostics

When a run-time exception occurs, how easily can the user diagnose the problem or search for a solution on-line? If they write to us, how helpful will the output be to us?

My test-case here is, again, the missing environment variable (if this happened, it would indicate a bug in some other part of 0install):

  Python (5)
  Python displays a stack-trace showing exactly what it was doing and gives the name of the variable it was looking for. The error includes the path to the Python code, so any programmer can easily open it and do further debugging. If the user posts the error to the mailing list, we would immediately know what was wrong. And I didn’t have to write any code to make it do this. Perfect.

    Traceback (most recent call last):
  File "./runenv2.py", line 5, in <module>
    args = json.loads(os.environ["0install-runenv-" + envname])
  File "/usr/lib/python2.7/UserDict.py", line 23, in __getitem__
    raise KeyError(key)
KeyError: '0install-runenv-runenv2.py'

  
  Haskell (3)
  A reasonably clear error, but no clue about where in the code it’s coming from:

    Runenv: 0install-runenv-Runenv: getEnv: does not exist (no environment variable)

  
  Rust (3)
  The compiler forced me to add some code to handle the error. It was easy to include the environment variable name in the error, so I did. This results in a reasonably clear message, but the location is in a generic part of the standard library and not helpful:

    rust: task failed at 'Environment variable '0install-runenv-runenv' not set', /build/src/rust-0.6/src/libcore/option.rs:300


    Update: cmrx64 notes that setting RUST_LOG=::rt::backtrace gives a
stack-trace. Currently, this isn’t very useful because it doesn’t include
line numbers, but it looks like this is going to improve.
  
  ATS (2)
  Like Rust, ATS forced me to handle the error. The simplest solution here was to throw an exception, which led to the somewhat-unhelpful message:

    exit(ATS): uncaught exception: _2home_2tal_2Projects_2ats_2runenv_2edats__Missing_EnvironmentVar(1025)

  
  C# (1)
  Fails to detect the problem and gives: System.ArgumentNullException at a later point in the code. You do get a stack-trace, though:

    Unhandled Exception:
System.ArgumentNullException: Argument cannot be null.
Parameter name: value
  at Newtonsoft.Json.JsonConvert.DeserializeObject (System.String value, System.Type type, Newtonsoft.Json.JsonSerializerSettings settings) [0x00000] in <filename unknown>:0 
  at Newtonsoft.Json.JsonConvert.DeserializeObject[String[]] (System.String value, Newtonsoft.Json.JsonSerializerSettings settings) [0x00000] in <filename unknown>:0 
  at Newtonsoft.Json.JsonConvert.DeserializeObject[String[]] (System.String value) [0x00000] in <filename unknown>:0 
  at Runenv.Main (System.String[] userArgs) [0x00000] in <filename unknown>:0 
[ERROR] FATAL UNHANDLED EXCEPTION: System.ArgumentNullException: Argument cannot be null.
Parameter name: value
  at Newtonsoft.Json.JsonConvert.DeserializeObject (System.String value, System.Type type, Newtonsoft.Json.JsonSerializerSettings settings) [0x00000] in <filename unknown>:0 
  at Newtonsoft.Json.JsonConvert.DeserializeObject[String[]] (System.String value, Newtonsoft.Json.JsonSerializerSettings settings) [0x00000] in <filename unknown>:0 
  at Newtonsoft.Json.JsonConvert.DeserializeObject[String[]] (System.String value) [0x00000] in <filename unknown>:0 
  at Runenv.Main (System.String[] userArgs) [0x00000] in <filename unknown>:0  of range`).

  
  OCaml (1)
  Gives a useless generic error. We’d have no clue where the problem was from this:

    Fatal error: exception Not_found


    Update: ygrek says it is possible to get the location, but you need to compile with -g and run with OCAMLRUNPARAM=b.
  
  Go (1)
  Fails to detect the problem at all and gives a useless error from another point in the code

    panic: runtime error: index out of range
  
goroutine 1 [running]:
main.main()
        /home/tal/Projects/go/runenv.go:16 +0x239
  
goroutine 2 [runnable]

  


Ease of writing

This is even more subjective than the other areas but here goes:

  Python (5)
  The code is clear, short and easy to understand even if you don’t know Python (but do know POSIX):


runenv.py 
1
2
3
4
5
  import os, sys, json
  envname = os.path.basename(sys.argv[0])
  args = json.loads(os.environ["0install-runenv-" + envname])
  os.execv(args[0], args + sys.argv[1:])


  C# (4)
  A little verbose, but quite readable (update: Bastian Eicher suggests some improvements to this code):


runenv.cs 
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
  using System;
  using Mono.Unix.Native;
  using Newtonsoft.Json;
  using System.Collections;
  using System.Windows.Forms;
  using System.IO;
  public class Runenv {
    public static void Main(String[] userArgs) {
      String basename = Path.GetFileName(Application.ExecutablePath);
      String json = Stdlib.getenv("0install-runenv-" + basename);
      String[] progArgs = JsonConvert.DeserializeObject<String[]>(json);
      String[] argv = new String[progArgs.Length + userArgs.Length];
      progArgs.CopyTo(argv, 0);
      userArgs.CopyTo(argv, progArgs.Length);
      Syscall.execv(argv[0], argv);
    }
  }


  OCaml (4)
  This code was contributed by “ygrek” (I wrote my own version first, which was longer but
also fairly clear):


runenv.ml 
1
2
3
4
5
6
7
8
9
10
  let () =
    match Array.to_list Sys.argv with
    | [] -> assert false
    | arg0::args ->
      let var = "0install-runenv-" ^ Filename.basename arg0 in
      let s = Sys.getenv var in
      let open Yojson.Basic in
      let envargs = Util.convert_each Util.to_string (from_string s) in
      Unix.execv (List.hd envargs) (Array.of_list (envargs @ args))


  Haskell (4)
  Tim Cuthbertson contributed this Haskell version:


runenv.hs 
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
  module Main where
  import System.Environment (getArgs, getEnv, getProgName)
  import System.Posix.Process (executeFile)
  import Text.JSON
  main = do
  	envName <- getProgName
  	argv <- getArgs
  	jsonContents <- getEnv $ "0install-runenv-" ++ envName
  	let jsv = parseJSON jsonContents
  	let program:extraArgs = parseArr jsv
  	executeFile program False (extraArgs ++ argv) Nothing
  	where
  		parseJSON str = decode str :: Result [JSString]
  		parseArr :: Result [JSString] -> [String]
  		parseArr (Ok jss) = map fromJSString jss


  Go (3)
  The Go version was pretty easy to write, except that the error handling was so bad that when I got things
wrong, it was hard to understand which bit was actually failing (hence why it explicitly handles the error
from Exec; this was to help me debug it when it failed silently the first time, despite my plan to 
add error handling only if the compiler told me to):


runenv.go 
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
  package main;
  import "path"
  import "encoding/json"
  import "os"
  import "syscall"
  func main() {
  	json_data := []byte(os.Getenv("0install-runenv-" + path.Base(os.Args[0])))
  	var argv []string
  	json.Unmarshal(json_data, &argv)
  	argv = append(argv, os.Args[1:]...)
  	err := syscall.Exec(argv[0], argv, syscall.Environ())
  	if err != nil {
  		panic(err)
  	}
  }


  Rust (3)
  This is only part of the code; I also wrote some support code to turn a list
of JSON strings into a list of Rust strings, and an implementation of execv.
Apart from that it was pretty easy:


runenv.rs 
1
2
3
4
5
6
7
8
9
10
11
12
13
14
  fn main() {
    let our_args = os::args();
    let prog :path::PosixPath = path::Path(our_args[0]);
    let basename :&str = prog.filename().expect(fmt!("Not a file '%?'", prog));
    let var :~str = ~"0install-runenv-" + basename;
    let json_str = os::getenv(var).expect(fmt!("Environment variable '%s' not set", var));
    let j = json::from_str(json_str).unwrap();
    let mut prog_args: ~[~str] = json_list_to_str_vector(&j);
    prog_args += our_args.tail();
    execv(prog_args);
  }


  ATS (1)
  It took me several days to learn enough to be able to write this, and the code ended up several hundred lines long. However, much of this was
support code (e.g. code for interfacing to libjson and safer handing of execv) and it looks like this won’t
be needed in ATS 2 (which is not yet ready for use).

    To help understand it a little, you need to know that there are both dynamic (runtime) variables,
(e.g. target_argv, a C pointer to an array) and static (compile-time) “proofs” (e.g. must_free_argv). These are
often written as proof1, proof2, ... | dyn1, dyn2, .... prval lines are only used at compile-time
for checking; they generate no code.

    The biggest problem programming in ATS however is that the compiler is very unforgiving. Most errors
result in a generic syntax error. Putting two operators too close together often causes them to be
treated as a single third operator. Errors about constraint violations are printed using the compiler’s
hard-to-read internal representation, etc.

    Here’s a small sample (the main function):
  


runenv.dats 
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
  [...]
  // The main function
  implement main_argc_argv(argc, argv): void = let
    // Get JSON string from the environment
    val prog_args_json = get_json(view@ argv | &argv)
    // Skip argv[0]
    val n_user_args = argc - 1
    prval (pr_self, pr_user_args) = array_v_uncons(view@ argv)
    val user_args: ptr = (&argv) + sizeof<string>
    // Parse the JSON
    val json_root = checked_json_tokener_parse(string1_of_strptr prog_args_json)
    val (borrowed_json | json_array) = json_object_get_array(json_root)
    val n_prog_args = array_list_get_length(json_array)
    val () = assert(n_prog_args > 0)
    // Allocate the target_argv vector to be passed to execv
    // It will contain prog_args + user_args (plus a null, added by string_array_alloc)
    val argv_len = n_prog_args + n_user_args
    val (must_free_argv, str_array_v | target_argv) = string_array_alloc (size1_of_int1 argv_len)
    // Populate target_argv from json_array and user_args
    val () = initialise_array(str_array_v, pr_user_args | target_argv,
          size1_of_int1 n_prog_args, json_array,
          size1_of_int1 n_user_args, user_args)
    // Extract the program name from target_argv
    // This is just a long way of writing "target_prog = target_argv[0]"
    prval (argv_initialised, borrowed_string_array) = string_array_take_contents(str_array_v)
    prval (head, tail) = array_v_uncons(argv_initialised)
    val target_prog: string = ptr_get_t<string>(head | target_argv)
    prval argv_initialised = array_v_cons(head, tail)
    prval () = str_array_v := string_array_return_contents(argv_initialised, borrowed_string_array)
    // Exec the target program if possible
    val error_code = execv(str_array_v | target_prog, target_argv)
    val () = print("Failed to execv\n")
    // Cleanup (only reached on failure)
    prval () = minus_addback(borrowed_json, json_array | json_root)   // Don't need json_array any longer
    val () = json_object_put(json_root)                               // Free JSON object
    val () = string_array_free(must_free_argv, str_array_v | target_argv) // Free target_argv
    prval () = view@ argv := array_v_cons(pr_self, pr_user_args)      // Rejoin argv[0] with argv[1:]
  in
    exit(1)
  end


Shared libraries
The 0install package contains both executables and library classes for use by other tools (0compile, 0test, 0repo, etc). It must be
possible to make a bug-fix release of the main library without having to make a new release of every tool that depends on it.

  ATS, C#, Python (5)
  I didn’t bother to test these, as it’s obvious that they support shared libraries with no problems.
  Rust (5)
      Works, but the reference compiled into the main binary includes a hash (e.g.
`libmylib-68a2c114141ca-1.4.so`). The docs say that “The alphanumerics
are a hash representing the crate metadata.” I added an `author` field to the
metadata, and sure enough the new library could no longer be used by the existing
binary. It’s not very clear whether there are other things that could break the hash.

    Update: see this explanation from the Rust developers. The
hash is just to make the library name unique (in case someone else writes a library with the same name) and
should never change. i.e. we should specify the “author” as “0install.net”. In addition, symbols get a hash
which includes their type signature (so incompatible changes become link-time errors) and the library hash (so you
don’t get symbol name conflicts between libraries).
So, I’ve now given Rust a 5 here (was 2).
  
  Haskell (2)
  Shared libraries do work, but are tied to the version of ghc used to compile them. e.g. Compiling with -dynamic gets a
dependency on libHSjson-0.7-ghc7.6.3.so. A Haskell library author would have to provide an enormous number of versions of their library to cover every GHC version on every platform.
  OCaml (1)
      There’s also a Dynlink module for loading shared code manually at runtime.
According to a StackOverflow answer: “Very often when upgrading the Ocaml compiler (e.g. from 3.12 to future 3.13), you previous `*.cmo` or `*.cma` files won’t be able to work without recompilation.” I haven’t yet managed to make shared libraries work.

    Update: The situation is essentially the same as with Haskell; shared libraries work but if a dependency changes then everything that uses it must be recompiled.
Dynlink does work, but it’s really intended for plugins (where the main executable provides a fixed API for the dynamically loaded plugin).
  
  Go (1)
  Doesn’t support shared libraries at all.


Static types

All languages here except for Python provide at least basic static type checking. After Python, Go has the
most primitive system, without even user-defined polymorphic types. However, even that would save us from
a number of problems common with Python code (particularly bugs in rarely tested error paths or breakage
caused by library APIs changing without us noticing).

Prevention of null pointer errors

  ATS, Rust, Haskell, OCaml
  These languages distinguish in their type systems between objects and null. Therefore,
if an object has type Foo, then it really is a Foo, not null, and the program can’t crash
with a NullPointerException or equivalent.
  C#, Go, Python
  These languages cannot distinguish between values and null at compile time.


Dependent types

ATS is the only language here with dependent types. You can do a lot of cool things with these.
For example, the assert(n_prog_args > 0) check in the ATS code above isn’t there by
accident; the ATS compiler required me to prove that the list generated by parsing the JSON wouldn’t be empty,
since I needed to take the first argument as the program name.

Note that the Go version failed with an index
out of range error (see above); that cannot happen in the ATS version (it will still fail, since I used an
assert, but it fails with a sensible error message at the correct point in the code). Using an assert or an if puts a
runtime check into the program. In other cases, you may be able to insert a proof that the array won’t be
empty.

Managing resources (linear types)

ATS and Rust support linear types. This allows them to ensure that, for example, if a file is opened
then it will also be closed again promptly, and not used after that. ATS’s support is more flexible, but
Rust’s is much easier to use.

Haskell, OCaml, C#, Go and Python do not detect (at compile time) attempts to read from a closed file,
or forgetting to close a file.

Note that all languages allow you to define a function that opens the file,
calls the function with the new handle, and then closes the handle, which works
well in many cases. However, this doesn’t prevent the function from saving a
reference to the file handle and trying to reuse it later, and doesn’t allow
storing handles in other objects, etc. It also doesn’t work when the nesting isn’t
fixed (e.g. opening and reading from several TCP streams in parallel, closing each one
when done).



Bounds on privilege

When parsing XML it is useful to know, without examining the XML parser in detail, that it cannot load files
from the local filesystem (some XML parsers allow XML documents to do this via their DTDs, which we want to
prevent for security reasons). When unpacking an
archive we’d like to know, without auditing the unzip code, that it won’t write anywhere outside of the target
directory. And so on.

  Haskell (5)
      Haskell seems ideal for this, since its functions are side-effect free, though I’m not sure if it
prevents libraries from using unsafe functions if they want to.

    Update: Tim Cuthbertson writes: “There is a Safe Haskell feature in GHC, which does guarantee that unsafe features are not used (which can applied when compiling a given module / package). So that’s good - but I haven’t used it, so I can’t vouch for its practicality.”
  
  OCaml (5)
  OCaml has a tool called Emily which enforces object-capability
rules on OCaml code. I haven’t tested it yet, though.
  C# (5)
  Looks pretty good. Bastian Eicher says: “Adding the attribute [SecurityPermission(SecurityAction.Deny, Flags = SecurityPermissionFlag.UnmanagedCode)] to a method prevents it from calling into unmanaged code directly or indirectly. Only data structures within the application itself can be touched and no IO is allowed. Native methods which have been deemed to be safe (e.g. retrieve the current system time) have the [SuppressUnmanagedCodeSecurityAttribute] attribute to bypasses this restriction. Disclaimer: I have not used these features in my own code so far.”
  ATS (2)
  ATS functions can be annotated as pure, but this is greatly limits what can be done (for example, a pure function
can’t even throw an exception).
  Go, Python, Rust (1)
  I’m not aware of any particular security features in these languages.


Mutability

Immutable objects (objects which you can rely on not to change) make programs safer and easier to reason about,
but can also be less efficient.

  Rust (5)
  Rust’s linear types mean that the compiler knows whether you hold the only pointer to something. This
means that you can create an object, mutate it (e.g. while building it), then pass it as an immutable
object to another function. Once the function has finished with it, you can mutate it again.
Efficient and safe - perfect.
  C#, OCaml, Go (4)
  Struct/object fields can be declared as mutable or immutable.

    Update: Blax points in the comments that Go actually provides control of
whether a field is exported or not. A field is then made “immutable” by not
providing a setter for it, only a getter.
  
  ATS (4)
  ATS generally doesn’t distinguish between mutable and immutable pointers (e.g. in the standard library),
although the type system is flexible enough that you could do this for custom types. Values can be declared
as val (immutable) or var (variable).
  Python (3)
  Everything is always mutable, which can lead to bugs (e.g. mutating a list without realising that it’s shared).
  Haskell (2)
  Everything is always immutable, which has benefits but can be very annoying and inefficient when you need mutability. Since 0install is written in an imperative style, a translation into Haskell would likely be difficult.


C interoperability

Everything we might want to interact with will provide at least a C API. How easy is it to use these?

  ATS (5)
  ATS’s runtime data structures are identical to C’s. All you have to do is declare the C function with an ATS type signature (as the C definitions are too vague to be useful). ATS produces C code as output, and you can even embed C functions in your ATS source code
and it will pass them through to the C compiler directly. As noted above, ATS
does make it difficult (though not impossible) to express some common
constraints (e.g. that a returned pointer will remain valid until the input
structure is mutated).
  C# (4)
  Interop with Native Libraries shows how to call C functions from C#.
  Go (4)
  cgo makes it easy to call C from Go.
  Haskell (4)
  C libraries can be wrapped using Haskell’s FFI.
  Rust (4)
  C functions can be declared with Rust raw pointers, but can only be called from unsafe code. You therefore need to write
wrappers for them. An annoyance here is that Rust’s types are not the same as C’s, so e.g. every string has to be copied
whenever you invoke a C function (Rust strings don’t always have null terminators) and get a result back (Rust strings have
a length header). Also, you can’t use Rust’s linear pointer types with C functions, because Rust assumes that there is a
header block on such types. This makes interfacing with C a little less efficient than it could be (this is no worse than e.g. C# or Haskell; I just feel Rust could do better).
  Python (3)
  Due to its huge popularity, most libraries also provide Python bindings. There’s also the ctypes module in the standard
library, but generally people seem to write Python binding code in C when they need to interact with C libraries.
  OCaml(3)
  Interfacing OCaml with C requires writing C code.

    Update: ygrek notes that there is a new OCaml-ctypes project which
allows bindings without writing C.
  


Asynchronous code

0install needs to be able to download from multiple sites in parallel and without blocking the UI.

  C# (5)
  Provides the Task<T> type for pending results and the await keyword to wait for them. Untested.
  Haskell (5)
  The Async type is used to represent a potential future result. Untested.
  Go (5)
  Go’s “goroutines” make it very easy to spawn an asynchronous task and its excellent channels make
it easy for goroutines to communicate safely. However, Go does not prevent
unsafe communication between threads (e.g. via shared variables).
  Rust (5)
  Like Go, Rust provides easy support for spawning light-weight threads and channels for communication.
Rust’s type system prevents unsafe concurrent access (every mutable object is owned by a single thread).
  OCaml (5)
  The LWT package provides support. Untested, but looks good.
  Python (4)
  Python’s generators make it easy to implement co-routines for asynchronous operations. Many such libraries have been implemented, but it looks like Tulip will soon become the official solution in the standard library.
  ATS (1)
  No special features, just raw C threading.


Summary

            Language
      Rust
      OCaml
      Python
      Haskell
      ATS
      C#
      Go
    
  
            Speed
      3
      4
      2
      4
      5
      1
      3
    
          Dependencies
      2
      4
      5
      3
      5
      1
      3
    
          Bin. compatibility
      2
      4
      5
      2
      4
      5
      3
    
          Bad stdout
      5
      1
      1
      1
      1
      1
      1
    
          Missing env
      5
      5
      5
      5
      5
      3
      1
    
          Memory safety
      4
      5
      5
      5
      3
      5
      5
    
          Diagnostics
      3
      1
      5
      3
      2
      1
      1
    
          Ease of coding
      3
      4
      5
      4
      1
      4
      3
    
          Shared libraries
      5
      1
      5
      2
      5
      5
      1
    
          Static types
      5
      4
      1
      4
      5
      3
      2
    
          Privilege bounds
      1
      5
      1
      5
      2
      5
      1
    
          Mutability
      5
      4
      3
      2
      4
      4
      4
    
          C interoperability
      4
      3
      3
      4
      5
      4
      4
    
          Asynchronous calls
      5
      5
      4
      5
      1
      5
      5
    
          Total
      52
      50
      50
      49
      48
      47
      37
    
  


So what does this tell us? There’s no clear winner here (although there is a clear loser).
Although the various languages differ widely in the individual aspects, overall they tend to
balance out.

Update: OCaml and Python were originally joint first. However, now that Rust’s library hashes
have been explained, it has moved into the lead. Of course, just summing up the scores doesn’t
make much sense anyway; it’s just a convenient way to sort them.

What would happen if we wrote 0install in … ?

  Rust
  The language is still changing rapidly at the moment. I suspect we’d hit
quite a few problems trying to use it in production. It’s looking very
promising though.
  OCaml
  0install would become faster and possibly more reliable, but runtime errors would be harder to debug. We might have
problems publishing updates to shared libraries. A perfectly reasonable option, though.
  Python
  Everything would stay as it is. Which, actually, is not bad at all. Maybe we could investigate
other ways to improve speed and type safety without leaving Python? That’s likely to be less work
than a rewrite and much less risky. Cython? ShedSkin? PyPy? RPython?
  Haskell
  We’d probably have issues with binary compatibility and shared libraries, but the code might
become more reliable. Converting the existing code to a purely functional style would likely
be very difficult though, and there’s a risk that some things would turn out to have no obvious
equivalent.
  ATS
  Everything would be incredibly fast, but getting new contributors would be very difficult due
to the learning curve. There’s a risk of crashes as the library is not entirely memory safe,
and there are likely to be changes ahead to the language. Probably writing the whole thing in
ATS would be too much work for anyone.
  C#
  Performance would improve slightly on Windows, but things would get worse for
Linux, Unix and OS X users due to the extra dependencies. Probably we could get some of the
same improvements on Windows using IronPython.
  Go
  Go is worse than OCaml in just about every respect, so I can’t see any reason to choose it if we
wanted to do a rewrite.


Please post corrections and suggestions below (or on the mailing list) - thanks!

Updates for other languages:

  Vala
  Anders F Björklund’s Vala code
  Haxe (not working)
  Tim Cuthbertson tries Haxe


Round 2

Continue to round 2
Hide
        
      
                    by Thomas Leonard at Jun 09, 2013 
      
      
    
  


       
                  Jun 2013 news update
      (OCL Monthly News)
    
    
                                      
The rain continues to plummet down relentlessly as "summer" starts in OCaml
Labs. The most exciting news has been the public release of the Real World
OCaml, which hit the front page of the usual news
aggregators and generated huge interest!  This (reminiscent of the Xen 1.0
release) promptly took down servers for a couple of hours, but we managed to
minimise downtime in time for the Californians waking up.
O'Reilly has also started selling PDF copies of the book under their Rough
Cuts program.  This gives you
a copy of the final book when it's released too.  Commenting is still open
on the online version, so please do feel free to
participate there if you have time.
Systems Projects

Mirage: Anil and Dave did the last of the sweeping build changes to make
Mirage friendlier to use for beginners.  Previously, we required a custom OPAM
switch to build kernels, but now we use virtual
packages to separate the
choice of compiler and packages.  This of course breaks all our documenta…Read more...      
The rain continues to plummet down relentlessly as "summer" starts in OCaml
Labs. The most exciting news has been the public release of the Real World
OCaml, which hit the front page of the usual news
aggregators and generated huge interest!  This (reminiscent of the Xen 1.0
release) promptly took down servers for a couple of hours, but we managed to
minimise downtime in time for the Californians waking up.
O'Reilly has also started selling PDF copies of the book under their Rough
Cuts program.  This gives you
a copy of the final book when it's released too.  Commenting is still open
on the online version, so please do feel free to
participate there if you have time.
Systems Projects

Mirage: Anil and Dave did the last of the sweeping build changes to make
Mirage friendlier to use for beginners.  Previously, we required a custom OPAM
switch to build kernels, but now we use virtual
packages to separate the
choice of compiler and packages.  This of course breaks all our documentation,
but we're going to do a big sweep in July before
OSCON with the
new scheme.  Vincent has also been burning through the core platform libraries,
cleaning them up and adding documentation strings.  He is also building a
shared memory vchan driver that will
make parallel-Mirage unikernels very easy to coordinate on the same host.
The huge news from our friends at Citrix is the open-sourcing of
XenServer, which is the popular Citrix product that
embeds the OCaml XAPI cloud management
stack.  There are almost 100 major components
released as part of this, several of which can
be directly reused with Mirage.  Mirage was always an ambitious project, but
it's all coming together now thanks to bold moves such as this from Citrix!
Signpost: We woke up to the excellent but slightly scary news that
our USENIX FOCI paper was accepted.
This now means that we get to present it in August at USENIX Security, but
the team is now racing to pull together the prototypes into a complete system
before the conference.  Nothing like a deadline to focus the mind!  We're
also working on the camera-ready version of the paper, which we will share
here when it's ready.
Platform Projects

OCamlot: David Sheets did an astounding job at pulling together a
working continuous build system in a very short amount of time, and promptly
managed to melt some of the older non-x86
machines in Anil's office.  Once Anil
sadly replaced them, the builder churned through a matrix of different compiler
versions (4.0,4.1dev,4.2dev), architectures (x86, x86_64, ARM, PowerPC), and
operating systems (Debian, Ubuntu, FreeBSD, OpenBSD to start with).  There's
a development URL, but
the next step is to retire this and move it to a proper home at ocaml.org.
Having continuous build for OPAM is really, really useful though, as it lets us
vet pull requests on several architectures before merging them. It also let
Anil repair OCaml on OpenBSD/macppc
too, which is possibly the most obscure fix he's done in a while.
The next steps with OCamlot are to take a shot at porting the core to
Jenga, which is Jane Street's next-generation
distributed build system.  This should let us improve the fault-tolerance and
logging aspects of it before putting it properly into production later in the summer.
Ctypes: The May release
brought with it a good chunk of feedback, so Jeremy spent time incorporating
that and contributing to the Real World OCaml ctypes chapter.  He also added
support for garbage-collecting closures passed to C, and also very cool support
for printing C types and values.
Our friends at Citrix have started looking at ctypes, and Rob Hoes has already
used it to write bindings to the
Netlink Protocol Library Suite.

OPAM-doc: Vincent Botbol got the documentation generator stable enough
to pass the Core library through.  This is particularly challenging since Core
exercises pretty much every trick in the book when it comes to the use of the
module system.  However, Vincent successfully demonstrated the workflow of
OPAM-doc at the end-of-month meeting, and is aiming to have a public release
via OPAM in July (hopefully in time for the next beta release of Real World OCaml,
which uses Core heavily).
Visualisation libraries: Daniel Bünzli has
been spending a few months based in Citrix, working on a foundational new
declarative drawing library written in pure OCaml.  The Vg is already quite functional despite
still being in beta, and features a Javascript backend
that renders to both SVG and Canvas in HTML5.  That's not all though!  He's
also developing the Vz visualization library
that uses Vg to assemble more complex scenes and graphs.  Daniel's going to
join us in OCaml Labs for the remainder of the summer, so we're looking
forward to developing this more and using it on our various Platform projects
such as OCamlot.
Outreach

Real World OCaml: As mentioned earlier, the beta release of RWO went
splendidly, with a pleasing vibe that the book is what people expected.  There
were some interesting criticisms of the choice of Github authentication, but
we've had over 6000 registered commentators despite this (and of course, we
have plans brewing to tackle the identity problem).
No beta release is perfect, of course, and our now-public commenting system has
resulted in over 1500
issues being raised.  Well, that's all of Yaron, Anil and Jason's free time
gone for some time!

OCaml.org: We're in the process of looking at the site as a whole and
designing the workflow we'd like to have for growing and maintaining it.  Some
discussions have taken place about using Markdown in place of the current HTML
snippets, which would make it easier for external contributors to get involved.
In the meantime, Amir has converted the current site to Markdown format to see
how this process would work in practice.  You can see his experimenting and
scripts in the temporary repo in
the markdown-site/ folder.
Philippe also showed off MPP
at the internal meetings, and is stabilising it for a public release this
summer (once it has been integrated into the ocaml.org workflow).
This month also had a number of programming language gurus show up at the Lab
for the Algebraic Effects and Handlers workshop organised by Sam
Staton.  Most of the group attended this, as
we're all interested in how to encode effects for several of our projects (most
notably Irminsule).
We also enjoyed a visit by Benoît Vaugon, who gave a talk on his
OCamlCC
OCaml-to-C compiler, and also participated in a talk on
OCAPIC. He
also chatted with us about his alternative GADT-based implementation of
Printf, which promises to both
speed up and simplify the printer support in OCaml (and also relieve Mirage of
another dependency on libc).
Link roundup:
XenServer open-sourced! (Jun 25th)Real World OCaml public beta now available. (Jun 17th)

   Hide
        
      
                    by Anil Madhavapeddy at Jun 01, 2013 
      
      
    
  


       
                  The road to a developer preview at OSCON 2013
      (Mirage OS)
    
    
                                      There's been a crazy stream of activity since the start of the year, but the most important news is that we have a release target for an integrated developer preview of the Mirage stack: a talk at O'Reilly OSCon in July!  Do turn up there and find Dave Scott and Anil Madhavapeddy showing off interactive demonstrations.
Meanwhile, another significant announcement has been that Xen is joining the Linux Foundation as a collaborative project.  This is great news for Mirage: as a library operating system, we can operate just as easily under other hypervisors, and even on bare-metal devices such as the Raspberry Pi.  We're very much looking forward to getting the Xen-based developer release done, and interacting with the wider Linux community (and FreeBSD, for that matter, thanks to Gabor Pali's kFreeBSD backend).
Here's some other significant news from the past few months:
OPAM 1.0 was released, giving Mirage a solid package manager for handling the many libraries required to glue an…Read more...      There's been a crazy stream of activity since the start of the year, but the most important news is that we have a release target for an integrated developer preview of the Mirage stack: a talk at O'Reilly OSCon in July!  Do turn up there and find Dave Scott and Anil Madhavapeddy showing off interactive demonstrations.
Meanwhile, another significant announcement has been that Xen is joining the Linux Foundation as a collaborative project.  This is great news for Mirage: as a library operating system, we can operate just as easily under other hypervisors, and even on bare-metal devices such as the Raspberry Pi.  We're very much looking forward to getting the Xen-based developer release done, and interacting with the wider Linux community (and FreeBSD, for that matter, thanks to Gabor Pali's kFreeBSD backend).
Here's some other significant news from the past few months:
OPAM 1.0 was released, giving Mirage a solid package manager for handling the many libraries required to glue an application together.  Vincent Bernardoff joined the team at Citrix and has been building a Mirage build-frontend called Mirari to hide much of the system complexity from a user who isn't too familiar with either Xen or OCaml.
A new group called the OCaml Labs has started up in the Cambridge Computer Laboratory, and is working on improving the OCaml toolchain and platform.  This gives Mirage a big boost, as we can re-use several of the documentation, build and test improvements in our own releases.  You can read up on the group's activities via the monthly updates, or browse through the various projects.  One of the more important projects is the OCamlot continuous build infrastructure, which will also be testing Mirage kernels as one of the supported backends.
As we head into release mode, we've started weekly meetings to coordinate all the activities.  We're keeping notes as we go along, so you should be able to skim the notes and mailing list archives to get a feel for the overall activities.  Anil is maintaining a release checklist for the summer developer preview.
Anil (along with Yaron Minsky and Jason Hickey) is finishing up an O'Reilly book on Real World OCaml, which will be a useful guide to using OCaml for systems and network programming. If you'd like to review an early copy, please get in touch.  The final book is anticipated to be released towards the end of the year, with a Rough Cut at the end of the summer.
The core system was described in an ASPLOS 2013 paper, which should help you understand the background behind library operating systems. Some of the Mirage libraries are also currently being integrated into the next-generation Windsor release of the Xen Cloud Platform, which means that several of the libraries will be used in production and hence move beyond research-quality code.


In the next few months, the installation notes and getting started guides will
all be revamped to match the reality of the new tooling, so expect some flux
there.   If you want to take an early try of Mirage beforehand, don't forget to
hop on the #mirage IRC channel on Freenode and ping us with questions
directly.  We will also be migrating some of the project infrastructure to be fully
self-hosted on Mirage and Xen, and placing some of the services onto the new xenproject.org infrastructure.

   Hide
        
      
                    by Anil Madhavapeddy at May 20, 2013 
      
      
    
  


       
                  May 2013 news update
      (OCL Monthly News)
    
    
                                      
May is exam time in Cambridge, and the corridors of the OCaml Labs resounded
with the wailing of frantic students finishing their dissertations and
preparing for exams.  We welcomed Vincent Botbol to join us for a summer internship,
and he started hacking on the new opam-doc
right away.
Anil, Thomas, Leo and Amir also visited Jane
Street HQ in New York City, where we had a productive
couple of days reviewing our projects and getting feedback from them about
approaches to multicore and type system enhancements. Ashish
Agarwal also organised a fun evening with the New
York OCaml Users Group,
where Anil and Thomas presented our plans for the nascent OCaml Platform.
Systems Projects

Mirage: This was a month of consolidation and bugfixing in Mirage.  We've
been settling into weekly meetings to coordinate the hacking between us and
Citrix, and the minutes (1
2
3) may be useful if you
want to catch up.  The biggest bugbear is always the build system, and we've
been exploring the use …Read more...      
May is exam time in Cambridge, and the corridors of the OCaml Labs resounded
with the wailing of frantic students finishing their dissertations and
preparing for exams.  We welcomed Vincent Botbol to join us for a summer internship,
and he started hacking on the new opam-doc
right away.
Anil, Thomas, Leo and Amir also visited Jane
Street HQ in New York City, where we had a productive
couple of days reviewing our projects and getting feedback from them about
approaches to multicore and type system enhancements. Ashish
Agarwal also organised a fun evening with the New
York OCaml Users Group,
where Anil and Thomas presented our plans for the nascent OCaml Platform.
Systems Projects

Mirage: This was a month of consolidation and bugfixing in Mirage.  We've
been settling into weekly meetings to coordinate the hacking between us and
Citrix, and the minutes (1
2
3) may be useful if you
want to catch up.  The biggest bugbear is always the build system, and we've
been exploring the use of Jenga as the
eventual async-aware coordination and build system for running Mirage kernels.
Dave made great progress with a
message-switch that coordinates
multiple kernels, and Balraj fixed several performance regressions in the
TCP/IP stack by building unit tests that spawn millions of parallel TCP
connections.
Signpost: We took a break from building prototypes to submit a paper on the
basic design to the USENIX Free and Open Communications (FOCI
2013) workshop.  Haris and Heidi
blazed a path on writing this paper, and we've got even more ideas rolling
around about how to use DNSSEC to break the cloud deadlock.  The
ocaml-dns continues to grow features too.
Platform Projects


Ctypes: Jeremy
announced
the first release of a new foreign-function mechanism for OCaml that doesn't
require you to write any C stubs at all!  You can browse the source
code and
tutorial, and
install it via OPAM.  This is very much the first 0.1 release, and we have
exciting future developments to turn this into a full-fledged replacement for
the fast-but-rather-difficult-to-use-right OCaml FFI.
OPAM-doc: Vincent Botbol started building on Leo's work on the new
opam-doc tool. This is intended to
replace the venerable ocamldoc with one that uses all the latest features of
the compiler.  In particular, it can use the new typed AST cmt output to
avoid duplicating the compiler functionality, and can also build up a global
package table to generate complete cross-references across an entire OPAM 
collection.
OCamlot: David has been building up the libraries and tools needed for
the continuous build infrastructure.
This includes much-improved
ocaml-github, bindings, which are now
being used to power the Real World OCaml site as
well as well as OCamlot.  In addition, he's got an interesting collection of
regular expressions to automatically triage common failures from OPAM
(such as missing external dependencies), that should help reduce the manual
burden of getting thousands of tests results dumped on the small OPAM team.
Compiler Projects


OCaml-Java: Xavier Clerc has been hacking away at his next-generation
OCaml-Java backend (using many new features in JDK7). He's released a
preview of the bindings to
Java concurrency, and is looking for feedback on it.
Performance profiling: Mark Shinwell has been hacking on improving the
integration of the runtime with perf.  This should give us the hooks to
reliably track where memory was allocated.  His branch isn't going to land in
OCaml 4.1, but should be available as an OPAM switch for people to easily
try out when it's more stable.
Outreach

OCaml.org: Philippe and Amir have been putting their heads together with
Christophe and Ashish to turn the ocaml.org build pipeline
into something a little more structured.  Philippe is building a template
processor for this purpose.
The OCaml site is a more complicated than the average site due to our desire to
embed js_of_ocaml interactive toplevels
throughout the tutorials, and also to have active OPAM integration throughout
the site to make it easier for newcomers to sample the language.
The design of a handful of pages are also now available to preview, if you
don't mind some manual git cloning. The best way to do this is to clone the
temporary repo onto your local
machine and look in the new-design/_site directory.  There are examples of
the home page, 100 lines of OCaml page and several others.  Since we're just
getting started with applying these changes the site isn't clicky (yet).  For
some extra fun, try resizing your browser window and see how the pages reflow
to suit smaller (mobile) screens!

Real World OCaml: Anil, Yaron and Jason continue to work hard on getting
a release out of the door.  We shipped a final alpha6 this month that is 
chapter-complete, and have been preparing for a big public June release of
the book.  Thousands of comments have been received and closed already,
making this an unusually active (but incredibly useful) ongoing review
process.  Leo and Jeremy also contributed portions of text for the Objects
and FFI chapters in alpha6, and join Stephen Weeks as external contributors
to the book.
Meanwhile, Leo himself has recovered from the elation of being granted his PhD,
to the harsh reality of having to finish corrections.  He has been forced by
his colleagues to stop hacking on OCaml and submit his final thesis.  Rumours
are that he will reemerge in June after delivering his SAS
2013 talk on using an
implication-algebra generalisation of logic programming to concisely analyse
OpenMP programs for parallisation opportunities.
This month's talk was courtesy of Mathias Bourgoin,
who visited from France and gave a talk on his PhD work on GPU processing.  His
tool, SPOC has been
released onto OPAM and is a set of easy-to-use tools for generating CUDA and
OpenCL code, and also a camlp4 extension to write external kernels directly in
OCaml.  Thanks for visiting, Mathias!

   Hide
        
      
                    by Anil Madhavapeddy at May 01, 2013 
      
      
    
  

 
  

© 2012-2013 Anil MadhavapeddyInformation provided by Anil Madhavapeddy

